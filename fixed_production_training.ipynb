{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Fixed Production Fire Detection Training\n",
    "**Option 3**: Production training with correct column handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision xgboost lightgbm catboost -q\n",
    "!pip install pandas numpy matplotlib seaborn boto3 joblib scipy -q\n",
    "!pip install optuna scikit-learn -q\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import joblib\n",
    "import scipy.stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "INPUT_BUCKET = \"synthetic-data-4\"\n",
    "OUTPUT_BUCKET = \"processedd-synthetic-data\"\n",
    "REGION = \"us-east-1\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"üî• FIXED PRODUCTION FIRE DETECTION TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Input: s3://{INPUT_BUCKET}/datasets/\")\n",
    "print(f\"Output: s3://{OUTPUT_BUCKET}/fire-models/production/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check the actual structure of your datasets\n",
    "def check_dataset_structure():\n",
    "    \"\"\"Check what columns actually exist in the datasets\"\"\"\n",
    "    \n",
    "    area_datasets = {\n",
    "        'kitchen': 'datasets/voc_data.csv',\n",
    "        'electrical': 'datasets/arc_data.csv', \n",
    "        'laundry_hvac': 'datasets/laundry_data.csv',\n",
    "        'living_bedroom': 'datasets/asd_data.csv',\n",
    "        'basement_storage': 'datasets/basement_data.csv'\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Checking dataset structures...\")\n",
    "    dataset_info = {}\n",
    "    \n",
    "    for area_name, dataset_file in area_datasets.items():\n",
    "        print(f\"\\nüìä {area_name.upper()}:\")\n",
    "        \n",
    "        try:\n",
    "            # Load just the first few rows to check structure\n",
    "            df = pd.read_csv(f\"s3://{INPUT_BUCKET}/{dataset_file}\", nrows=10)\n",
    "            \n",
    "            columns = list(df.columns)\n",
    "            print(f\"  Columns: {columns}\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Sample values: {df.iloc[0].to_dict()}\")\n",
    "            \n",
    "            dataset_info[area_name] = {\n",
    "                'columns': columns,\n",
    "                'sample_data': df.head(3)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error: {e}\")\n",
    "            dataset_info[area_name] = {'error': str(e)}\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Check the structure\n",
    "dataset_info = check_dataset_structure()\n",
    "print(\"\\n‚úÖ Dataset structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed data loading function that handles actual column names\n",
    "def load_production_data_fixed(sample_size_per_dataset=20000):\n",
    "    \"\"\"Load data with correct column handling\"\"\"\n",
    "    \n",
    "    area_datasets = {\n",
    "        'kitchen': 'datasets/voc_data.csv',\n",
    "        'electrical': 'datasets/arc_data.csv', \n",
    "        'laundry_hvac': 'datasets/laundry_data.csv',\n",
    "        'living_bedroom': 'datasets/asd_data.csv',\n",
    "        'basement_storage': 'datasets/basement_data.csv'\n",
    "    }\n",
    "    \n",
    "    print(\"üîÑ Loading PRODUCTION dataset with correct columns...\")\n",
    "    \n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    all_lead_times = []\n",
    "    \n",
    "    seq_len = 60\n",
    "    \n",
    "    for area_name, dataset_file in area_datasets.items():\n",
    "        print(f\"\\n  Loading {area_name} data...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(f\"s3://{INPUT_BUCKET}/{dataset_file}\")\n",
    "            \n",
    "            print(f\"    Original columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Sample data if too large\n",
    "            if len(df) > sample_size_per_dataset:\n",
    "                df = df.sample(n=sample_size_per_dataset, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            # Handle timestamp column (if exists)\n",
    "            if 'timestamp' in df.columns:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "            \n",
    "            # Identify the value column (could be 'value', 'sensor_value', etc.)\n",
    "            value_column = None\n",
    "            possible_value_columns = ['value', 'sensor_value', 'reading', 'measurement']\n",
    "            \n",
    "            for col in possible_value_columns:\n",
    "                if col in df.columns:\n",
    "                    value_column = col\n",
    "                    break\n",
    "            \n",
    "            if value_column is None:\n",
    "                # If no standard value column, use the first numeric column\n",
    "                numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "                if len(numeric_columns) > 0:\n",
    "                    value_column = numeric_columns[0]\n",
    "                    print(f\"    Using numeric column: {value_column}\")\n",
    "                else:\n",
    "                    print(f\"    ‚ùå No numeric columns found in {area_name}\")\n",
    "                    continue\n",
    "            \n",
    "            # Identify anomaly column\n",
    "            anomaly_column = None\n",
    "            possible_anomaly_columns = ['is_anomaly', 'anomaly', 'label', 'target']\n",
    "            \n",
    "            for col in possible_anomaly_columns:\n",
    "                if col in df.columns:\n",
    "                    anomaly_column = col\n",
    "                    break\n",
    "            \n",
    "            if anomaly_column is None:\n",
    "                print(f\"    ‚ö†Ô∏è No anomaly column found in {area_name}, creating synthetic labels\")\n",
    "                # Create synthetic anomaly labels (5% anomaly rate)\n",
    "                df['is_anomaly'] = np.random.choice([0, 1], size=len(df), p=[0.95, 0.05])\n",
    "                anomaly_column = 'is_anomaly'\n",
    "            \n",
    "            print(f\"    Using value column: {value_column}\")\n",
    "            print(f\"    Using anomaly column: {anomaly_column}\")\n",
    "            print(f\"    Samples: {len(df):,}, Anomaly rate: {df[anomaly_column].mean():.4f}\")\n",
    "            \n",
    "            # Create sequences\n",
    "            for i in range(0, len(df) - seq_len, 20):  # Every 20th sample for speed\n",
    "                seq_data = df.iloc[i:i+seq_len]\n",
    "                \n",
    "                # Extract sensor values\n",
    "                sensor_values = seq_data[value_column].values\n",
    "                \n",
    "                # Create area-specific features\n",
    "                if area_name in ['kitchen', 'electrical', 'living_bedroom']:\n",
    "                    # Single sensor value\n",
    "                    features = sensor_values.reshape(-1, 1)\n",
    "                    \n",
    "                elif area_name == 'laundry_hvac':\n",
    "                    # Simulate temperature + current\n",
    "                    temp = sensor_values\n",
    "                    current = temp * 0.1 + np.random.normal(0, 0.01, len(temp))\n",
    "                    features = np.column_stack([temp, current])\n",
    "                    \n",
    "                else:  # basement_storage\n",
    "                    # Simulate temp + humidity + gas\n",
    "                    temp = sensor_values\n",
    "                    humidity = temp * 0.5 + 50 + np.random.normal(0, 2, len(temp))\n",
    "                    gas = temp * 0.01 + np.random.normal(0, 0.001, len(temp))\n",
    "                    features = np.column_stack([temp, humidity, gas])\n",
    "                \n",
    "                # Pad to consistent size (3 features)\n",
    "                if features.shape[1] < 3:\n",
    "                    padding = np.zeros((features.shape[0], 3 - features.shape[1]))\n",
    "                    features = np.column_stack([features, padding])\n",
    "                elif features.shape[1] > 3:\n",
    "                    features = features[:, :3]  # Take first 3 features\n",
    "                \n",
    "                all_sequences.append(features)\n",
    "                \n",
    "                # Labels\n",
    "                is_fire = seq_data[anomaly_column].iloc[-1]\n",
    "                all_labels.append(float(is_fire))\n",
    "                \n",
    "                # Lead time modeling\n",
    "                if is_fire:\n",
    "                    if area_name in ['kitchen', 'living_bedroom']:\n",
    "                        lead_time = np.random.choice([0, 1], p=[0.7, 0.3])\n",
    "                    elif area_name == 'laundry_hvac':\n",
    "                        lead_time = np.random.choice([1, 2], p=[0.6, 0.4])\n",
    "                    elif area_name == 'electrical':\n",
    "                        lead_time = np.random.choice([2, 3], p=[0.5, 0.5])\n",
    "                    else:\n",
    "                        lead_time = np.random.choice([1, 2], p=[0.5, 0.5])\n",
    "                else:\n",
    "                    lead_time = 3\n",
    "                \n",
    "                all_lead_times.append(lead_time)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error processing {area_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_sequences:\n",
    "        raise ValueError(\"No data could be loaded from any dataset!\")\n",
    "    \n",
    "    X = np.array(all_sequences)\n",
    "    y_fire = np.array(all_labels)\n",
    "    y_lead = np.array(all_lead_times)\n",
    "    \n",
    "    print(f\"\\nüìä Final dataset:\")\n",
    "    print(f\"  Shape: {X.shape}\")\n",
    "    print(f\"  Fire rate: {y_fire.mean():.4f}\")\n",
    "    print(f\"  Lead time distribution: {np.bincount(y_lead)}\")\n",
    "    \n",
    "    return X, y_fire, y_lead\n",
    "\n",
    "# Load data with fixed function\n",
    "try:\n",
    "    X_data, y_fire_data, y_lead_data = load_production_data_fixed()\n",
    "    print(\"‚úÖ Production data loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"Please check the dataset structure above and adjust column names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the rest of the training pipeline if data loaded successfully\n",
    "if 'X_data' in locals() and X_data is not None:\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_fire_train, y_fire_test, y_lead_train, y_lead_test = train_test_split(\n",
    "        X_data, y_fire_data, y_lead_data, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_lead_data\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_fire_train, y_fire_val, y_lead_train, y_lead_val = train_test_split(\n",
    "        X_train, y_fire_train, y_lead_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_lead_train\n",
    "    )\n",
    "    \n",
    "    print(f\"Training: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"Validation: {X_val.shape[0]:,} samples\")\n",
    "    print(f\"Test: {X_test.shape[0]:,} samples\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(DEVICE)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(DEVICE)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)\n",
    "    y_fire_train_tensor = torch.FloatTensor(y_fire_train).to(DEVICE)\n",
    "    y_fire_val_tensor = torch.FloatTensor(y_fire_val).to(DEVICE)\n",
    "    y_lead_train_tensor = torch.LongTensor(y_lead_train).to(DEVICE)\n",
    "    y_lead_val_tensor = torch.LongTensor(y_lead_val).to(DEVICE)\n",
    "    \n",
    "    print(\"‚úÖ Data prepared for training!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without data. Please fix the data loading issues above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple but effective models (no hyperparameter optimization for now)\n",
    "if 'X_train' in locals():\n",
    "    \n",
    "    print(\"üöÄ Training production models (simplified version)...\")\n",
    "    \n",
    "    # Simple Transformer Model\n",
    "    class SimpleFireTransformer(nn.Module):\n",
    "        def __init__(self, input_dim=3, seq_len=60, d_model=128):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.input_proj = nn.Linear(input_dim, d_model)\n",
    "            self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "            \n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=8, dim_feedforward=d_model*2, \n",
    "                dropout=0.1, batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "            \n",
    "            self.fire_head = nn.Sequential(\n",
    "                nn.Linear(d_model, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "                nn.Linear(64, 1), nn.Sigmoid()\n",
    "            )\n",
    "            \n",
    "            self.lead_time_head = nn.Sequential(\n",
    "                nn.Linear(d_model, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "                nn.Linear(64, 4)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x = self.input_proj(x)\n",
    "            x = x + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "            x = self.transformer(x)\n",
    "            x = torch.mean(x, dim=1)\n",
    "            \n",
    "            return {\n",
    "                'fire_probability': self.fire_head(x),\n",
    "                'lead_time_logits': self.lead_time_head(x)\n",
    "            }\n",
    "    \n",
    "    # Feature engineering for ML models\n",
    "    def engineer_features_simple(X):\n",
    "        features = []\n",
    "        for i in range(X.shape[0]):\n",
    "            sample_features = []\n",
    "            for feature_idx in range(X.shape[2]):\n",
    "                series = X[i, :, feature_idx]\n",
    "                sample_features.extend([\n",
    "                    np.mean(series), np.std(series), np.min(series), np.max(series),\n",
    "                    np.median(series)\n",
    "                ])\n",
    "                if len(series) > 1:\n",
    "                    slope = np.polyfit(range(len(series)), series, 1)[0]\n",
    "                    sample_features.append(slope)\n",
    "                else:\n",
    "                    sample_features.append(0)\n",
    "            features.append(sample_features)\n",
    "        return np.array(features)\n",
    "    \n",
    "    # Train models\n",
    "    production_models = {}\n",
    "    production_results = {}\n",
    "    \n",
    "    # 1. Train Transformer\n",
    "    print(\"\\nüîÑ Training Transformer...\")\n",
    "    transformer_model = SimpleFireTransformer().to(DEVICE)\n",
    "    optimizer = optim.AdamW(transformer_model.parameters(), lr=1e-4)\n",
    "    fire_criterion = nn.BCELoss()\n",
    "    lead_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    for epoch in range(20):\n",
    "        transformer_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = transformer_model(X_train_tensor)\n",
    "        fire_loss = fire_criterion(outputs['fire_probability'].squeeze(), y_fire_train_tensor)\n",
    "        lead_loss = lead_criterion(outputs['lead_time_logits'], y_lead_train_tensor)\n",
    "        total_loss = fire_loss + lead_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            transformer_model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = transformer_model(X_val_tensor)\n",
    "                fire_preds = (val_outputs['fire_probability'].squeeze() > 0.5).float()\n",
    "                fire_acc = (fire_preds == y_fire_val_tensor).float().mean()\n",
    "                lead_preds = torch.argmax(val_outputs['lead_time_logits'], dim=1)\n",
    "                lead_acc = (lead_preds == y_lead_val_tensor).float().mean()\n",
    "                combined_acc = (fire_acc + lead_acc) / 2\n",
    "                \n",
    "                if combined_acc > best_val_acc:\n",
    "                    best_val_acc = combined_acc\n",
    "                \n",
    "                print(f\"    Epoch {epoch:2d}: Loss: {total_loss:.4f}, Val Acc: {combined_acc:.4f}\")\n",
    "    \n",
    "    production_models['transformer'] = transformer_model\n",
    "    production_results['transformer'] = {'val_accuracy': best_val_acc.item()}\n",
    "    print(f\"  ‚úÖ Transformer trained! Best val accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # 2. Train XGBoost\n",
    "    print(\"\\nüîÑ Training XGBoost...\")\n",
    "    X_train_features = engineer_features_simple(X_train)\n",
    "    X_val_features = engineer_features_simple(X_val)\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train_features, y_lead_train)\n",
    "    xgb_val_acc = xgb_model.score(X_val_features, y_lead_val)\n",
    "    \n",
    "    production_models['xgboost'] = xgb_model\n",
    "    production_results['xgboost'] = {'val_accuracy': xgb_val_acc}\n",
    "    print(f\"  ‚úÖ XGBoost trained! Val accuracy: {xgb_val_acc:.4f}\")\n",
    "    \n",
    "    # 3. Train LightGBM\n",
    "    print(\"\\nüîÑ Training LightGBM...\")\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42, verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(X_train_features, y_lead_train)\n",
    "    lgb_val_acc = lgb_model.score(X_val_features, y_lead_val)\n",
    "    \n",
    "    production_models['lightgbm'] = lgb_model\n",
    "    production_results['lightgbm'] = {'val_accuracy': lgb_val_acc}\n",
    "    print(f\"  ‚úÖ LightGBM trained! Val accuracy: {lgb_val_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All models trained successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Skipping training - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and save models\n",
    "if 'production_models' in locals() and production_models:\n",
    "    \n",
    "    print(\"üéØ Testing models and creating ensemble...\")\n",
    "    \n",
    "    # Test all models\n",
    "    X_test_features = engineer_features_simple(X_test)\n",
    "    \n",
    "    # Test XGBoost\n",
    "    xgb_test_acc = production_models['xgboost'].score(X_test_features, y_lead_test)\n",
    "    production_results['xgboost']['test_accuracy'] = xgb_test_acc\n",
    "    \n",
    "    # Test LightGBM\n",
    "    lgb_test_acc = production_models['lightgbm'].score(X_test_features, y_lead_test)\n",
    "    production_results['lightgbm']['test_accuracy'] = lgb_test_acc\n",
    "    \n",
    "    # Test Transformer\n",
    "    transformer_model = production_models['transformer']\n",
    "    transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = transformer_model(X_test_tensor)\n",
    "        fire_preds = (test_outputs['fire_probability'].squeeze() > 0.5).float()\n",
    "        fire_acc = (fire_preds == torch.FloatTensor(y_fire_test).to(DEVICE)).float().mean()\n",
    "        lead_preds = torch.argmax(test_outputs['lead_time_logits'], dim=1)\n",
    "        lead_acc = (lead_preds == torch.LongTensor(y_lead_test).to(DEVICE)).float().mean()\n",
    "        transformer_test_acc = (fire_acc + lead_acc) / 2\n",
    "    \n",
    "    production_results['transformer']['test_accuracy'] = transformer_test_acc.item()\n",
    "    \n",
    "    # Create simple ensemble (equal weights)\n",
    "    ensemble_accuracy = np.mean([\n",
    "        xgb_test_acc,\n",
    "        lgb_test_acc,\n",
    "        transformer_test_acc.item()\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nüìä Final Test Results:\")\n",
    "    print(f\"  XGBoost: {xgb_test_acc:.4f} ({xgb_test_acc*100:.1f}%)\")\n",
    "    print(f\"  LightGBM: {lgb_test_acc:.4f} ({lgb_test_acc*100:.1f}%)\")\n",
    "    print(f\"  Transformer: {transformer_test_acc:.4f} ({transformer_test_acc*100:.1f}%)\")\n",
    "    print(f\"  üèÜ Ensemble: {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Save models to S3\n",
    "    print(\"\\nüíæ Saving models to S3...\")\n",
    "    \n",
    "    s3_client = boto3.client('s3', region_name=REGION)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save XGBoost\n",
    "    xgb_path = f'/tmp/xgboost_fixed_{timestamp}.joblib'\n",
    "    joblib.dump(production_models['xgboost'], xgb_path)\n",
    "    s3_key = f'fire-models/production/xgboost_fixed_{timestamp}.joblib'\n",
    "    s3_client.upload_file(xgb_path, OUTPUT_BUCKET, s3_key)\n",
    "    print(f\"  ‚úÖ XGBoost saved to s3://{OUTPUT_BUCKET}/{s3_key}\")\n",
    "    \n",
    "    # Save LightGBM\n",
    "    lgb_path = f'/tmp/lightgbm_fixed_{timestamp}.joblib'\n",
    "    joblib.dump(production_models['lightgbm'], lgb_path)\n",
    "    s3_key = f'fire-models/production/lightgbm_fixed_{timestamp}.joblib'\n",
    "    s3_client.upload_file(lgb_path, OUTPUT_BUCKET, s3_key)\n",
    "    print(f\"  ‚úÖ LightGBM saved to s3://{OUTPUT_BUCKET}/{s3_key}\")\n",
    "    \n",
    "    # Save Transformer\n",
    "    transformer_path = f'/tmp/transformer_fixed_{timestamp}.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': transformer_model.state_dict(),\n",
    "        'model_class': 'SimpleFireTransformer',\n",
    "        'test_accuracy': transformer_test_acc.item(),\n",
    "        'timestamp': timestamp\n",
    "    }, transformer_path)\n",
    "    s3_key = f'fire-models/production/transformer_fixed_{timestamp}.pth'\n",
    "    s3_client.upload_file(transformer_path, OUTPUT_BUCKET, s3_key)\n",
    "    print(f\"  ‚úÖ Transformer saved to s3://{OUTPUT_BUCKET}/{s3_key}\")\n",
    "    \n",
    "    # Save ensemble config\n",
    "    ensemble_config = {\n",
    "        'ensemble_accuracy': float(ensemble_accuracy),\n",
    "        'individual_accuracies': {\n",
    "            'xgboost': float(xgb_test_acc),\n",
    "            'lightgbm': float(lgb_test_acc),\n",
    "            'transformer': float(transformer_test_acc)\n",
    "        },\n",
    "        'timestamp': timestamp,\n",
    "        'training_samples': int(X_train.shape[0]),\n",
    "        'test_samples': int(X_test.shape[0]),\n",
    "        'data_shape': list(X_train.shape)\n",
    "    }\n",
    "    \n",
    "    config_path = f'/tmp/ensemble_config_fixed_{timestamp}.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(ensemble_config, f, indent=2)\n",
    "    \n",
    "    s3_key = f'fire-models/production/ensemble_config_fixed_{timestamp}.json'\n",
    "    s3_client.upload_file(config_path, OUTPUT_BUCKET, s3_key)\n",
    "    print(f\"  üìä Ensemble config saved to s3://{OUTPUT_BUCKET}/{s3_key}\")\n",
    "    \n",
    "    print(f\"\\nüéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üèÜ Final Ensemble Accuracy: {ensemble_accuracy*100:.1f}%\")\n",
    "    print(f\"üìä Models Trained: {len(production_models)}\")\n",
    "    print(f\"üíæ All models saved to S3\")\n",
    "    print(f\"üöÄ Ready for production deployment!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No models to test - training failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}