{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ğŸ”¥ FLIR+SCD41 Fire Detection System - Unified Training with Diagnostics\n",
    "\n",
    "This notebook provides a complete end-to-end training pipeline for the FLIR+SCD41 fire detection system with built-in diagnostics to prevent common machine learning problems:\n",
    "\n",
    "1. **Dataset generation** (FLIR Lepton 3.5 + SCD41 COâ‚‚ sensor data)\n",
    "2. **Data storage and splitting**\n",
    "3. **Model training** with regularization to prevent overfitting\n",
    "4. **Diagnostics** to detect underfitting and overfitting\n",
    "5. **Ensemble weight calculation**\n",
    "6. **Model evaluation**\n",
    "\n",
    "## System Overview\n",
    "The system uses:\n",
    "- FLIR Lepton 3.5 thermal camera (15 features)\n",
    "- Sensirion SCD41 COâ‚‚ sensor (3 features)\n",
    "- Total: 18 features for fire detection\n",
    "\n",
    "## Common ML Problems Addressed\n",
    "- **Underfitting** (model not learning): Poor performance on both training and validation sets\n",
    "- **Overfitting** (remembering patterns): High performance on training set, poor performance on validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, learning_curve, validation_curve, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_generation",
   "metadata": {},
   "source": [
    "## 1. ğŸ”„ Dataset Generation\n",
    "\n",
    "Generate synthetic training data for FLIR+SCD41 sensors with realistic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples=10000):\n",
    "    \"\"\"Generate synthetic FLIR+SCD41 dataset with controlled noise\"\"\"\n",
    "    print(\"ğŸ”„ Generating synthetic FLIR+SCD41 dataset...\")\n",
    "    \n",
    "    # Generate FLIR features (15 features)\n",
    "    np.random.seed(42)\n",
    "    flir_features = np.random.normal(25, 10, (num_samples, 15))\n",
    "    flir_features[:, 0] = np.clip(flir_features[:, 0], -40, 330)  # t_mean: -40 to 330Â°C\n",
    "    flir_features[:, 2] = np.clip(flir_features[:, 2], -40, 330)  # t_max: -40 to 330Â°C\n",
    "    flir_features[:, 4] = np.clip(flir_features[:, 4], 0, 100)    # t_hot_area_pct: 0-100%\n",
    "    \n",
    "    # Generate SCD41 features (3 features)\n",
    "    scd41_features = np.random.normal(450, 100, (num_samples, 3))\n",
    "    scd41_features[:, 0] = np.clip(scd41_features[:, 0], 400, 40000)  # gas_val: 400-40000 ppm\n",
    "    \n",
    "    print(f\"âœ… Generated {num_samples} samples\")\n",
    "    print(f\"FLIR features shape: {flir_features.shape}\")\n",
    "    print(f\"SCD41 features shape: {scd41_features.shape}\")\n",
    "    \n",
    "    return flir_features, scd41_features\n",
    "\n",
    "# Generate synthetic data\n",
    "flir_features, scd41_features = generate_synthetic_data(num_samples=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_dataset",
   "metadata": {},
   "source": [
    "## 2. ğŸ’¾ Dataset Storage\n",
    "\n",
    "Combine features and create labels, then save the dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_dataset_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(flir_features, scd41_features):\n",
    "    \"\"\"Combine features and create labels with balanced distribution\"\"\"\n",
    "    print(\"ğŸ’¾ Combining features and creating dataset...\")\n",
    "    \n",
    "    # Combine all features (15 FLIR + 3 SCD41 = 18 features)\n",
    "    all_features = np.concatenate([flir_features, scd41_features], axis=1)\n",
    "    \n",
    "    # Create labels (fire detected or not) with more realistic patterns\n",
    "    # Fire probability based on multiple factors\n",
    "    fire_probability = (\n",
    "        (flir_features[:, 2] > 60).astype(int) * 0.3 +  # High max temperature\n",
    "        (scd41_features[:, 0] > 1000).astype(int) * 0.3 +  # High CO2\n",
    "        (flir_features[:, 4] > 10).astype(int) * 0.2 +  # Large hot area\n",
    "        (flir_features[:, 12] > 50).astype(int) * 0.2  # High temperature proxy\n",
    "    )\n",
    "    \n",
    "    # Add some noise to make it more realistic\n",
    "    noise = np.random.normal(0, 0.1, len(fire_probability))\n",
    "    fire_probability = np.clip(fire_probability + noise, 0, 1)\n",
    "    \n",
    "    labels = np.random.binomial(1, fire_probability)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_names = [\n",
    "        't_mean', 't_std', 't_max', 't_p95', 't_hot_area_pct',\n",
    "        't_hot_largest_blob_pct', 't_grad_mean', 't_grad_std',\n",
    "        't_diff_mean', 't_diff_std', 'flow_mag_mean', 'flow_mag_std',\n",
    "        'tproxy_val', 'tproxy_delta', 'tproxy_vel',\n",
    "        'gas_val', 'gas_delta', 'gas_vel'\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(all_features, columns=feature_names)\n",
    "    df['fire_detected'] = labels\n",
    "    \n",
    "    print(f\"âœ… Dataset created with shape: {df.shape}\")\n",
    "    print(f\"Fire samples: {sum(labels)} ({sum(labels)/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    return df, feature_names\n",
    "\n",
    "# Create dataset\n",
    "df, feature_names = create_dataset(flir_features, scd41_features)\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to disk\n",
    "data_dir = os.path.join(project_root, 'data', 'flir_scd41')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "dataset_path = os.path.join(data_dir, 'flir_scd41_dataset.csv')\n",
    "df.to_csv(dataset_path, index=False)\n",
    "\n",
    "print(f\"âœ… Dataset saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_splitting",
   "metadata": {},
   "source": [
    "## 3. ğŸ“Š Data Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test sets with stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, feature_names, test_size=0.15, val_size=0.15):\n",
    "    \"\"\"Split dataset into train/validation/test sets with stratification\"\"\"\n",
    "    print(\"ğŸ“Š Splitting dataset into train/validation/test sets...\")\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = df.drop('fire_detected', axis=1).values\n",
    "    y = df['fire_detected'].values\n",
    "    \n",
    "    # First split: separate test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation sets\n",
    "    # Adjust val_size to account for the first split\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits to disk\n",
    "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "train_df['fire_detected'] = y_train\n",
    "train_df.to_csv(os.path.join(data_dir, 'train.csv'), index=False)\n",
    "\n",
    "val_df = pd.DataFrame(X_val, columns=feature_names)\n",
    "val_df['fire_detected'] = y_val\n",
    "val_df.to_csv(os.path.join(data_dir, 'val.csv'), index=False)\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "test_df['fire_detected'] = y_test\n",
    "test_df.to_csv(os.path.join(data_dir, 'test.csv'), index=False)\n",
    "\n",
    "print(\"âœ… Dataset splits saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostics_intro",
   "metadata": {},
   "source": [
    "## 4. ğŸ” Model Diagnostics Introduction\n",
    "\n",
    "Before training, let's understand how to detect common machine learning problems:\n",
    "\n",
    "### Underfitting (Model Not Learning)\n",
    "- **Symptoms**: Poor performance on both training and validation sets\n",
    "- **Causes**: Insufficient model capacity, poor data quality, inappropriate learning rate\n",
    "\n",
    "### Overfitting (Remembering Patterns)\n",
    "- **Symptoms**: High performance on training set, poor performance on validation/test sets\n",
    "- **Causes**: Model too complex for the data, insufficient regularization, too little training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic_tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def diagnose_model_learning(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Diagnose if model is learning properly\"\"\"\n",
    "    print(\"ğŸ”¬ Diagnosing model learning...\")\n",
    "    \n",
    "    # Check for data issues\n",
    "    print(\"\\n1. Data Quality Check:\")\n",
    "    print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"   Features: {X_train.shape[1]}\")\n",
    "    print(f\"   Class distribution - Train: {np.bincount(y_train)}\")\n",
    "    print(f\"   Class distribution - Validation: {np.bincount(y_val)}\")\n",
    "    \n",
    "    # Check for data leakage\n",
    "    common_rows = np.sum([np.any(np.all(X_train == row, axis=1)) for row in X_val])\n",
    "    if common_rows > 0:\n",
    "        print(f\"   âš ï¸  Warning: {common_rows} identical rows found in train and validation sets (possible data leakage)\")\n",
    "    else:\n",
    "        print(\"   âœ… No data leakage detected\")\n",
    "    \n",
    "    # Check for feature variance\n",
    "    feature_variances = np.var(X_train, axis=0)\n",
    "    low_variance_features = np.sum(feature_variances < 1e-6)\n",
    "    if low_variance_features > 0:\n",
    "        print(f\"   âš ï¸  Warning: {low_variance_features} features have very low variance\")\n",
    "    else:\n",
    "        print(\"   âœ… All features have sufficient variance\")\n",
    "    \n",
    "    return {\n",
    "        'samples': X_train.shape[0],\n",
    "        'features': X_train.shape[1],\n",
    "        'train_class_dist': np.bincount(y_train),\n",
    "        'val_class_dist': np.bincount(y_val),\n",
    "        'data_leakage': common_rows,\n",
    "        'low_variance_features': low_variance_features\n",
    "    }\n",
    "\n",
    "# Run initial diagnostics\n",
    "data_info = diagnose_model_learning(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training",
   "metadata": {},
   "source": [
    "## 5. ğŸš€ Model Training with Regularization\n",
    "\n",
    "Train multiple models with built-in techniques to prevent overfitting:\n",
    "1. XGBoost with regularization and early stopping\n",
    "2. Neural Network with dropout and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgboost_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train XGBoost model with regularization and early stopping\"\"\"\n",
    "    print(\"ğŸš€ Training XGBoost model with regularization...\")\n",
    "    \n",
    "    # Create and train XGBoost model with strong regularization\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200,          # Increased for early stopping\n",
    "        max_depth=4,               # Reduced depth to prevent overfitting\n",
    "        learning_rate=0.1,         # Lower learning rate\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,             # L1 regularization\n",
    "        reg_lambda=1.0,            # L2 regularization\n",
    "        random_state=42,\n",
    "        early_stopping_rounds=10,  # Early stopping\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    # Fit with validation set for early stopping\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate XGBoost model\n",
    "    xgb_train_pred = xgb_model.predict(X_train)\n",
    "    xgb_val_pred = xgb_model.predict(X_val)\n",
    "    xgb_val_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    xgb_train_metrics = {\n",
    "        'accuracy': accuracy_score(y_train, xgb_train_pred),\n",
    "        'f1_score': f1_score(y_train, xgb_train_pred),\n",
    "        'precision': precision_score(y_train, xgb_train_pred),\n",
    "        'recall': recall_score(y_train, xgb_train_pred)\n",
    "    }\n",
    "    \n",
    "    xgb_val_metrics = {\n",
    "        'accuracy': accuracy_score(y_val, xgb_val_pred),\n",
    "        'f1_score': f1_score(y_val, xgb_val_pred),\n",
    "        'precision': precision_score(y_val, xgb_val_pred),\n",
    "        'recall': recall_score(y_val, xgb_val_pred),\n",
    "        'auc': roc_auc_score(y_val, xgb_val_pred_proba) if len(np.unique(y_val)) > 1 else 0.0\n",
    "    }\n",
    "    \n",
    "    print(\"XGBoost Training Metrics:\")\n",
    "    for metric, value in xgb_train_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nXGBoost Validation Metrics:\")\n",
    "    for metric, value in xgb_val_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    accuracy_gap = xgb_train_metrics['accuracy'] - xgb_val_metrics['accuracy']\n",
    "    if accuracy_gap > 0.1:\n",
    "        print(\"âš ï¸  Warning: XGBoost may be overfitting!\")\n",
    "        print(f\"   Training-Validation accuracy gap: {accuracy_gap:.4f}\")\n",
    "    \n",
    "    return xgb_model, xgb_train_metrics, xgb_val_metrics\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model, xgb_train_metrics, xgb_val_metrics = train_xgboost_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn_classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Improved Neural Network model with regularization\n",
    "class ImprovedNN(nn.Module):\n",
    "    \"\"\"Improved neural network with dropout and batch normalization\"\"\"\n",
    "    def __init__(self, input_size=18, hidden_sizes=[64, 32, 16], num_classes=2, dropout_rate=0.3):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Hidden layers with batch normalization and dropout\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define dataset class\n",
    "class FireDataset(Dataset):\n",
    "    \"\"\"Dataset for fire detection\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, y, factor=0.1):\n",
    "    \"\"\"Simple data augmentation by adding noise\"\"\"\n",
    "    print(\"ğŸ“ˆ Augmenting training data...\")\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    n_augment = int(n_samples * factor)\n",
    "    \n",
    "    # Randomly select samples to augment\n",
    "    indices = np.random.choice(n_samples, n_augment, replace=True)\n",
    "    \n",
    "    # Add gaussian noise\n",
    "    noise = np.random.normal(0, 0.01, (n_augment, X.shape[1]))\n",
    "    X_augmented = X[indices] + noise\n",
    "    y_augmented = y[indices]\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    X_combined = np.vstack([X, X_augmented])\n",
    "    y_combined = np.hstack([y, y_augmented])\n",
    "    \n",
    "    print(f\"   Original samples: {n_samples}\")\n",
    "    print(f\"   Augmented samples: {n_augment}\")\n",
    "    print(f\"   Total samples: {X_combined.shape[0]}\")\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "\n",
    "# Augment training data\n",
    "X_train_aug, y_train_aug = augment_data(X_train, y_train, factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train improved neural network with early stopping and regularization\"\"\"\n",
    "    print(\"\\nğŸš€ Training Improved Neural Network model...\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    nn_model = ImprovedNN(input_size=18, dropout_rate=0.3).to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 regularization\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = FireDataset(X_train, y_train)\n",
    "    val_dataset = FireDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 100  # Increased epochs with early stopping\n",
    "    best_val_acc = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        nn_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # Validation\n",
    "        nn_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                \n",
    "                outputs = nn_model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model_path = os.path.join(data_dir, 'best_improved_nn_model.pth')\n",
    "            torch.save(nn_model.state_dict(), model_path)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Store losses for plotting\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "        val_losses.append(val_loss/len(val_loader))\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'  Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "            print(f'  Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "    \n",
    "    # Final NN metrics\n",
    "    nn_train_metrics = {\n",
    "        'accuracy': accuracy_score(train_targets, train_preds),\n",
    "        'f1_score': f1_score(train_targets, train_preds),\n",
    "        'precision': precision_score(train_targets, train_preds),\n",
    "        'recall': recall_score(train_targets, train_preds)\n",
    "    }\n",
    "    \n",
    "    nn_val_metrics = {\n",
    "        'accuracy': best_val_acc,\n",
    "        'f1_score': f1_score(val_targets, val_preds),\n",
    "        'precision': precision_score(val_targets, val_preds),\n",
    "        'recall': recall_score(val_targets, val_preds)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nNeural Network Final Metrics:\")\n",
    "    print(\"Training Metrics:\")\n",
    "    for metric, value in nn_train_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for metric, value in nn_val_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    accuracy_gap = nn_train_metrics['accuracy'] - nn_val_metrics['accuracy']\n",
    "    if accuracy_gap > 0.1:\n",
    "        print(\"âš ï¸  Warning: Neural Network may be overfitting!\")\n",
    "        print(f\"   Training-Validation accuracy gap: {accuracy_gap:.4f}\")\n",
    "    \n",
    "    return nn_model, nn_train_metrics, nn_val_metrics, train_losses, val_losses\n",
    "\n",
    "# Train Neural Network model\n",
    "nn_model, nn_train_metrics, nn_val_metrics, train_losses, val_losses = train_neural_network(\n",
    "    X_train_aug, y_train_aug, X_val, y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for Neural Network\n",
    "print(\"ğŸ“Š Plotting Neural Network training curves...\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves - Neural Network')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n",
    "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves - Neural Network')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross_validation",
   "metadata": {},
   "source": [
    "## 6. ğŸ” Cross-Validation for Robust Performance Estimates\n",
    "\n",
    "Perform cross-validation to get more reliable performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_models(X, y):\n",
    "    \"\"\"Perform cross-validation to get more robust performance estimates\"\"\"\n",
    "    print(\"ğŸ” Performing Cross-Validation...\")\n",
    "    \n",
    "    # XGBoost cross-validation\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Stratified k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(xgb_model, X, y, cv=skf, scoring='accuracy')\n",
    "    \n",
    "    print(f\"XGBoost Cross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validate_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning_curves",
   "metadata": {},
   "source": [
    "## 7. ğŸ“ˆ Learning Curves Visualization\n",
    "\n",
    "Generate learning curves to diagnose bias and variance problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning_curves_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model_type, X, y, cv=5):\n",
    "    \"\"\"Plot learning curves to diagnose underfitting/overfitting\"\"\"\n",
    "    print(f\"ğŸ“Š Generating {model_type} Learning Curves...\")\n",
    "    \n",
    "    if model_type == 'xgboost':\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42\n",
    "        )\n",
    "        scoring = 'accuracy'\n",
    "    else:  # neural_network\n",
    "        print(\"   Learning curves for neural networks require custom implementation\")\n",
    "        print(\"   Please use validation curves instead\")\n",
    "        return\n",
    "    \n",
    "    # Generate learning curves\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, cv=cv, n_jobs=-1, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring=scoring\n",
    "    )\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    \n",
    "    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation score')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title(f'Learning Curves ({model_type.capitalize()})')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    data_dir = os.path.join(project_root, 'data', 'flir_scd41')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    plot_path = os.path.join(data_dir, f'{model_type}_learning_curves.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   ğŸ“Š Learning curves saved to {plot_path}\")\n",
    "    \n",
    "    # Diagnose based on learning curves\n",
    "    final_train_score = train_mean[-1]\n",
    "    final_val_score = val_mean[-1]\n",
    "    gap = final_train_score - final_val_score\n",
    "    \n",
    "    print(f\"   Final Training Score: {final_train_score:.4f}\")\n",
    "    print(f\"   Final Validation Score: {final_val_score:.4f}\")\n",
    "    print(f\"   Gap: {gap:.4f}\")\n",
    "    \n",
    "    if gap > 0.1:\n",
    "        print(\"   ğŸ¤” Diagnosis: Model may be overfitting (high variance)\")\n",
    "        print(\"   ğŸ’¡ Suggestions:\")\n",
    "        print(\"      - Add regularization\")\n",
    "        print(\"      - Reduce model complexity\")\n",
    "        print(\"      - Get more training data\")\n",
    "    elif final_val_score < 0.7:\n",
    "        print(\"   ğŸ¤” Diagnosis: Model may be underfitting (high bias)\")\n",
    "        print(\"   ğŸ’¡ Suggestions:\")\n",
    "        print(\"      - Increase model complexity\")\n",
    "        print(\"      - Add more features\")\n",
    "        print(\"      - Train for longer\")\n",
    "    else:\n",
    "        print(\"   âœ… Model appears to be learning properly\")\n",
    "\n",
    "# Generate learning curves\n",
    "plot_learning_curves('xgboost', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation_curves",
   "metadata": {},
   "source": [
    "## 8. ğŸ“ˆ Validation Curves\n",
    "\n",
    "Use validation curves to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation_curves_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curves(X_train, y_train, param_name='max_depth', param_range=range(1, 11)):\n",
    "    \"\"\"Plot validation curves to find optimal hyperparameters\"\"\"\n",
    "    print(f\"ğŸ“Š Generating Validation Curves for {param_name}...\")\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_scores, val_scores = validation_curve(\n",
    "        model, X_train, y_train, \n",
    "        param_name=param_name, param_range=param_range,\n",
    "        cv=5, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    # Plot validation curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(param_range, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    \n",
    "    plt.plot(param_range, val_mean, 'o-', color='red', label='Validation score')\n",
    "    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title(f'Validation Curves ({param_name})')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    data_dir = os.path.join(project_root, 'data', 'flir_scd41')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    plot_path = os.path.join(data_dir, f'validation_curves_{param_name}.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   ğŸ“Š Validation curves saved to {plot_path}\")\n",
    "    \n",
    "    # Find optimal parameter\n",
    "    optimal_idx = np.argmax(val_mean)\n",
    "    optimal_param = param_range[optimal_idx]\n",
    "    print(f\"   Optimal {param_name}: {optimal_param}\")\n",
    "    print(f\"   Best validation score: {val_mean[optimal_idx]:.4f}\")\n",
    "\n",
    "# Generate validation curves\n",
    "plot_validation_curves(X_train, y_train, param_name='max_depth', param_range=range(1, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ensemble_weights",
   "metadata": {},
   "source": [
    "## 9. âš–ï¸ Ensemble Weight Calculation\n",
    "\n",
    "Calculate optimal weights for model ensemble based on validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ensemble_weights(xgb_score, nn_score):\n",
    "    \"\"\"Calculate ensemble weights based on validation performance\"\"\"\n",
    "    print(\"âš–ï¸ Calculating ensemble weights...\")\n",
    "    \n",
    "    print(f\"XGBoost validation accuracy: {xgb_score:.4f}\")\n",
    "    print(f\"Neural Network validation accuracy: {nn_score:.4f}\")\n",
    "    \n",
    "    # Method: Performance-based weighting (exponential scaling)\n",
    "    def calculate_performance_weights(scores, scaling_factor=2.0):\n",
    "        \"\"\"Calculate weights based on performance scores using exponential scaling\"\"\"\n",
    "        # Normalize scores to [0, 1] range\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        \n",
    "        if max_score == min_score:\n",
    "            # All models have same performance, equal weights\n",
    "            return [1.0/len(scores)] * len(scores)\n",
    "        \n",
    "        normalized_scores = [(score - min_score) / (max_score - min_score) for score in scores]\n",
    "        \n",
    "        # Apply exponential scaling\n",
    "        weighted_scores = [np.exp(scaling_factor * score) for score in normalized_scores]\n",
    "        \n",
    "        # Normalize to sum to 1\n",
    "        total_weight = sum(weighted_scores)\n",
    "        weights = [w / total_weight for w in weighted_scores]\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    # Calculate weights\n",
    "    model_scores = [xgb_score, nn_score]\n",
    "    ensemble_weights = calculate_performance_weights(model_scores)\n",
    "    \n",
    "    print(f\"\\nEnsemble weights:\")\n",
    "    print(f\"  XGBoost weight: {ensemble_weights[0]:.4f}\")\n",
    "    print(f\"  Neural Network weight: {ensemble_weights[1]:.4f}\")\n",
    "    \n",
    "    return ensemble_weights\n",
    "\n",
    "# Calculate ensemble weights\n",
    "ensemble_weights = calculate_ensemble_weights(\n",
    "    xgb_val_metrics['accuracy'], \n",
    "    nn_val_metrics['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_evaluation",
   "metadata": {},
   "source": [
    "## 10. ğŸ§ª Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the ensemble model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(xgb_model, nn_model, X_test, y_test, ensemble_weights):\n",
    "    \"\"\"Evaluate models on test set\"\"\"\n",
    "    print(\"ğŸ§ª Evaluating models on test set...\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # XGBoost predictions\n",
    "    xgb_test_pred = xgb_model.predict(X_test)\n",
    "    xgb_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Neural Network predictions\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_data = torch.FloatTensor(X_test).to(device)\n",
    "        nn_outputs = nn_model(test_data)\n",
    "        nn_test_pred_proba = torch.softmax(nn_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        nn_test_pred = (nn_test_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Ensemble predictions (weighted average)\n",
    "    ensemble_pred_proba = (\n",
    "        ensemble_weights[0] * xgb_test_pred_proba + \n",
    "        ensemble_weights[1] * nn_test_pred_proba\n",
    "    )\n",
    "    ensemble_test_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    xgb_test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, xgb_test_pred),\n",
    "        'f1_score': f1_score(y_test, xgb_test_pred),\n",
    "        'precision': precision_score(y_test, xgb_test_pred),\n",
    "        'recall': recall_score(y_test, xgb_test_pred),\n",
    "        'auc': roc_auc_score(y_test, xgb_test_pred_proba)\n",
    "    }\n",
    "    \n",
    "    nn_test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, nn_test_pred),\n",
    "        'f1_score': f1_score(y_test, nn_test_pred),\n",
    "        'precision': precision_score(y_test, nn_test_pred),\n",
    "        'recall': recall_score(y_test, nn_test_pred),\n",
    "        'auc': roc_auc_score(y_test, nn_test_pred_proba)\n",
    "    }\n",
    "    \n",
    "    ensemble_test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, ensemble_test_pred),\n",
    "        'f1_score': f1_score(y_test, ensemble_test_pred),\n",
    "        'precision': precision_score(y_test, ensemble_test_pred),\n",
    "        'recall': recall_score(y_test, ensemble_test_pred),\n",
    "        'auc': roc_auc_score(y_test, ensemble_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(\"Test Set Performance:\")\n",
    "    print(\"\\nXGBoost:\")\n",
    "    for metric, value in xgb_test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nNeural Network:\")\n",
    "    for metric, value in nn_test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nEnsemble:\")\n",
    "    for metric, value in ensemble_test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return xgb_test_metrics, nn_test_metrics, ensemble_test_metrics\n",
    "\n",
    "# Evaluate models\n",
    "xgb_test_metrics, nn_test_metrics, ensemble_test_metrics = evaluate_models(\n",
    "    xgb_model, nn_model, X_test, y_test, ensemble_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_visualization",
   "metadata": {},
   "source": [
    "## 11. ğŸ“Š Results Visualization\n",
    "\n",
    "Visualize model performance and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison visualization\n",
    "print(\"ğŸ“Š Creating performance visualization...\")\n",
    "\n",
    "# Prepare data for plotting\n",
    "metrics = ['accuracy', 'f1_score', 'precision', 'recall', 'auc']\n",
    "xgb_scores = [xgb_test_metrics[m] for m in metrics]\n",
    "nn_scores = [nn_test_metrics[m] for m in metrics]\n",
    "ensemble_scores = [ensemble_test_metrics[m] for m in metrics]\n",
    "\n",
    "# Create bar plot\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width, xgb_scores, width, label='XGBoost', color='skyblue')\n",
    "bars2 = ax.bar(x, nn_scores, width, label='Neural Network', color='lightcoral')\n",
    "bars3 = ax.bar(x + width, ensemble_scores, width, label='Ensemble', color='lightgreen')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('FLIR+SCD41 Fire Detection Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_value_labels(bars1)\n",
    "add_value_labels(bars2)\n",
    "add_value_labels(bars3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for the ensemble model\n",
    "print(\"ğŸ“Š Creating confusion matrix...\")\n",
    "\n",
    "# Calculate ensemble predictions for confusion matrix\n",
    "xgb_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_data = torch.FloatTensor(X_test).to(device)\n",
    "    nn_outputs = nn_model(test_data)\n",
    "    nn_test_pred_proba = torch.softmax(nn_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "# Ensemble predictions (weighted average)\n",
    "ensemble_pred_proba = (\n",
    "    ensemble_weights[0] * xgb_test_pred_proba + \n",
    "    ensemble_weights[1] * nn_test_pred_proba\n",
    ")\n",
    "ensemble_test_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, ensemble_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Fire', 'Fire'], \n",
    "            yticklabels=['No Fire', 'Fire'])\n",
    "plt.title('Confusion Matrix - FLIR+SCD41 Ensemble Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report - FLIR+SCD41 Ensemble Model:\")\n",
    "print(classification_report(y_test, ensemble_test_pred, target_names=['No Fire', 'Fire']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_saving",
   "metadata": {},
   "source": [
    "## 12. ğŸ’¾ Model Saving\n",
    "\n",
    "Save all trained models and components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_and_results(df, xgb_model, nn_model, ensemble_weights, \n",
    "                          xgb_test_metrics, nn_test_metrics, ensemble_test_metrics, feature_names):\n",
    "    \"\"\"Save all models and results\"\"\"\n",
    "    print(\"ğŸ’¾ Saving models and results...\")\n",
    "    \n",
    "    # Create data directory\n",
    "    data_dir = os.path.join(project_root, 'data', 'flir_scd41')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Save dataset\n",
    "    dataset_path = os.path.join(data_dir, 'flir_scd41_dataset.csv')\n",
    "    df.to_csv(dataset_path, index=False)\n",
    "    \n",
    "    # Save XGBoost model\n",
    "    xgb_model_path = os.path.join(data_dir, 'flir_scd41_xgboost_model_improved.json')\n",
    "    xgb_model.save_model(xgb_model_path)\n",
    "    \n",
    "    # Neural Network model already saved during training\n",
    "    nn_model_path = os.path.join(data_dir, 'best_improved_nn_model.pth')\n",
    "    \n",
    "    # Save ensemble weights\n",
    "    weights_data = {\n",
    "        'models': ['xgboost', 'neural_network'],\n",
    "        'weights': ensemble_weights,\n",
    "        'validation_scores': {\n",
    "            'xgboost': xgb_test_metrics['accuracy'],\n",
    "            'neural_network': nn_test_metrics['accuracy']\n",
    "        },\n",
    "        'calculation_method': 'performance_based_exponential_scaling',\n",
    "        'scaling_factor': 2.0\n",
    "    }\n",
    "    \n",
    "    weights_path = os.path.join(data_dir, 'ensemble_weights_improved.json')\n",
    "    with open(weights_path, 'w') as f:\n",
    "        json.dump(weights_data, f, indent=2)\n",
    "    \n",
    "    # Save model information\n",
    "    model_info = {\n",
    "        'xgboost': {\n",
    "            'model_path': xgb_model_path,\n",
    "            'metrics': xgb_test_metrics\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'model_path': nn_model_path,\n",
    "            'metrics': nn_test_metrics\n",
    "        },\n",
    "        'ensemble': {\n",
    "            'weights_path': weights_path,\n",
    "            'metrics': ensemble_test_metrics\n",
    "        },\n",
    "        'feature_names': feature_names,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    model_info_path = os.path.join(data_dir, 'model_info_improved.json')\n",
    "    with open(model_info_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Dataset saved to {dataset_path}\")\n",
    "    print(f\"âœ… XGBoost model saved to {xgb_model_path}\")\n",
    "    print(f\"âœ… Neural Network model saved to {nn_model_path}\")\n",
    "    print(f\"âœ… Ensemble weights saved to {weights_path}\")\n",
    "    print(f\"âœ… Model information saved to {model_info_path}\")\n",
    "\n",
    "# Save models and results\n",
    "save_models_and_results(\n",
    "    df, xgb_model, nn_model, ensemble_weights,\n",
    "    xgb_test_metrics, nn_test_metrics, ensemble_test_metrics, feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_summary",
   "metadata": {},
   "source": [
    "## 13. ğŸ Training Summary and Diagnostics\n",
    "\n",
    "Summary of the entire training process with diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ Training Process Summary with Diagnostics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset Size: {len(df):,} samples\")\n",
    "print(f\"Features: {len(feature_names)} (15 FLIR + 3 SCD41)\")\n",
    "print(f\"Fire Samples: {sum(df['fire_detected'])} ({sum(df['fire_detected'])/len(df)*100:.2f}%)\")\n",
    "print(f\"Training Samples: {len(X_train):,}\")\n",
    "print(f\"Validation Samples: {len(X_val):,}\")\n",
    "print(f\"Test Samples: {len(X_test):,}\")\n",
    "print()\n",
    "print(\"Model Performance (Test Set):\")\n",
    "print(f\"  XGBoost Accuracy: {xgb_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Neural Network Accuracy: {nn_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Ensemble Accuracy: {ensemble_test_metrics['accuracy']:.4f}\")\n",
    "print()\n",
    "print(\"Ensemble Weights:\")\n",
    "print(f\"  XGBoost: {ensemble_weights[0]:.4f}\")\n",
    "print(f\"  Neural Network: {ensemble_weights[1]:.4f}\")\n",
    "print()\n",
    "# Diagnostics summary\n",
    "print(\"Diagnostics Summary:\")\n",
    "xgb_gap = xgb_train_metrics['accuracy'] - xgb_val_metrics['accuracy']\n",
    "nn_gap = nn_train_metrics['accuracy'] - nn_val_metrics['accuracy']\n",
    "print(f\"  XGBoost Train-Val Gap: {xgb_gap:.4f}\")\n",
    "print(f\"  Neural Network Train-Val Gap: {nn_gap:.4f}\")\n",
    "if xgb_gap > 0.1:\n",
    "    print(\"  âš ï¸  XGBoost may be overfitting\")\n",
    "elif xgb_gap < -0.1:\n",
    "    print(\"  âš ï¸  XGBoost may be underfitting\")\n",
    "else:\n",
    "    print(\"  âœ… XGBoost appears to be learning properly\")\n",
    "    \n",
    "if nn_gap > 0.1:\n",
    "    print(\"  âš ï¸  Neural Network may be overfitting\")\n",
    "elif nn_gap < -0.1:\n",
    "    print(\"  âš ï¸  Neural Network may be underfitting\")\n",
    "else:\n",
    "    print(\"  âœ… Neural Network appears to be learning properly\")\n",
    "print()\n",
    "print(\"Files Saved:\")\n",
    "print(f\"  Dataset: {os.path.join(data_dir, 'flir_scd41_dataset.csv')}\")\n",
    "print(f\"  Train Split: {os.path.join(data_dir, 'train.csv')}\")\n",
    "print(f\"  Validation Split: {os.path.join(data_dir, 'val.csv')}\")\n",
    "print(f\"  Test Split: {os.path.join(data_dir, 'test.csv')}\")\n",
    "print(f\"  XGBoost Model: {os.path.join(data_dir, 'flir_scd41_xgboost_model_improved.json')}\")\n",
    "print(f\"  Neural Network Model: {os.path.join(data_dir, 'best_improved_nn_model.pth')}\")\n",
    "print(f\"  Ensemble Weights: {os.path.join(data_dir, 'ensemble_weights_improved.json')}\")\n",
    "print(f\"  Model Info: {os.path.join(data_dir, 'model_info_improved.json')}\")\n",
    "print()\n",
    "print(\"âœ… End-to-end training pipeline with diagnostics completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_takeaways",
   "metadata": {},
   "source": [
    "## 14. ğŸ¯ Key Takeaways and Best Practices\n",
    "\n",
    "### Detecting Learning Issues\n",
    "1. **Compare training and validation performance**\n",
    "   - Large gap = overfitting\n",
    "   - Poor performance on both = underfitting\n",
    "\n",
    "2. **Use learning curves**\n",
    "   - Converge to low scores = underfitting\n",
    "   - Large gap = overfitting\n",
    "   - Converge to high scores with small gap = good fit\n",
    "\n",
    "3. **Use validation curves**\n",
    "   - Find optimal hyperparameters\n",
    "   - Identify when increasing complexity hurts performance\n",
    "\n",
    "### Preventing Overfitting\n",
    "1. **Regularization**\n",
    "   - L1/L2 regularization\n",
    "   - Dropout layers\n",
    "   - Early stopping\n",
    "\n",
    "2. **Model Architecture**\n",
    "   - Reduce model complexity\n",
    "   - Use appropriate depth/width\n",
    "\n",
    "3. **Data Techniques**\n",
    "   - Data augmentation\n",
    "   - Cross-validation\n",
    "   - More training data\n",
    "\n",
    "### Ensuring Proper Learning\n",
    "1. **Data Quality**\n",
    "   - Check for data leakage\n",
    "   - Ensure sufficient variance in features\n",
    "   - Balance class distributions\n",
    "\n",
    "2. **Model Configuration**\n",
    "   - Appropriate learning rate\n",
    "   - Sufficient training time\n",
    "   - Proper feature scaling\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Use separate test set\n",
    "   - Cross-validation for robust estimates\n",
    "   - Multiple metrics (accuracy, F1, precision, recall)\n",
    "\n",
    "### For the FLIR+SCD41 Fire Detection System\n",
    "These techniques ensure that our models:\n",
    "1. Learn meaningful patterns from thermal and gas sensor data\n",
    "2. Generalize well to new fire detection scenarios\n",
    "3. Maintain high accuracy while avoiding overfitting to synthetic data\n",
    "4. Provide reliable fire detection in real-world deployments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
