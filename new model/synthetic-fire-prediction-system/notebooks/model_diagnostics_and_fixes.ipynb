{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# üîç FLIR+SCD41 Model Diagnostics and Fixes\n",
    "\n",
    "This notebook demonstrates how to diagnose and fix common machine learning problems:\n",
    "1. Model not learning\n",
    "2. Overfitting (remembering patterns instead of learning)\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### Model Not Learning\n",
    "- **Symptoms**: Poor performance on both training and validation sets\n",
    "- **Causes**: \n",
    "  - Insufficient model capacity\n",
    "  - Poor data quality\n",
    "  - Inappropriate learning rate\n",
    "  - Feature scaling issues\n",
    "\n",
    "### Overfitting\n",
    "- **Symptoms**: High performance on training set, poor performance on validation/test sets\n",
    "- **Causes**:\n",
    "  - Model too complex for the data\n",
    "  - Insufficient regularization\n",
    "  - Too little training data\n",
    "  - Data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, learning_curve, validation_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_generation",
   "metadata": {},
   "source": [
    "## 1. üîÑ Generate Sample Data\n",
    "\n",
    "Create synthetic data for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "print(\"üîÑ Generating synthetic dataset for demonstration...\")\n",
    "\n",
    "# Generate data using numpy for simplicity\n",
    "np.random.seed(42)\n",
    "num_samples = 2000  # Smaller dataset for demo\n",
    "\n",
    "# Generate features\n",
    "X = np.random.randn(num_samples, 18)\n",
    "\n",
    "# Create labels with a clear pattern (first 5 features are important)\n",
    "y_prob = 1 / (1 + np.exp(-(\n",
    "    2*X[:, 0] + 1.5*X[:, 1] + X[:, 2] + \n",
    "    0.5*X[:, 3] + 0.3*X[:, 4] + \n",
    "    0.1*np.sum(X[:, 5:], axis=1) + \n",
    "    np.random.normal(0, 0.1, num_samples)\n",
    ")))\n",
    "y = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Generated {num_samples} samples with 18 features\")\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution - Train: {np.bincount(y_train)}\")\n",
    "print(f\"Class distribution - Validation: {np.bincount(y_val)}\")\n",
    "print(f\"Class distribution - Test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnose_not_learning",
   "metadata": {},
   "source": [
    "## 2. üîç Diagnose Model Not Learning\n",
    "\n",
    "Train a model that's too simple and diagnose the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underfitting_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a model that's too simple (underfitting)\n",
    "print(\"üß™ Example: Model that's too simple (underfitting)\")\n",
    "\n",
    "# Create a very simple model (too simple for the data)\n",
    "simple_model = xgb.XGBClassifier(\n",
    "    n_estimators=10,      # Very few trees\n",
    "    max_depth=1,          # Very shallow trees\n",
    "    learning_rate=0.01,   # Very low learning rate\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "simple_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on train and test sets\n",
    "train_pred = simple_model.predict(X_train)\n",
    "val_pred = simple_model.predict(X_val)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "print(f\"Simple Model Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Gap: {abs(train_acc - val_acc):.4f}\")\n",
    "\n",
    "# Diagnosis\n",
    "if train_acc < 0.7 and val_acc < 0.7:\n",
    "    print(\"\\nü§î Diagnosis: Model is not learning properly (underfitting)\")\n",
    "    print(\"üí° Solutions:\")\n",
    "    print(\"   1. Increase model complexity (more trees, deeper trees)\")\n",
    "    print(\"   2. Increase learning rate\")\n",
    "    print(\"   3. Add more relevant features\")\n",
    "    print(\"   4. Try a different algorithm\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Model appears to be learning properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnose_overfitting",
   "metadata": {},
   "source": [
    "## 3. üîç Diagnose Overfitting\n",
    "\n",
    "Train a model that's too complex and diagnose overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overfitting_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a model that's too complex (overfitting)\n",
    "print(\"üß™ Example: Model that's too complex (overfitting)\")\n",
    "\n",
    "# Create a very complex model\n",
    "complex_model = xgb.XGBClassifier(\n",
    "    n_estimators=1000,    # Many trees\n",
    "    max_depth=10,         # Deep trees\n",
    "    learning_rate=0.3,    # High learning rate\n",
    "    subsample=1.0,        # No subsampling\n",
    "    colsample_bytree=1.0, # No column subsampling\n",
    "    reg_alpha=0,          # No L1 regularization\n",
    "    reg_lambda=0,         # No L2 regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "complex_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on train and test sets\n",
    "train_pred = complex_model.predict(X_train)\n",
    "val_pred = complex_model.predict(X_val)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "print(f\"Complex Model Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "# Diagnosis\n",
    "if (train_acc - val_acc) > 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è  Diagnosis: Model is overfitting (remembering patterns)\")\n",
    "    print(\"üí° Solutions:\")\n",
    "    print(\"   1. Add regularization (reg_alpha, reg_lambda)\")\n",
    "    print(\"   2. Reduce model complexity (max_depth, n_estimators)\")\n",
    "    print(\"   3. Use early stopping\")\n",
    "    print(\"   4. Add dropout or other regularization techniques\")\n",
    "    print(\"   5. Get more training data\")\n",
    "    print(\"   6. Use cross-validation for better evaluation\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Model does not appear to be overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning_curves",
   "metadata": {},
   "source": [
    "## 4. üìà Learning Curves Visualization\n",
    "\n",
    "Use learning curves to diagnose bias and variance problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning_curves_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves\n",
    "print(\"üìä Generating learning curves...\")\n",
    "\n",
    "# Simple model learning curve\n",
    "simple_model_lc = xgb.XGBClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=1,\n",
    "    learning_rate=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    simple_model_lc, X_train, y_train, cv=5, n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Simple model\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation score')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Learning Curves - Simple Model (Underfitting)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "# Complex model\n",
    "complex_model_lc = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    complex_model_lc, X_train, y_train, cv=5, n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation score')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Learning Curves - Complex Model (Overfitting)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning curves help diagnose:\")\n",
    "print(\"1. High bias (underfitting): Both curves converge to low scores\")\n",
    "print(\"2. High variance (overfitting): Large gap between curves\")\n",
    "print(\"3. Good fit: Both curves converge to high scores with small gap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation_curves",
   "metadata": {},
   "source": [
    "## 5. üìà Validation Curves\n",
    "\n",
    "Use validation curves to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation_curves_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate validation curves\n",
    "print(\"üìä Generating validation curves...\")\n",
    "\n",
    "# Validation curve for max_depth\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_scores, val_scores = validation_curve(\n",
    "    model, X_train, y_train,\n",
    "    param_name='max_depth', param_range=range(1, 11),\n",
    "    cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot validation curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), train_mean, 'o-', color='blue', label='Training score')\n",
    "plt.fill_between(range(1, 11), train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.plot(range(1, 11), val_mean, 'o-', color='red', label='Validation score')\n",
    "plt.fill_between(range(1, 11), val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Validation Curves - Max Depth')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal parameter\n",
    "optimal_idx = np.argmax(val_mean)\n",
    "optimal_max_depth = range(1, 11)[optimal_idx]\n",
    "print(f\"Optimal max_depth: {optimal_max_depth}\")\n",
    "print(f\"Best validation score: {val_mean[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "properly_tuned_model",
   "metadata": {},
   "source": [
    "## 6. ‚úÖ Properly Tuned Model\n",
    "\n",
    "Train a model with proper regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "well_tuned_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a well-tuned model with regularization\n",
    "print(\"‚úÖ Training a well-tuned model with regularization...\")\n",
    "\n",
    "# Well-tuned model with regularization\n",
    "tuned_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,         # Reasonable number of trees\n",
    "    max_depth=4,              # Moderate depth\n",
    "    learning_rate=0.1,        # Moderate learning rate\n",
    "    subsample=0.8,            # Subsampling to prevent overfitting\n",
    "    colsample_bytree=0.8,     # Column subsampling\n",
    "    reg_alpha=0.1,            # L1 regularization\n",
    "    reg_lambda=1.0,           # L2 regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on all sets\n",
    "train_pred = tuned_model.predict(X_train)\n",
    "val_pred = tuned_model.predict(X_val)\n",
    "test_pred = tuned_model.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(f\"Well-Tuned Model Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Train-Val Gap: {train_acc - val_acc:.4f}\")\n",
    "\n",
    "# Diagnosis\n",
    "if train_acc > 0.8 and val_acc > 0.8 and (train_acc - val_acc) < 0.1:\n",
    "    print(\"\\nüéâ Success: Model is well-tuned and properly learning!\")\n",
    "    print(\"   - Good performance on training set\")\n",
    "    print(\"   - Good performance on validation set\")\n",
    "    print(\"   - Small gap between training and validation\")\n",
    "    print(\"   - Generalizes well to test set\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Model may still need tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural_network_example",
   "metadata": {},
   "source": [
    "## 7. üß† Neural Network Overfitting Example\n",
    "\n",
    "Demonstrate overfitting and regularization in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn_overfitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network without regularization (prone to overfitting)\n",
    "print(\"üß† Neural Network Overfitting Example\")\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=18, hidden_size=128):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cpu')\n",
    "simple_nn = SimpleNN().to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(simple_nn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (no regularization, prone to overfitting)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Training\n",
    "    simple_nn.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = simple_nn(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_pred = torch.argmax(outputs, dim=1)\n",
    "    train_acc = accuracy_score(y_train_tensor, train_pred)\n",
    "    \n",
    "    # Validation\n",
    "    simple_nn.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = simple_nn(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        val_pred = torch.argmax(val_outputs, dim=1)\n",
    "        val_acc = accuracy_score(y_val_tensor, val_pred)\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Print progress every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100]')\n",
    "        print(f'  Train Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "\n",
    "# Plot training and validation curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves - Simple NN (Overfitting)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curves - Simple NN (Overfitting)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for overfitting\n",
    "final_train_acc = train_accuracies[-1]\n",
    "final_val_acc = val_accuracies[-1]\n",
    "gap = final_train_acc - final_val_acc\n",
    "\n",
    "print(f\"\\nSimple NN Final Performance:\")\n",
    "print(f\"  Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"  Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"  Gap: {gap:.4f}\")\n",
    "\n",
    "if gap > 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è  Diagnosis: Neural Network is overfitting!\")\n",
    "    print(\"üí° Solutions:\")\n",
    "    print(\"   1. Add dropout layers\")\n",
    "    print(\"   2. Add L1/L2 regularization\")\n",
    "    print(\"   3. Reduce network complexity\")\n",
    "    print(\"   4. Use early stopping\")\n",
    "    print(\"   5. Data augmentation\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Neural Network does not appear to be overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularized_nn",
   "metadata": {},
   "source": [
    "## 8. ‚úÖ Regularized Neural Network\n",
    "\n",
    "Train a neural network with proper regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regularized_nn_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized neural network with dropout and L2 regularization\n",
    "print(\"‚úÖ Regularized Neural Network Example\")\n",
    "\n",
    "class RegularizedNN(nn.Module):\n",
    "    def __init__(self, input_size=18, hidden_sizes=[64, 32], dropout_rate=0.3):\n",
    "        super(RegularizedNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Hidden layers with dropout\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, 2))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create model with regularization\n",
    "regularized_nn = RegularizedNN(dropout_rate=0.3).to(device)\n",
    "\n",
    "# Training setup with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(regularized_nn.parameters(), lr=0.001, weight_decay=1e-4)  # L2 regularization\n",
    "\n",
    "# Training loop with early stopping\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Training\n",
    "    regularized_nn.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = regularized_nn(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_pred = torch.argmax(outputs, dim=1)\n",
    "    train_acc = accuracy_score(y_train_tensor, train_pred)\n",
    "    \n",
    "    # Validation\n",
    "    regularized_nn.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = regularized_nn(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        val_pred = torch.argmax(val_outputs, dim=1)\n",
    "        val_acc = accuracy_score(y_val_tensor, val_pred)\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Print progress every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{100}]')\n",
    "        print(f'  Train Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "\n",
    "# Plot training and validation curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves - Regularized NN')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curves - Regularized NN')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for overfitting\n",
    "final_train_acc = train_accuracies[-1]\n",
    "final_val_acc = val_accuracies[-1]\n",
    "gap = final_train_acc - final_val_acc\n",
    "\n",
    "print(f\"\\nRegularized NN Final Performance:\")\n",
    "print(f\"  Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"  Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"  Gap: {gap:.4f}\")\n",
    "\n",
    "if gap <= 0.1:\n",
    "    print(\"\\nüéâ Success: Regularized Neural Network is properly learning!\")\n",
    "    print(\"   - Good performance on training set\")\n",
    "    print(\"   - Good performance on validation set\")\n",
    "    print(\"   - Small gap between training and validation\")\n",
    "    print(\"   - Early stopping prevented overfitting\")\n",
    "    print(\"   - Dropout and L2 regularization helped generalization\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Neural Network may still be overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 9. üéØ Key Takeaways\n",
    "\n",
    "### Detecting Learning Issues\n",
    "1. **Compare training and validation performance**\n",
    "   - Large gap = overfitting\n",
    "   - Poor performance on both = underfitting\n",
    "\n",
    "2. **Use learning curves**\n",
    "   - Converge to low scores = underfitting\n",
    "   - Large gap = overfitting\n",
    "   - Converge to high scores with small gap = good fit\n",
    "\n",
    "3. **Use validation curves**\n",
    "   - Find optimal hyperparameters\n",
    "   - Identify when increasing complexity hurts performance\n",
    "\n",
    "### Preventing Overfitting\n",
    "1. **Regularization**\n",
    "   - L1/L2 regularization\n",
    "   - Dropout layers\n",
    "   - Early stopping\n",
    "\n",
    "2. **Model Architecture**\n",
    "   - Reduce model complexity\n",
    "   - Use appropriate depth/width\n",
    "\n",
    "3. **Data Techniques**\n",
    "   - Data augmentation\n",
    "   - Cross-validation\n",
    "   - More training data\n",
    "\n",
    "### Ensuring Proper Learning\n",
    "1. **Data Quality**\n",
    "   - Check for data leakage\n",
    "   - Ensure sufficient variance in features\n",
    "   - Balance class distributions\n",
    "\n",
    "2. **Model Configuration**\n",
    "   - Appropriate learning rate\n",
    "   - Sufficient training time\n",
    "   - Proper feature scaling\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Use separate test set\n",
    "   - Cross-validation for robust estimates\n",
    "   - Multiple metrics (accuracy, F1, precision, recall)\n",
    "\n",
    "For the FLIR+SCD41 fire detection system, these techniques are implemented in:\n",
    "- `scripts/improved_flir_scd41_training.py`\n",
    "- `scripts/model_diagnostics.py`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
