{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c7d3d9",
   "metadata": {},
   "source": [
    "# 🔥 FLIR+SCD41 Fire Detection System - Complete End-to-End Training Pipeline\n",
    "\n",
    "This notebook demonstrates the complete workflow for training the FLIR+SCD41 fire detection system:\n",
    "1. Dataset generation\n",
    "2. Data storage\n",
    "3. Data splitting\n",
    "4. Model training\n",
    "5. Ensemble weight calculation\n",
    "6. Model evaluation\n",
    "\n",
    "## System Overview\n",
    "The system uses:\n",
    "- FLIR Lepton 3.5 thermal camera (15 features)\n",
    "- Sensirion SCD41 CO₂ sensor (3 features)\n",
    "- Total: 18 features for fire detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_generation",
   "metadata": {},
   "source": [
    "## 1. 🔄 Dataset Generation\n",
    "\n",
    "Generate synthetic training data for FLIR+SCD41 sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples=10000):\n",
    "    \"\"\"Generate synthetic FLIR+SCD41 dataset\"\"\"\n",
    "    print(\"🔄 Generating synthetic FLIR+SCD41 dataset...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate FLIR features (15 features)\n",
    "    flir_features = np.random.normal(25, 10, (num_samples, 15))\n",
    "    flir_features[:, 0] = np.clip(flir_features[:, 0], -40, 330)  # t_mean: -40 to 330°C\n",
    "    flir_features[:, 2] = np.clip(flir_features[:, 2], -40, 330)  # t_max: -40 to 330°C\n",
    "    flir_features[:, 4] = np.clip(flir_features[:, 4], 0, 100)    # t_hot_area_pct: 0-100%\n",
    "    \n",
    "    # Generate SCD41 features (3 features)\n",
    "    scd41_features = np.random.normal(450, 100, (num_samples, 3))\n",
    "    scd41_features[:, 0] = np.clip(scd41_features[:, 0], 400, 40000)  # gas_val: 400-40000 ppm\n",
    "    \n",
    "    print(f\"✅ Generated {num_samples} samples\")\n",
    "    print(f\"FLIR features shape: {flir_features.shape}\")\n",
    "    print(f\"SCD41 features shape: {scd41_features.shape}\")\n",
    "    \n",
    "    return flir_features, scd41_features\n",
    "\n",
    "# Generate synthetic data\n",
    "flir_features, scd41_features = generate_synthetic_data(num_samples=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_storage",
   "metadata": {},
   "source": [
    "## 2. 💾 Data Storage\n",
    "\n",
    "Combine features and create labels, then save the dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(flir_features, scd41_features):\n",
    "    \"\"\"Combine features and create labels\"\"\"\n",
    "    print(\"💾 Combining features and creating dataset...\")\n",
    "    \n",
    "    # Combine all features (15 FLIR + 3 SCD41 = 18 features)\n",
    "    all_features = np.concatenate([flir_features, scd41_features], axis=1)\n",
    "    \n",
    "    # Create labels (fire detected or not)\n",
    "    # Create synthetic labels based on feature values\n",
    "    fire_probability = (\n",
    "        (flir_features[:, 2] > 60).astype(int) * 0.4 +  # High max temperature\n",
    "        (scd41_features[:, 0] > 1000).astype(int) * 0.4 +  # High CO2\n",
    "        (flir_features[:, 4] > 10).astype(int) * 0.2  # Large hot area\n",
    "    )\n",
    "    labels = np.random.binomial(1, np.clip(fire_probability, 0, 1))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_names = [\n",
    "        't_mean', 't_std', 't_max', 't_p95', 't_hot_area_pct',\n",
    "        't_hot_largest_blob_pct', 't_grad_mean', 't_grad_std',\n",
    "        't_diff_mean', 't_diff_std', 'flow_mag_mean', 'flow_mag_std',\n",
    "        'tproxy_val', 'tproxy_delta', 'tproxy_vel',\n",
    "        'gas_val', 'gas_delta', 'gas_vel'\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(all_features, columns=feature_names)\n",
    "    df['fire_detected'] = labels\n",
    "    \n",
    "    print(f\"✅ Dataset created with shape: {df.shape}\")\n",
    "    print(f\"Fire samples: {sum(labels)} ({sum(labels)/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    return df, feature_names\n",
    "\n",
    "# Create dataset\n",
    "df, feature_names = create_dataset(flir_features, scd41_features)\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to disk\n",
    "data_dir = os.path.join(project_root, 'data', 'flir_scd41')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "dataset_path = os.path.join(data_dir, 'flir_scd41_dataset.csv')\n",
    "df.to_csv(dataset_path, index=False)\n",
    "\n",
    "print(f\"✅ Dataset saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_splitting",
   "metadata": {},
   "source": [
    "## 3. 📊 Data Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, feature_names):\n",
    "    \"\"\"Split dataset into train/validation/test sets\"\"\"\n",
    "    print(\"📊 Splitting dataset into train/validation/test sets...\")\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = df.drop('fire_detected', axis=1).values\n",
    "    y = df['fire_detected'].values\n",
    "    \n",
    "    # Split into train (70%), validation (15%), test (15%)\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits to disk\n",
    "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "train_df['fire_detected'] = y_train\n",
    "train_df.to_csv(os.path.join(data_dir, 'train.csv'), index=False)\n",
    "\n",
    "val_df = pd.DataFrame(X_val, columns=feature_names)\n",
    "val_df['fire_detected'] = y_val\n",
    "val_df.to_csv(os.path.join(data_dir, 'val.csv'), index=False)\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "test_df['fire_detected'] = y_test\n",
    "test_df.to_csv(os.path.join(data_dir, 'test.csv'), index=False)\n",
    "\n",
    "print(\"✅ Dataset splits saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training",
   "metadata": {},
   "source": [
    "## 4. 🚀 Model Training\n",
    "\n",
    "Train multiple models: XGBoost and Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgboost_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train XGBoost model\"\"\"\n",
    "    print(\"🚀 Training XGBoost model...\")\n",
    "    \n",
    "    # Create and train XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate XGBoost model\n",
    "    xgb_train_pred = xgb_model.predict(X_train)\n",
    "    xgb_val_pred = xgb_model.predict(X_val)\n",
    "    xgb_val_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    xgb_train_metrics = {\n",
    "        'accuracy': accuracy_score(y_train, xgb_train_pred),\n",
    "        'f1_score': f1_score(y_train, xgb_train_pred),\n",
    "        'precision': precision_score(y_train, xgb_train_pred),\n",
    "        'recall': recall_score(y_train, xgb_train_pred)\n",
    "    }\n",
    "    \n",
    "    xgb_val_metrics = {\n",
    "        'accuracy': accuracy_score(y_val, xgb_val_pred),\n",
    "        'f1_score': f1_score(y_val, xgb_val_pred),\n",
    "        'precision': precision_score(y_val, xgb_val_pred),\n",
    "        'recall': recall_score(y_val, xgb_val_pred),\n",
    "        'auc': roc_auc_score(y_val, xgb_val_pred_proba) if len(np.unique(y_val)) > 1 else 0.0\n",
    "    }\n",
    "    \n",
    "    print(\"XGBoost Training Metrics:\")\n",
    "    for metric, value in xgb_train_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nXGBoost Validation Metrics:\")\n",
    "    for metric, value in xgb_val_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return xgb_model, xgb_val_metrics\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model, xgb_val_metrics = train_xgboost_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network model\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Simple neural network for fire detection\"\"\"\n",
    "    def __init__(self, input_size=18, hidden_size=64, num_classes=2):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define dataset class\n",
    "class FireDataset(Dataset):\n",
    "    \"\"\"Dataset for fire detection\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train neural network model\"\"\"\n",
    "    print(\"\\n🚀 Training Neural Network model...\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    nn_model = SimpleNN(input_size=18).to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = FireDataset(X_train, y_train)\n",
    "    val_dataset = FireDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 30\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        nn_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # Validation\n",
    "        nn_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                \n",
    "                outputs = nn_model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model_path = os.path.join(data_dir, 'best_nn_model.pth')\n",
    "            torch.save(nn_model.state_dict(), model_path)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'  Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "            print(f'  Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "    \n",
    "    # Final NN metrics\n",
    "    nn_val_metrics = {\n",
    "        'accuracy': best_val_acc,\n",
    "        'f1_score': f1_score(val_targets, val_preds),\n",
    "        'precision': precision_score(val_targets, val_preds),\n",
    "        'recall': recall_score(val_targets, val_preds)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nNeural Network Validation Metrics:\")\n",
    "    for metric, value in nn_val_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return nn_model, nn_val_metrics\n",
    "\n",
    "# Train Neural Network model\n",
    "nn_model, nn_val_metrics = train_neural_network(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ensemble_weights",
   "metadata": {},
   "source": [
    "## 5. ⚖️ Ensemble Weight Calculation\n",
    "\n",
    "Calculate optimal weights for model ensemble based on validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ensemble_weights(xgb_score, nn_score):\n",
    "    \"\"\"Calculate ensemble weights based on validation performance\"\"\"\n",
    "    print(\"⚖️ Calculating ensemble weights...\")\n",
    "    \n",
    "    print(f\"XGBoost validation accuracy: {xgb_score:.4f}\")\n",
    "    print(f\"Neural Network validation accuracy: {nn_score:.4f}\")\n",
    "    \n",
    "    # Method: Performance-based weighting (exponential scaling)\n",
    "    def calculate_performance_weights(scores, scaling_factor=2.0):\n",
    "        \"\"\"Calculate weights based on performance scores using exponential scaling\"\"\"\n",
    "        # Normalize scores to [0, 1] range\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        \n",
    "        if max_score == min_score:\n",
    "            # All models have same performance, equal weights\n",
    "            return [1.0/len(scores)] * len(scores)\n",
    "        \n",
    "        normalized_scores = [(score - min_score) / (max_score - min_score) for score in scores]\n",
    "        \n",
    "        # Apply exponential scaling\n",
    "        weighted_scores = [np.exp(scaling_factor * score) for score in normalized_scores]\n",
    "        \n",
    "        # Normalize to sum to 1\n",
    "        total_weight = sum(weighted_scores)\n",
    "        weights = [w / total_weight for w in weighted_scores]\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    # Calculate weights\n",
    "    model_scores = [xgb_score, nn_score]\n",
    "    ensemble_weights = calculate_performance_weights(model_scores)\n",
    "    \n",
    "    print(f\"\\nEnsemble weights:\")\n",
    "    print(f\"  XGBoost weight: {ensemble_weights[0]:.4f}\")\n",
    "    print(f\"  Neural Network weight: {ensemble_weights[1]:.4f}\")\n",
    "    \n",
    "    return ensemble_weights\n",
    "\n",
    "# Calculate ensemble weights\n",
    "ensemble_weights = calculate_ensemble_weights(\n",
    "    xgb_val_metrics['accuracy'], \n",
    "    nn_val_metrics['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_evaluation",
   "metadata": {},
   "source": [
    "## 6. 🧪 Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the ensemble model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(xgb_model, nn_model, X_test, y_test, ensemble_weights):\n",
    "    \"\"\"Evaluate models on test set\"\"\"\n",
    "    print(\"🧪 Evaluating models on test set...\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # XGBoost predictions\n",
    "    xgb_test_pred = xgb_model.predict(X_test)\n",
    "    xgb_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Neural Network predictions\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_data = torch.FloatTensor(X_test).to(device)\n",
    "        nn_outputs = nn_model(test_data)\n",
    "        nn_test_pred_proba = torch.softmax(nn_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        nn_test_pred = (nn_test_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Ensemble predictions (weighted average)\n",
    "    ensemble_pred_proba = (\n",
    "        ensemble_weights[0] * xgb_test_pred_proba + \n",
    "        ensemble_weights[1] * nn_test_pred_proba\n",
    "    )\n",
    "    ensemble_test_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    xgb_test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, xgb_test_pred),\n",
    "        'f1_score': f1_score(y_test, xgb_test_pred),\n",
    "        'precision': precision_score(y_test, xgb_test_pred),\n",
    "        'recall': recall_score(y_test, xgb_test_pred),\n",
    "        'auc': roc_auc_score(y_test, xgb_test_pred_proba)\n",
    "    }\n",
    "    \n",
    "    nn_test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, nn_test_pred),\n",
    "        'f1_score': f1_score(y_test, nn_test_pred),\n",
    "        'precision': precision_score(y_test, nn_test_pred),\n",
    "        'recall': recall_score(y_test, nn_test_pred),\n",
    "        'auc': roc_auc_score(y_test, nn_test_pred_proba)\n",
    "    }\n",
    "    \n",
    "    ensemble_test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, ensemble_test_pred),\n",
    "        'f1_score': f1_score(y_test, ensemble_test_pred),\n",
    "        'precision': precision_score(y_test, ensemble_test_pred),\n",
    "        'recall': recall_score(y_test, ensemble_test_pred),\n",
    "        'auc': roc_auc_score(y_test, ensemble_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(\"Test Set Performance:\")\n",
    "    print(\"\\nXGBoost:\")\n",
    "    for metric, value in xgb_test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nNeural Network:\")\n",
    "    for metric, value in nn_test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nEnsemble:\")\n",
    "    for metric, value in ensemble_test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return xgb_test_metrics, nn_test_metrics, ensemble_test_metrics\n",
    "\n",
    "# Evaluate models\n",
    "xgb_test_metrics, nn_test_metrics, ensemble_test_metrics = evaluate_models(\n",
    "    xgb_model, nn_model, X_test, y_test, ensemble_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "## 7. 📊 Results Visualization\n",
    "\n",
    "Visualize model performance and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison visualization\n",
    "print(\"📊 Creating performance visualization...\")\n",
    "\n",
    "# Prepare data for plotting\n",
    "metrics = ['accuracy', 'f1_score', 'precision', 'recall', 'auc']\n",
    "xgb_scores = [xgb_test_metrics[m] for m in metrics]\n",
    "nn_scores = [nn_test_metrics[m] for m in metrics]\n",
    "ensemble_scores = [ensemble_test_metrics[m] for m in metrics]\n",
    "\n",
    "# Create bar plot\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width, xgb_scores, width, label='XGBoost', color='skyblue')\n",
    "bars2 = ax.bar(x, nn_scores, width, label='Neural Network', color='lightcoral')\n",
    "bars3 = ax.bar(x + width, ensemble_scores, width, label='Ensemble', color='lightgreen')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('FLIR+SCD41 Fire Detection Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_value_labels(bars1)\n",
    "add_value_labels(bars2)\n",
    "add_value_labels(bars3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for the ensemble model\n",
    "print(\"📊 Creating confusion matrix...\")\n",
    "\n",
    "# Calculate ensemble predictions for confusion matrix\n",
    "xgb_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_data = torch.FloatTensor(X_test).to(device)\n",
    "    nn_outputs = nn_model(test_data)\n",
    "    nn_test_pred_proba = torch.softmax(nn_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "# Ensemble predictions (weighted average)\n",
    "ensemble_pred_proba = (\n",
    "    ensemble_weights[0] * xgb_test_pred_proba + \n",
    "    ensemble_weights[1] * nn_test_pred_proba\n",
    ")\n",
    "ensemble_test_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, ensemble_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Fire', 'Fire'], \n",
    "            yticklabels=['No Fire', 'Fire'])\n",
    "plt.title('Confusion Matrix - FLIR+SCD41 Ensemble Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report - FLIR+SCD41 Ensemble Model:\")\n",
    "print(classification_report(y_test, ensemble_test_pred, target_names=['No Fire', 'Fire']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_saving",
   "metadata": {},
   "source": [
    "## 8. 💾 Model Saving\n",
    "\n",
    "Save all trained models and components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_and_results(df, xgb_model, nn_model, ensemble_weights, \n",
    "                          xgb_test_metrics, nn_test_metrics, ensemble_test_metrics, feature_names):\n",
    "    \"\"\"Save all models and results\"\"\"\n",
    "    print(\"💾 Saving models and results...\")\n",
    "    \n",
    "    # Create data directory\n",
    "    data_dir = os.path.join(project_root, 'data', 'flir_scd41')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Save dataset\n",
    "    dataset_path = os.path.join(data_dir, 'flir_scd41_dataset.csv')\n",
    "    df.to_csv(dataset_path, index=False)\n",
    "    \n",
    "    # Save XGBoost model\n",
    "    xgb_model_path = os.path.join(data_dir, 'flir_scd41_xgboost_model.json')\n",
    "    xgb_model.save_model(xgb_model_path)\n",
    "    \n",
    "    # Neural Network model already saved during training\n",
    "    nn_model_path = os.path.join(data_dir, 'best_nn_model.pth')\n",
    "    \n",
    "    # Save ensemble weights\n",
    "    weights_data = {\n",
    "        'models': ['xgboost', 'neural_network'],\n",
    "        'weights': ensemble_weights,\n",
    "        'validation_scores': {\n",
    "            'xgboost': xgb_test_metrics['accuracy'],\n",
    "            'neural_network': nn_test_metrics['accuracy']\n",
    "        },\n",
    "        'calculation_method': 'performance_based_exponential_scaling',\n",
    "        'scaling_factor': 2.0\n",
    "    }\n",
    "    \n",
    "    weights_path = os.path.join(data_dir, 'ensemble_weights.json')\n",
    "    with open(weights_path, 'w') as f:\n",
    "        json.dump(weights_data, f, indent=2)\n",
    "    \n",
    "    # Save model information\n",
    "    model_info = {\n",
    "        'xgboost': {\n",
    "            'model_path': xgb_model_path,\n",
    "            'metrics': xgb_test_metrics\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'model_path': nn_model_path,\n",
    "            'metrics': nn_test_metrics\n",
    "        },\n",
    "        'ensemble': {\n",
    "            'weights_path': weights_path,\n",
    "            'metrics': ensemble_test_metrics\n",
    "        },\n",
    "        'feature_names': feature_names,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    model_info_path = os.path.join(data_dir, 'model_info.json')\n",
    "    with open(model_info_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Dataset saved to {dataset_path}\")\n",
    "    print(f\"✅ XGBoost model saved to {xgb_model_path}\")\n",
    "    print(f\"✅ Neural Network model saved to {nn_model_path}\")\n",
    "    print(f\"✅ Ensemble weights saved to {weights_path}\")\n",
    "    print(f\"✅ Model information saved to {model_info_path}\")\n",
    "\n",
    "# Save models and results\n",
    "save_models_and_results(\n",
    "    df, xgb_model, nn_model, ensemble_weights,\n",
    "    xgb_test_metrics, nn_test_metrics, ensemble_test_metrics, feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_summary",
   "metadata": {},
   "source": [
    "## 9. 🏁 Training Summary\n",
    "\n",
    "Summary of the entire training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏁 Training Process Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset Size: {len(df):,} samples\")\n",
    "print(f\"Features: {len(feature_names)} (15 FLIR + 3 SCD41)\")\n",
    "print(f\"Fire Samples: {sum(df['fire_detected'])} ({sum(df['fire_detected'])/len(df)*100:.2f}%)\")\n",
    "print(f\"Training Samples: {len(X_train):,}\")\n",
    "print(f\"Validation Samples: {len(X_val):,}\")\n",
    "print(f\"Test Samples: {len(X_test):,}\")\n",
    "print()\n",
    "print(\"Model Performance (Test Set):\")\n",
    "print(f\"  XGBoost Accuracy: {xgb_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Neural Network Accuracy: {nn_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Ensemble Accuracy: {ensemble_test_metrics['accuracy']:.4f}\")\n",
    "print()\n",
    "print(\"Ensemble Weights:\")\n",
    "print(f\"  XGBoost: {ensemble_weights[0]:.4f}\")\n",
    "print(f\"  Neural Network: {ensemble_weights[1]:.4f}\")\n",
    "print()\n",
    "print(\"Files Saved:\")\n",
    "print(f\"  Dataset: {os.path.join(data_dir, 'flir_scd41_dataset.csv')}\")\n",
    "print(f\"  Train Split: {os.path.join(data_dir, 'train.csv')}\")\n",
    "print(f\"  Validation Split: {os.path.join(data_dir, 'val.csv')}\")\n",
    "print(f\"  Test Split: {os.path.join(data_dir, 'test.csv')}\")\n",
    "print(f\"  XGBoost Model: {os.path.join(data_dir, 'flir_scd41_xgboost_model.json')}\")\n",
    "print(f\"  Neural Network Model: {os.path.join(data_dir, 'best_nn_model.pth')}\")\n",
    "print(f\"  Ensemble Weights: {os.path.join(data_dir, 'ensemble_weights.json')}\")\n",
    "print(f\"  Model Info: {os.path.join(data_dir, 'model_info.json')}\")\n",
    "print()\n",
    "print(\"✅ End-to-end training pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "Now that you've successfully trained the FLIR+SCD41 fire detection models, you can:\n",
    "\n",
    "1. **Deploy the models** using the saved files in the `data/flir_scd41` directory\n",
    "2. **Use the models for inference** by loading the saved models and applying them to new data\n",
    "3. **Fine-tune the models** with real-world data to improve performance\n",
    "4. **Integrate with the alert system** to generate real-time fire detection alerts\n",
    "\n",
    "The ensemble model with performance-based weighting provides the best balance of accuracy and robustness for fire detection applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
