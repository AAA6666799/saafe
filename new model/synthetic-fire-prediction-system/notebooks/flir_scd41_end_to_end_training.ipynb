{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3042ed",
   "metadata": {},
   "source": [
    "# End-to-End Training Pipeline for FLIR+SCD41 Fire Detection System\n",
    "\n",
    "This notebook demonstrates the complete workflow for training the FLIR+SCD41 fire detection system:\n",
    "1. Dataset generation\n",
    "2. Data storage\n",
    "3. Data splitting\n",
    "4. Model training\n",
    "5. Ensemble weight calculation\n",
    "6. Model evaluation\n",
    "\n",
    "## System Overview\n",
    "The system uses:\n",
    "- FLIR Lepton 3.5 thermal camera (15 features)\n",
    "- Sensirion SCD41 CO₂ sensor (3 features)\n",
    "- Total: 18 features for fire detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfae31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import our custom modules\n",
    "from src.data_generation.flir_scd41.flir_data_generator import FlirDataGenerator\n",
    "from src.data_generation.flir_scd41.scd41_data_generator import Scd41DataGenerator\n",
    "from src.feature_engineering.extractors.flir_thermal_extractor import FlirThermalExtractor\n",
    "from src.feature_engineering.extractors.scd41_gas_extractor import Scd41GasExtractor\n",
    "from src.training.flir_scd41.train_flir_scd41_model import FlirScd41XGBoost, FlirScd41LSTM\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042b522",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation\n",
    "\n",
    "Generate synthetic training data for FLIR+SCD41 sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "print(\"🔄 Generating synthetic FLIR+SCD41 dataset...\")\n",
    "\n",
    "# Create data generators\n",
    "flir_generator = FlirDataGenerator()\n",
    "scd41_generator = Scd41DataGenerator()\n",
    "\n",
    "# Generate data\n",
    "num_samples = 50000  # 50K samples for training\n",
    "flir_data = flir_generator.generate_data(num_samples)\n",
    "scd41_data = scd41_generator.generate_data(num_samples)\n",
    "\n",
    "print(f\"✅ Generated {len(flir_data)} FLIR samples and {len(scd41_data)} SCD41 samples\")\n",
    "print(f\"FLIR data shape: {flir_data.shape}\")\n",
    "print(f\"SCD41 data shape: {scd41_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a5ce3",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "Extract features from raw sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83989faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "print(\"🔄 Extracting features from sensor data...\")\n",
    "\n",
    "# Create feature extractors\n",
    "flir_extractor = FlirThermalExtractor()\n",
    "scd41_extractor = Scd41GasExtractor()\n",
    "\n",
    "# Extract features for all samples\n",
    "flir_features = []\n",
    "scd41_features = []\n",
    "\n",
    "for i in range(len(flir_data)):\n",
    "    # Extract FLIR features (15 features)\n",
    "    flir_sample = flir_extractor.extract_features(flir_data[i])\n",
    "    flir_features.append(flir_sample)\n",
    "    \n",
    "    # Extract SCD41 features (3 features)\n",
    "    scd41_sample = scd41_extractor.extract_features(scd41_data[i])\n",
    "    scd41_features.append(scd41_sample)\n",
    "\n",
    "flir_features = np.array(flir_features)\n",
    "scd41_features = np.array(scd41_features)\n",
    "\n",
    "print(f\"✅ Extracted FLIR features: {flir_features.shape}\")\n",
    "print(f\"✅ Extracted SCD41 features: {scd41_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310841fa",
   "metadata": {},
   "source": [
    "## 3. Dataset Storage\n",
    "\n",
    "Save the generated dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab858f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and create labels\n",
    "print(\"💾 Combining features and creating dataset...\")\n",
    "\n",
    "# Combine all features (15 FLIR + 3 SCD41 = 18 features)\n",
    "all_features = np.concatenate([flir_features, scd41_features], axis=1)\n",
    "\n",
    "# Create labels (fire detected or not)\n",
    "# In a real scenario, this would come from the data generation process\n",
    "# For this example, we'll create synthetic labels\n",
    "np.random.seed(42)\n",
    "fire_probability = 0.15  # 15% of samples are fire events\n",
    "labels = np.random.choice([0, 1], size=len(all_features), p=[1-fire_probability, fire_probability])\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [\n",
    "    't_mean', 't_std', 't_max', 't_p95', 't_hot_area_pct',\n",
    "    't_hot_largest_blob_pct', 't_grad_mean', 't_grad_std',\n",
    "    't_diff_mean', 't_diff_std', 'flow_mag_mean', 'flow_mag_std',\n",
    "    'tproxy_val', 'tproxy_delta', 'tproxy_vel',\n",
    "    'gas_val', 'gas_delta', 'gas_vel'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(all_features, columns=feature_names)\n",
    "df['fire_detected'] = labels\n",
    "\n",
    "# Save to CSV\n",
    "data_dir = os.path.join(project_root, 'data', 'flir_scd41')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "dataset_path = os.path.join(data_dir, 'flir_scd41_dataset.csv')\n",
    "df.to_csv(dataset_path, index=False)\n",
    "\n",
    "print(f\"✅ Dataset saved to {dataset_path}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fire samples: {sum(labels)} ({sum(labels)/len(labels)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7f2dd",
   "metadata": {},
   "source": [
    "## 4. Data Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a25031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "print(\"📊 Splitting dataset into train/validation/test sets...\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('fire_detected', axis=1).values\n",
    "y = df['fire_detected'].values\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)  # 0.176 ≈ 0.15/0.85\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save splits\n",
    "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "train_df['fire_detected'] = y_train\n",
    "train_df.to_csv(os.path.join(data_dir, 'train.csv'), index=False)\n",
    "\n",
    "val_df = pd.DataFrame(X_val, columns=feature_names)\n",
    "val_df['fire_detected'] = y_val\n",
    "val_df.to_csv(os.path.join(data_dir, 'val.csv'), index=False)\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "test_df['fire_detected'] = y_test\n",
    "test_df.to_csv(os.path.join(data_dir, 'test.csv'), index=False)\n",
    "\n",
    "print(\"✅ Dataset splits saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427e069",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train multiple models: XGBoost and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "print(\"🚀 Training XGBoost model...\")\n",
    "\n",
    "# Create and train XGBoost model\n",
    "xgb_model = FlirScd41XGBoost(\n",
    "    max_depth=6,\n",
    "    eta=0.3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "xgb_val_pred_proba = xgb_model.predict_proba(X_val)\n",
    "\n",
    "xgb_train_metrics = {\n",
    "    'accuracy': accuracy_score(y_train, xgb_train_pred),\n",
    "    'f1_score': f1_score(y_train, xgb_train_pred),\n",
    "    'precision': precision_score(y_train, xgb_train_pred),\n",
    "    'recall': recall_score(y_train, xgb_train_pred)\n",
    "}\n",
    "\n",
    "xgb_val_metrics = {\n",
    "    'accuracy': accuracy_score(y_val, xgb_val_pred),\n",
    "    'f1_score': f1_score(y_val, xgb_val_pred),\n",
    "    'precision': precision_score(y_val, xgb_val_pred),\n",
    "    'recall': recall_score(y_val, xgb_val_pred),\n",
    "    'auc': roc_auc_score(y_val, xgb_val_pred_proba) if len(np.unique(y_val)) > 1 else 0.0\n",
    "}\n",
    "\n",
    "print(\"XGBoost Training Metrics:\")\n",
    "for metric, value in xgb_train_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nXGBoost Validation Metrics:\")\n",
    "for metric, value in xgb_val_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38440a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model\n",
    "print(\"\\n🚀 Training LSTM model...\")\n",
    "\n",
    "# Create and train LSTM model\n",
    "lstm_model = FlirScd41LSTM(input_size=18, hidden_size=64, num_layers=2, num_classes=2)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lstm_model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "class FlirScd41Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = FlirScd41Dataset(X_train, y_train)\n",
    "val_dataset = FlirScd41Dataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "    \n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        train_targets.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    # Validation\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in val_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            outputs = lstm_model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            val_targets.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(train_targets, train_preds)\n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(lstm_model.state_dict(), os.path.join(data_dir, 'best_lstm_model.pth'))\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'  Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "        print(f'  Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "# Final LSTM metrics\n",
    "lstm_val_metrics = {\n",
    "    'accuracy': best_val_acc,\n",
    "    'f1_score': f1_score(val_targets, val_preds),\n",
    "    'precision': precision_score(val_targets, val_preds),\n",
    "    'recall': recall_score(val_targets, val_preds)\n",
    "}\n",
    "\n",
    "print(\"\\nLSTM Validation Metrics:\")\n",
    "for metric, value in lstm_val_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d64022",
   "metadata": {},
   "source": [
    "## 6. Ensemble Weight Calculation\n",
    "\n",
    "Calculate optimal weights for model ensemble based on validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ensemble weights based on validation performance\n",
    "print(\"⚖️ Calculating ensemble weights...\")\n",
    "\n",
    "# Performance scores for each model (using accuracy as the metric)\n",
    "xgb_score = xgb_val_metrics['accuracy']\n",
    "lstm_score = lstm_val_metrics['accuracy']\n",
    "\n",
    "print(f\"XGBoost validation accuracy: {xgb_score:.4f}\")\n",
    "print(f\"LSTM validation accuracy: {lstm_score:.4f}\")\n",
    "\n",
    "# Method 1: Performance-based weighting (exponential scaling)\n",
    "def calculate_performance_weights(scores, scaling_factor=2.0):\n",
    "    \"\"\"Calculate weights based on performance scores using exponential scaling\"\"\"\n",
    "    # Normalize scores to [0, 1] range\n",
    "    min_score = min(scores)\n",
    "    max_score = max(scores)\n",
    "    \n",
    "    if max_score == min_score:\n",
    "        # All models have same performance, equal weights\n",
    "        return [1.0/len(scores)] * len(scores)\n",
    "    \n",
    "    normalized_scores = [(score - min_score) / (max_score - min_score) for score in scores]\n",
    "    \n",
    "    # Apply exponential scaling\n",
    "    weighted_scores = [np.exp(scaling_factor * score) for score in normalized_scores]\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    total_weight = sum(weighted_scores)\n",
    "    weights = [w / total_weight for w in weighted_scores]\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Calculate weights\n",
    "model_scores = [xgb_score, lstm_score]\n",
    "ensemble_weights = calculate_performance_weights(model_scores)\n",
    "\n",
    "print(f\"\\nEnsemble weights:\")\n",
    "print(f\"  XGBoost weight: {ensemble_weights[0]:.4f}\")\n",
    "print(f\"  LSTM weight: {ensemble_weights[1]:.4f}\")\n",
    "\n",
    "# Save weights\n",
    "weights_data = {\n",
    "    'models': ['xgboost', 'lstm'],\n",
    "    'weights': ensemble_weights,\n",
    "    'validation_scores': {\n",
    "        'xgboost': xgb_score,\n",
    "        'lstm': lstm_score\n",
    "    },\n",
    "    'calculation_method': 'performance_based_exponential_scaling',\n",
    "    'scaling_factor': 2.0\n",
    "}\n",
    "\n",
    "weights_path = os.path.join(data_dir, 'ensemble_weights.json')\n",
    "with open(weights_path, 'w') as f:\n",
    "    json.dump(weights_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Ensemble weights saved to {weights_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf52af",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the ensemble model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "print(\"🧪 Evaluating models on test set...\")\n",
    "\n",
    "# XGBoost predictions\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "xgb_test_pred_proba = xgb_model.predict_proba(X_test)\n",
    "\n",
    "# LSTM predictions\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_data = torch.FloatTensor(X_test).to(device)\n",
    "    lstm_outputs = lstm_model(test_data)\n",
    "    lstm_test_pred_proba = torch.softmax(lstm_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "    lstm_test_pred = (lstm_test_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Ensemble predictions (weighted average)\n",
    "ensemble_pred_proba = (\n",
    "    ensemble_weights[0] * xgb_test_pred_proba + \n",
    "    ensemble_weights[1] * lstm_test_pred_proba\n",
    ")\n",
    "ensemble_test_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_test_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, xgb_test_pred),\n",
    "    'f1_score': f1_score(y_test, xgb_test_pred),\n",
    "    'precision': precision_score(y_test, xgb_test_pred),\n",
    "    'recall': recall_score(y_test, xgb_test_pred),\n",
    "    'auc': roc_auc_score(y_test, xgb_test_pred_proba)\n",
    "}\n",
    "\n",
    "lstm_test_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, lstm_test_pred),\n",
    "    'f1_score': f1_score(y_test, lstm_test_pred),\n",
    "    'precision': precision_score(y_test, lstm_test_pred),\n",
    "    'recall': recall_score(y_test, lstm_test_pred),\n",
    "    'auc': roc_auc_score(y_test, lstm_test_pred_proba)\n",
    "}\n",
    "\n",
    "ensemble_test_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, ensemble_test_pred),\n",
    "    'f1_score': f1_score(y_test, ensemble_test_pred),\n",
    "    'precision': precision_score(y_test, ensemble_test_pred),\n",
    "    'recall': recall_score(y_test, ensemble_test_pred),\n",
    "    'auc': roc_auc_score(y_test, ensemble_pred_proba)\n",
    "}\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"\\nXGBoost:\")\n",
    "for metric, value in xgb_test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nLSTM:\")\n",
    "for metric, value in lstm_test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEnsemble:\")\n",
    "for metric, value in ensemble_test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda49e39",
   "metadata": {},
   "source": [
    "## 8. Model Saving\n",
    "\n",
    "Save all trained models and components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "print(\"💾 Saving trained models...\")\n",
    "\n",
    "# Save XGBoost model\n",
    "xgb_model_path = os.path.join(data_dir, 'flir_scd41_xgboost_model.json')\n",
    "xgb_model.model.save_model(xgb_model_path)\n",
    "\n",
    "# Save XGBoost scaler\n",
    "xgb_scaler_path = os.path.join(data_dir, 'flir_scd41_xgboost_scaler.joblib')\n",
    "joblib.dump(xgb_model.scaler, xgb_scaler_path)\n",
    "\n",
    "# LSTM model already saved during training\n",
    "lstm_model_path = os.path.join(data_dir, 'best_lstm_model.pth')\n",
    "\n",
    "# Save model information\n",
    "model_info = {\n",
    "    'xgboost': {\n",
    "        'model_path': xgb_model_path,\n",
    "        'scaler_path': xgb_scaler_path,\n",
    "        'metrics': xgb_test_metrics\n",
    "    },\n",
    "    'lstm': {\n",
    "        'model_path': lstm_model_path,\n",
    "        'metrics': lstm_test_metrics\n",
    "    },\n",
    "    'ensemble': {\n",
    "        'weights_path': weights_path,\n",
    "        'metrics': ensemble_test_metrics\n",
    "    },\n",
    "    'feature_names': feature_names,\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "model_info_path = os.path.join(data_dir, 'model_info.json')\n",
    "with open(model_info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"✅ XGBoost model saved to {xgb_model_path}\")\n",
    "print(f\"✅ XGBoost scaler saved to {xgb_scaler_path}\")\n",
    "print(f\"✅ LSTM model saved to {lstm_model_path}\")\n",
    "print(f\"✅ Model information saved to {model_info_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08444e",
   "metadata": {},
   "source": [
    "## 9. Results Visualization\n",
    "\n",
    "Visualize model performance and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34234cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison visualization\n",
    "print(\"📊 Creating performance visualization...\")\n",
    "\n",
    "# Prepare data for plotting\n",
    "metrics = ['accuracy', 'f1_score', 'precision', 'recall', 'auc']\n",
    "xgb_scores = [xgb_test_metrics[m] for m in metrics]\n",
    "lstm_scores = [lstm_test_metrics[m] for m in metrics]\n",
    "ensemble_scores = [ensemble_test_metrics[m] for m in metrics]\n",
    "\n",
    "# Create bar plot\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width, xgb_scores, width, label='XGBoost', color='skyblue')\n",
    "bars2 = ax.bar(x, lstm_scores, width, label='LSTM', color='lightcoral')\n",
    "bars3 = ax.bar(x + width, ensemble_scores, width, label='Ensemble', color='lightgreen')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('FLIR+SCD41 Fire Detection Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_value_labels(bars1)\n",
    "add_value_labels(bars2)\n",
    "add_value_labels(bars3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n🏆 Model Performance Summary:\")\n",
    "print(f\"XGBoost Accuracy: {xgb_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"LSTM Accuracy: {lstm_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Ensemble Accuracy: {ensemble_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"\\nBest Model: {'Ensemble' if ensemble_test_metrics['accuracy'] > max(xgb_test_metrics['accuracy'], lstm_test_metrics['accuracy']) else 'XGBoost' if xgb_test_metrics['accuracy'] > lstm_test_metrics['accuracy'] else 'LSTM'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d141d7",
   "metadata": {},
   "source": [
    "## 10. Training Summary\n",
    "\n",
    "Summary of the entire training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏁 Training Process Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset Size: {len(df):,} samples\")\n",
    "print(f\"Features: {len(feature_names)} (15 FLIR + 3 SCD41)\")\n",
    "print(f\"Fire Samples: {sum(labels):,} ({sum(labels)/len(labels)*100:.2f}%)\")\n",
    "print(f\"Training Samples: {len(X_train):,}\")\n",
    "print(f\"Validation Samples: {len(X_val):,}\")\n",
    "print(f\"Test Samples: {len(X_test):,}\")\n",
    "print()\n",
    "print(\"Model Performance (Test Set):\")\n",
    "print(f\"  XGBoost Accuracy: {xgb_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  LSTM Accuracy: {lstm_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Ensemble Accuracy: {ensemble_test_metrics['accuracy']:.4f}\")\n",
    "print()\n",
    "print(\"Ensemble Weights:\")\n",
    "print(f\"  XGBoost: {ensemble_weights[0]:.4f}\")\n",
    "print(f\"  LSTM: {ensemble_weights[1]:.4f}\")\n",
    "print()\n",
    "print(\"Files Saved:\")\n",
    "print(f\"  Dataset: {dataset_path}\")\n",
    "print(f\"  Train Split: {os.path.join(data_dir, 'train.csv')}\")\n",
    "print(f\"  Validation Split: {os.path.join(data_dir, 'val.csv')}\")\n",
    "print(f\"  Test Split: {os.path.join(data_dir, 'test.csv')}\")\n",
    "print(f\"  XGBoost Model: {xgb_model_path}\")\n",
    "print(f\"  LSTM Model: {lstm_model_path}\")\n",
    "print(f\"  Ensemble Weights: {weights_path}\")\n",
    "print(f\"  Model Info: {model_info_path}\")\n",
    "print()\n",
    "print(\"✅ End-to-end training pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
