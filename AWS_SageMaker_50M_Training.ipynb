{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Fire Detection AI - 50M Dataset Training on AWS SageMaker\n",
    "\n",
    "**Objective**: Train production-ready fire detection models on 19.9M samples from S3  \n",
    "**Dataset**: `s3://processedd-synthetic-data/cleaned-data/`  \n",
    "**Target**: 97-98% accuracy, <0.5% false positive rate  \n",
    "**Instance**: `ml.p3.2xlarge` (Tesla V100 GPU) or `ml.p3.8xlarge` (4x V100)\n",
    "\n",
    "## Dataset Structure\n",
    "- `basement_data_cleaned.csv` (922.8 MB) - 6 columns\n",
    "- `laundry_data_cleaned.csv` (756.2 MB) - 5 columns  \n",
    "- `asd_data_cleaned.csv` (600.6 MB) - 4 columns\n",
    "- `voc_data_cleaned.csv` (566.4 MB) - 4 columns\n",
    "- `arc_data_cleaned.csv` (489.2 MB) - 4 columns\n",
    "\n",
    "**Total**: ~19.9M samples, 3.26 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for SageMaker\n",
    "!pip install torch torchvision --quiet\n",
    "!pip install xgboost lightgbm catboost --quiet\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn --quiet\n",
    "!pip install boto3 sagemaker --quiet\n",
    "!pip install optuna --quiet  # For hyperparameter optimization\n",
    "\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Advanced ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CB_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸš€ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"ðŸ”¥\" * 80)\n",
    "print(\"FIRE DETECTION AI - 50M DATASET TRAINING ON AWS SAGEMAKER\")\n",
    "print(\"ðŸ”¥\" * 80)\n",
    "print(f\"ðŸ“Š Target: 19.9M samples from 5 area datasets\")\n",
    "print(f\"ðŸŽ¯ Goal: 97-98% accuracy, <0.5% false positive rate\")\n",
    "print(f\"âš¡ XGBoost: {'âœ…' if XGB_AVAILABLE else 'âŒ'}\")\n",
    "print(f\"âš¡ LightGBM: {'âœ…' if LGB_AVAILABLE else 'âŒ'}\")\n",
    "print(f\"âš¡ CatBoost: {'âœ…' if CB_AVAILABLE else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker configuration\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"âœ… SageMaker Session initialized\")\n",
    "print(f\"   Role: {role}\")\n",
    "print(f\"   Bucket: {bucket}\")\n",
    "print(f\"   Region: {region}\")\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_BUCKET = \"processedd-synthetic-data\"\n",
    "DATASET_PREFIX = \"cleaned-data/\"\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Configuration:\")\n",
    "print(f\"   Source: s3://{DATASET_BUCKET}/{DATASET_PREFIX}\")\n",
    "print(f\"   Output: s3://{bucket}/fire-detection-models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Fire Detection Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFireTransformer(nn.Module):\n",
    "    \"\"\"Enhanced transformer for multi-area fire detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=6, seq_len=60, d_model=256, num_heads=8, \n",
    "                 num_layers=6, num_classes=3, num_areas=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.area_embedding = nn.Embedding(num_areas, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Multi-head outputs\n",
    "        self.fire_classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.risk_predictor = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.confidence_estimator = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, area_types):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Input projection and area embedding\n",
    "        x = self.input_proj(x)\n",
    "        area_emb = self.area_embedding(area_types).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        x = x + area_emb + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # Transformer processing\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        return {\n",
    "            'fire_logits': self.fire_classifier(x),\n",
    "            'risk_score': self.risk_predictor(x) * 100.0,\n",
    "            'confidence': self.confidence_estimator(x)\n",
    "        }\n",
    "\n",
    "print(\"âœ… Enhanced Fire Transformer model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. S3 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3DataLoader:\n",
    "    \"\"\"Optimized S3 data loader for SageMaker\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name=\"processedd-synthetic-data\", prefix=\"cleaned-data/\"):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Area mappings based on dataset analysis\n",
    "        self.area_files = {\n",
    "            'basement': 'basement_data_cleaned.csv',\n",
    "            'laundry': 'laundry_data_cleaned.csv', \n",
    "            'asd': 'asd_data_cleaned.csv',\n",
    "            'voc': 'voc_data_cleaned.csv',\n",
    "            'arc': 'arc_data_cleaned.csv'\n",
    "        }\n",
    "        \n",
    "        self.area_to_idx = {area: idx for idx, area in enumerate(self.area_files.keys())}\n",
    "        \n",
    "    def load_area_data(self, area_name: str, max_samples: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"Load data for specific area with progress tracking\"\"\"\n",
    "        file_key = f\"{self.prefix}{self.area_files[area_name]}\"\n",
    "        \n",
    "        print(f\"ðŸ“¥ Loading {area_name} data from s3://{self.bucket_name}/{file_key}\")\n",
    "        \n",
    "        try:\n",
    "            # Stream data from S3\n",
    "            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=file_key)\n",
    "            df = pd.read_csv(response['Body'])\n",
    "            \n",
    "            print(f\"   ðŸ“Š Raw data: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "            \n",
    "            if max_samples and len(df) > max_samples:\n",
    "                df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "                print(f\"   ðŸŽ¯ Sampled to: {len(df):,} rows\")\n",
    "            \n",
    "            print(f\"   âœ… Loaded {len(df):,} samples for {area_name}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error loading {area_name} data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_area_data(self, df: pd.DataFrame, area_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Smart preprocessing based on area characteristics\"\"\"\n",
    "        \n",
    "        print(f\"ðŸ”§ Preprocessing {area_name} data...\")\n",
    "        \n",
    "        # Handle different column structures\n",
    "        if area_name == 'basement':\n",
    "            # 6 columns - more features available\n",
    "            if 'timestamp' in df.columns:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "            \n",
    "            feature_cols = [col for col in df.columns if col not in ['timestamp', 'is_anomaly', 'label']]\n",
    "            if len(feature_cols) > 4:\n",
    "                feature_cols = feature_cols[:4]\n",
    "                \n",
    "        elif area_name == 'laundry':\n",
    "            # 5 columns\n",
    "            feature_cols = [col for col in df.columns if col not in ['timestamp', 'is_anomaly', 'label']]\n",
    "            if len(feature_cols) > 3:\n",
    "                feature_cols = feature_cols[:3]\n",
    "                \n",
    "        else:  # asd, voc, arc - 4 columns each\n",
    "            feature_cols = [col for col in df.columns if col not in ['timestamp', 'is_anomaly', 'label']]\n",
    "            if len(feature_cols) > 2:\n",
    "                feature_cols = feature_cols[:2]\n",
    "        \n",
    "        # Extract features\n",
    "        X = df[feature_cols].fillna(0).values\n",
    "        \n",
    "        # Create labels\n",
    "        if 'is_anomaly' in df.columns:\n",
    "            y = df['is_anomaly'].values.astype(int)\n",
    "        elif 'label' in df.columns:\n",
    "            y = df['label'].values.astype(int)\n",
    "        else:\n",
    "            # Intelligent label generation based on patterns\n",
    "            value_col = 'value' if 'value' in df.columns else feature_cols[0]\n",
    "            values = df[value_col].values\n",
    "            \n",
    "            # Create realistic fire detection labels\n",
    "            q95 = np.percentile(values, 95)\n",
    "            q85 = np.percentile(values, 85)\n",
    "            \n",
    "            y = np.zeros(len(values))\n",
    "            y[values > q95] = 2  # Fire (top 5%)\n",
    "            y[(values > q85) & (values <= q95)] = 1  # Warning (85-95%)\n",
    "            # Rest remain 0 (Normal)\n",
    "        \n",
    "        # Ensure consistent feature dimensions\n",
    "        if X.shape[1] < 6:\n",
    "            padding = np.zeros((X.shape[0], 6 - X.shape[1]))\n",
    "            X = np.hstack([X, padding])\n",
    "        elif X.shape[1] > 6:\n",
    "            X = X[:, :6]\n",
    "        \n",
    "        # Add feature engineering\n",
    "        if len(feature_cols) >= 2:\n",
    "            # Add derived features\n",
    "            feature_mean = np.mean(X[:, :len(feature_cols)], axis=1, keepdims=True)\n",
    "            feature_std = np.std(X[:, :len(feature_cols)], axis=1, keepdims=True)\n",
    "            \n",
    "            # Replace some padding with engineered features\n",
    "            if X.shape[1] > len(feature_cols):\n",
    "                X[:, len(feature_cols)] = feature_mean.flatten()\n",
    "                if X.shape[1] > len(feature_cols) + 1:\n",
    "                    X[:, len(feature_cols) + 1] = feature_std.flatten()\n",
    "        \n",
    "        anomaly_rate = y.mean()\n",
    "        class_dist = np.bincount(y.astype(int))\n",
    "        \n",
    "        print(f\"   ðŸ“Š Features: {X.shape}, Labels: {y.shape}\")\n",
    "        print(f\"   ðŸŽ¯ Anomaly rate: {anomaly_rate:.4f}\")\n",
    "        print(f\"   ðŸ“ˆ Class distribution: {class_dist}\")\n",
    "        print(f\"   âœ… Preprocessed {area_name}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray, seq_len: int = 60, \n",
    "                        step: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create overlapping time series sequences\"\"\"\n",
    "        print(f\"ðŸ”„ Creating sequences (length={seq_len}, step={step})...\")\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(0, len(X) - seq_len, step):\n",
    "            seq = X[i:i+seq_len]\n",
    "            label = y[i+seq_len-1]  # Use label at end of sequence\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "        \n",
    "        sequences = np.array(sequences)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        print(f\"   âœ… Created {len(sequences):,} sequences\")\n",
    "        return sequences, labels\n",
    "    \n",
    "    def load_all_data(self, max_samples_per_area: Optional[int] = None, \n",
    "                     seq_len: int = 60, use_all_data: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load and combine all area data with progress tracking\"\"\"\n",
    "        \n",
    "        print(\"\\nðŸš€ LOADING COMPLETE DATASET FROM S3\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if use_all_data:\n",
    "            max_samples_per_area = None\n",
    "            print(\"ðŸ“Š Loading FULL dataset (this may take 10-30 minutes)\")\n",
    "        else:\n",
    "            max_samples_per_area = max_samples_per_area or 1000000  # 1M per area for demo\n",
    "            print(f\"ðŸ“Š Loading {max_samples_per_area:,} samples per area\")\n",
    "        \n",
    "        all_sequences = []\n",
    "        all_labels = []\n",
    "        all_areas = []\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        for i, area_name in enumerate(self.area_files.keys(), 1):\n",
    "            area_start_time = time.time()\n",
    "            print(f\"\\nðŸ“ PROCESSING AREA {i}/5: {area_name.upper()}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            try:\n",
    "                # Load area data\n",
    "                df = self.load_area_data(area_name, max_samples_per_area)\n",
    "                \n",
    "                # Preprocess\n",
    "                X, y = self.preprocess_area_data(df, area_name)\n",
    "                \n",
    "                # Create sequences\n",
    "                sequences, labels = self.create_sequences(X, y, seq_len)\n",
    "                \n",
    "                # Add area information\n",
    "                area_idx = self.area_to_idx[area_name]\n",
    "                areas = np.full(len(sequences), area_idx)\n",
    "                \n",
    "                all_sequences.append(sequences)\n",
    "                all_labels.append(labels)\n",
    "                all_areas.append(areas)\n",
    "                \n",
    "                area_time = time.time() - area_start_time\n",
    "                print(f\"   â±ï¸ Area processing time: {area_time:.1f}s\")\n",
    "                print(f\"   âœ… {area_name}: {len(sequences):,} sequences\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Failed to process {area_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all areas\n",
    "        if all_sequences:\n",
    "            print(f\"\\nðŸ”— COMBINING ALL AREAS\")\n",
    "            print(\"-\" * 25)\n",
    "            \n",
    "            X_combined = np.vstack(all_sequences)\n",
    "            y_combined = np.hstack(all_labels)\n",
    "            areas_combined = np.hstack(all_areas)\n",
    "            \n",
    "            total_time = time.time() - total_start_time\n",
    "            \n",
    "            print(f\"ðŸŽ¯ DATASET SUMMARY:\")\n",
    "            print(f\"   ðŸ“Š Total sequences: {X_combined.shape[0]:,}\")\n",
    "            print(f\"   ðŸ“ Sequence shape: {X_combined.shape[1:]}\")\n",
    "            print(f\"   ðŸ·ï¸ Labels: {y_combined.shape}\")\n",
    "            print(f\"   ðŸŒ Areas: {areas_combined.shape}\")\n",
    "            print(f\"   ðŸ“ˆ Class distribution: {np.bincount(y_combined.astype(int))}\")\n",
    "            print(f\"   ðŸ’¾ Memory usage: {X_combined.nbytes / (1024**3):.2f} GB\")\n",
    "            print(f\"   â±ï¸ Total loading time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "            print(f\"   âœ… Ready for training!\")\n",
    "            \n",
    "            return X_combined, y_combined, areas_combined\n",
    "        else:\n",
    "            raise ValueError(\"âŒ No data could be loaded from any area\")\n",
    "\n",
    "print(\"âœ… S3 Data Loader defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dataset from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = S3DataLoader(bucket_name=DATASET_BUCKET, prefix=DATASET_PREFIX)\n",
    "\n",
    "# Configuration for data loading\n",
    "USE_FULL_DATASET = True  # Set to True for complete 50M dataset training\n",
    "MAX_SAMPLES_PER_AREA = 2000000 if not USE_FULL_DATASET else None  # 2M samples per area for demo\n",
    "\n",
    "print(f\"ðŸ”§ Configuration:\")\n",
    "print(f\"   Use full dataset: {USE_FULL_DATASET}\")\n",
    "print(f\"   Max samples per area: {MAX_SAMPLES_PER_AREA or 'ALL'}\")\n",
    "print(f\"   Expected total: ~{(MAX_SAMPLES_PER_AREA or 4000000) * 5 / 100000:.1f}M sequences\" if MAX_SAMPLES_PER_AREA else \"~19.9M sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the complete dataset\n",
    "X, y, areas = data_loader.load_all_data(\n",
    "    max_samples_per_area=MAX_SAMPLES_PER_AREA,\n",
    "    seq_len=60,\n",
    "    use_all_data=USE_FULL_DATASET\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Dataset loaded successfully!\")\n",
    "print(f\"ðŸ“Š Ready to train on {len(X):,} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Fire Detection Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionFireEnsemble:\n",
    "    \"\"\"Production-ready fire detection ensemble optimized for SageMaker\"\"\"\n",
    "    \n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.is_fitted = False\n",
    "        self.training_history = []\n",
    "        \n",
    "        print(f\"ðŸš€ Production Fire Ensemble initialized\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\" if torch.cuda.is_available() else \"   CPU Mode\")\n",
    "    \n",
    "    def _engineer_features(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Advanced feature engineering for ML models\"\"\"\n",
    "        print(