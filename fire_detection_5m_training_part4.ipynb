{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Training with Early Stopping\n",
    "\n",
    "### 5.1 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def train_transformer_with_early_stopping(model, X_train, y_train, areas_train, X_val, y_val, areas_val, device):\n",
    "    \"\"\"Train the optimized transformer model with error handling\"\"\"\n",
    "    logger.info(\"ü§ñ TRAINING OPTIMIZED TRANSFORMER\")\n",
    "    logger.info(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Convert to tensors with error checking\n",
    "        try:\n",
    "            X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "            X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "            y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "            y_val_tensor = torch.LongTensor(y_val).to(device)\n",
    "            areas_train_tensor = torch.LongTensor(areas_train).to(device)\n",
    "            areas_val_tensor = torch.LongTensor(areas_val).to(device)\n",
    "        except Exception as e:\n",
    "            raise ModelInitializationError(f\"Error converting data to tensors: {e}\")\n",
    "        \n",
    "        # Training setup\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        logger.info(f\"Training for {EPOCHS} epochs with early stopping...\")\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        training_history = []\n",
    "        \n",
    "        # Training loop with error handling and checkpointing\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Training\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(X_train_tensor, areas_train_tensor)\n",
    "                loss = criterion(outputs['fire_logits'], y_train_tensor)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(X_val_tensor, areas_val_tensor)\n",
    "                    val_preds = torch.argmax(val_outputs['fire_logits'], dim=1)\n",
    "                    val_acc = (val_preds == y_val_tensor).float().mean().item()\n",
    "                    \n",
    "                    # Track memory usage\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_usage = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "                    else:\n",
    "                        memory_usage = 0\n",
    "                    \n",
    "                    # Log progress\n",
    "                    epoch_time = time.time() - epoch_start_time\n",
    "                    logger.info(f\"Epoch {epoch:3d}: Loss={loss:.4f}, Val_Acc={val_acc:.4f}, \"\n",
    "                               f\"Time={epoch_time:.1f}s, Memory={memory_usage:.2f}GB\")\n",
    "                    \n",
    "                    # Update training history\n",
    "                    training_history.append({\n",
    "                        'epoch': epoch,\n",
    "                        'train_loss': loss.item(),\n",
    "                        'val_accuracy': val_acc,\n",
    "                        'learning_rate': scheduler.get_last_lr()[0],\n",
    "                        'time': epoch_time,\n",
    "                        'memory_usage': memory_usage\n",
    "                    })\n",
    "                    \n",
    "                    # Update dashboard\n",
    "                    if epoch % VISUALIZATION_CONFIG['update_interval'] == 0:\n",
    "                        update_dashboard(\n",
    "                            dashboard_fig, dashboard_axes, dashboard_data,\n",
    "                            epoch, loss.item(), val_acc, scheduler.get_last_lr()[0],\n",
    "                            epoch_time, memory_usage\n",
    "                        )\n",
    "                    \n",
    "                    # Check for improvement\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        best_model_state = model.state_dict().copy()\n",
    "                        patience_counter = 0\n",
    "                        \n",
    "                        # Save checkpoint\n",
    "                        checkpoint_path = f'checkpoints/transformer_epoch_{epoch}_acc_{val_acc:.4f}.pt'\n",
    "                        torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'scheduler_state_dict': scheduler.state_dict(),\n",
    "                            'val_accuracy': val_acc,\n",
    "                            'training_history': training_history\n",
    "                        }, checkpoint_path)\n",
    "                        logger.info(f\"   üíæ Saved checkpoint: {checkpoint_path}\")\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                        logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                # Check if it's an out-of-memory error\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    logger.error(f\"‚ùå CUDA out of memory at epoch {epoch}\")\n",
    "                    logger.error(f\"   Try reducing batch size or model size\")\n",
    "                    \n",
    "                    # Try to recover by clearing cache\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Save current best model before exiting\n",
    "                    if best_model_state:\n",
    "                        recovery_path = f'checkpoints/transformer_recovery_acc_{best_val_acc:.4f}.pt'\n",
    "                        torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': best_model_state,\n",
    "                            'val_accuracy': best_val_acc,\n",
    "                            'training_history': training_history\n",
    "                        }, recovery_path)\n",
    "                        logger.info(f\"   üíæ Saved recovery checkpoint: {recovery_path}\")\n",
    "                    \n",
    "                    raise TrainingProcessError(f\"CUDA out of memory at epoch {epoch}\")\n",
    "                else:\n",
    "                    # Other runtime errors\n",
    "                    logger.error(f\"‚ùå Runtime error at epoch {epoch}: {e}\")\n",
    "                    raise TrainingProcessError(f\"Runtime error at epoch {epoch}: {e}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error during epoch {epoch}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "                \n",
    "                # Try to save checkpoint before exiting\n",
    "                if best_model_state:\n",
    "                    recovery_path = f'checkpoints/transformer_recovery_acc_{best_val_acc:.4f}.pt'\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': best_model_state,\n",
    "                        'val_accuracy': best_val_acc,\n",
    "                        'training_history': training_history\n",
    "                    }, recovery_path)\n",
    "                    logger.info(f\"   üíæ Saved recovery checkpoint: {recovery_path}\")\n",
    "                \n",
    "                raise TrainingProcessError(f\"Error during epoch {epoch}: {e}\")\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state:\n",
    "            model.load_state_dict(best_model_state)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        logger.info(f\"‚úÖ Transformer training completed!\")\n",
    "        logger.info(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        logger.info(f\"   Training time: {training_time:.1f}s ({training_time/60:.1f} min)\")\n",
    "        \n",
    "        return model, best_val_acc, training_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Catch any uncaught exceptions\n",
    "        logger.error(f\"‚ùå Unexpected error in transformer training: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise TrainingProcessError(f\"Unexpected error in transformer training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Learning Curves Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(training_history):\n",
    "    \"\"\"Plot learning curves from training history\"\"\"\n",
    "    \n",
    "    # Extract metrics\n",
    "    epochs = [entry['epoch'] for entry in training_history]\n",
    "    train_losses = [entry['train_loss'] for entry in training_history]\n",
    "    val_accuracies = [entry['val_accuracy'] for entry in training_history]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Loss curve\n",
    "    ax1.plot(epochs, train_losses, 'b-', marker='o', label='Training Loss')\n",
    "    ax1.set_title('Training Loss Curve')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add moving average\n",
    "    window_size = min(5, len(train_losses))\n",
    "    if window_size > 1:\n",
    "        moving_avg = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        ax1.plot(epochs[window_size-1:], moving_avg, 'r--', label=f'{window_size}-epoch Moving Avg')\n",
    "        ax1.legend()\n",
    "    \n",
    "    # Accuracy curve\n",
    "    ax2.plot(epochs, val_accuracies, 'g-', marker='o', label='Validation Accuracy')\n",
    "    ax2.set_title('Validation Accuracy Curve')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add moving average\n",
    "    if window_size > 1:\n",
    "        moving_avg = np.convolve(val_accuracies, np.ones(window_size)/window_size, mode='valid')\n",
    "        ax2.plot(epochs[window_size-1:], moving_avg, 'r--', label=f'{window_size}-epoch Moving Avg')\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if enabled\n",
    "    if VISUALIZATION_CONFIG['save_figures']:\n",
    "        plt.savefig(f\"{VISUALIZATION_CONFIG['figure_dir']}/learning_curves.png\", dpi=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train transformer model\n",
    "model, best_val_acc, training_history = train_transformer_with_early_stopping(\n",
    "    model, X_train, y_train, areas_train, X_val, y_val, areas_val, device\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves(training_history)\n",
    "\n",
    "# Visualize memory usage\n",
    "visualize_memory_usage(memory_usage, timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ML Ensemble Models\n",
    "\n",
    "### 6.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(X):\n",
    "    \"\"\"Advanced feature engineering for ML models\"\"\"\n",
    "    \n",
    "    logger.info(\"üîß Engineering features...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    features = []\n",
    "    for i in range(X.shape[0]):\n",
    "        sample_features = []\n",
    "        for j in range(X.shape[2]):\n",
    "            series = X[i, :, j]\n",
    "            # Statistical features\n",
    "            sample_features.extend([\n",
    "                np.mean(series), np.std(series), np.min(series), np.max(series),\n",
    "                np.median(series), np.percentile(series, 25), np.percentile(series, 75)\n",
    "            ])\n",
    "            # Trend features\n",
    "            if len(series) > 1:\n",
    "                slope = np.polyfit(range(len(series)), series, 1)[0]\n",
    "                sample_features.append(slope)\n",
    "                diff = np.diff(series)\n",
    "                sample_features.extend([np.mean(np.abs(diff)), np.std(diff)])\n",
    "            else:\n",
    "                sample_features.extend([0, 0, 0])\n",
    "        features.append(sample_features)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    \n",
    "    logger.info(f\"   ‚úÖ Engineered {features.shape[1]} features for {features.shape[0]} samples\")\n",
    "    logger.info(f\"   ‚è±Ô∏è Time: {time.time() - start_time:.1f}s\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ML Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def train_optimized_ml_ensemble(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train ML ensemble models\"\"\"\n",
    "    logger.info(\"üìä TRAINING ML ENSEMBLE\")\n",
    "    logger.info(\"=\" * 30)\n",
    "    \n",
    "    # Feature engineering\n",
    "    logger.info(\"üîß Engineering features...\")\n",
    "    X_train_features = engineer_features(X_train)\n",
    "    X_val_features = engineer_features(X_val)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "    X_val_scaled = scaler.transform(X_val_features)\n",
    "    \n",
    "    ml_models = {}\n",
    "    ml_results = {}\n",
    "    \n",
    "    # Random Forest\n",
    "    logger.info(\"üå≥ Training Random Forest...\")\n",
    "    rf_start_time = time.time()\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,  # Reduced from 200\n",
    "        max_depth=15,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    rf_val_acc = rf_model.score(X_val_scaled, y_val)\n",
    "    ml_models['random_forest'] = rf_model\n",
    "    ml_results['random_forest'] = rf_val_acc\n",
    "    logger.info(f\"   ‚úÖ Random Forest Val Acc: {rf_val_acc:.4f}, Time: {time.time() - rf_start_time:.1f}s\")\n",
    "    \n",
    "    # XGBoost (if available)\n",
    "    if XGB_AVAILABLE:\n",
    "        logger.info(\"‚ö° Training XGBoost...\")\n",
    "        xgb_start_time = time.time()\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=100,  # Reduced from 300\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        xgb_model.fit(X_train_scaled, y_train)\n",
    "        xgb_val_acc = xgb_model.score(X_val_scaled, y_val)\n",
    "        ml_models['xgboost'] = xgb_model\n",
    "        ml_results['xgboost'] = xgb_val_acc\n",
    "        logger.info(f\"   ‚úÖ XGBoost Val Acc: {xgb_val_acc:.4f}, Time: {time.time() - xgb_start_time:.1f}s\")\n",
    "    \n",
    "    # LightGBM (if available)\n",
    "    if LGB_AVAILABLE:\n",
    "        logger.info(\"üí° Training LightGBM...\")\n",
    "        lgb_start_time = time.time()\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "            n_estimators=100,  # Reduced from 300\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=RANDOM_SEED,\n",
    "            verbose=-1\n",
    "        )\n",
    "        lgb_model.fit(X_train_scaled, y_train)\n",
    "        lgb_val_acc = lgb_model.score(X_val_scaled, y_val)\n",
    "        ml_models['lightgbm'] = lgb_model\n",
    "        ml_results['lightgbm'] = lgb_val_acc\n",
    "        logger.info(f\"   ‚úÖ LightGBM Val Acc: {lgb_val_acc:.4f}, Time: {time.time() - lgb_start_time:.1f}s\")\n",
    "    \n",
    "    return ml_models, ml_results, scaler, X_train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Train ML Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ML ensemble models\n",
    "ml_models, ml_results, scaler, X_train_features = train_optimized_ml_ensemble(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Print results\n",
    "logger.info(\"ML Ensemble Results:\")\n",
    "for model_name, val_acc in ml_results.items():\n",
    "    logger.info(f\"   {model_name}: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}