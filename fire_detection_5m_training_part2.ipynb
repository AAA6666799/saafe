{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Sampling\n",
    "\n",
    "### 2.1 Optimized Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDataLoader:\n",
    "    \"\"\"Optimized data loader with sampling for 5M dataset\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.s3_client = boto3.client('s3') if AWS_AVAILABLE else None\n",
    "        self.area_files = {\n",
    "            'basement': 'basement_data_cleaned.csv',\n",
    "            'laundry': 'laundry_data_cleaned.csv',\n",
    "            'asd': 'asd_data_cleaned.csv',\n",
    "            'voc': 'voc_data_cleaned.csv',\n",
    "            'arc': 'arc_data_cleaned.csv'\n",
    "        }\n",
    "        self.area_to_idx = {area: idx for idx, area in enumerate(self.area_files.keys())}\n",
    "    \n",
    "    @error_handler\n",
    "    def analyze_dataset_structure(self):\n",
    "        \"\"\"Analyze the structure of the full dataset\"\"\"\n",
    "        if not self.s3_client:\n",
    "            logger.error(\"‚ùå S3 client not available\")\n",
    "            return {}\n",
    "        \n",
    "        # Get list of all dataset files\n",
    "        response = self.s3_client.list_objects_v2(Bucket=DATASET_BUCKET, Prefix=DATASET_PREFIX)\n",
    "        \n",
    "        dataset_stats = {}\n",
    "        total_size = 0\n",
    "        \n",
    "        for obj in response.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            size = obj['Size']\n",
    "            total_size += size\n",
    "            \n",
    "            # Extract area name from key\n",
    "            area_name = key.split('/')[-1].split('_')[0]\n",
    "            \n",
    "            if area_name not in dataset_stats:\n",
    "                dataset_stats[area_name] = {\n",
    "                    'files': [],\n",
    "                    'total_size': 0,\n",
    "                    'estimated_rows': 0\n",
    "                }\n",
    "            \n",
    "            dataset_stats[area_name]['files'].append(key)\n",
    "            dataset_stats[area_name]['total_size'] += size\n",
    "            # Rough estimate: assume each sample is ~200 bytes on average\n",
    "            dataset_stats[area_name]['estimated_rows'] += size // 200\n",
    "        \n",
    "        # Print summary\n",
    "        logger.info(f\"üìä Dataset Analysis Summary:\")\n",
    "        logger.info(f\"   Total size: {total_size / (1024*1024*1024):.2f} GB\")\n",
    "        \n",
    "        for area, stats in dataset_stats.items():\n",
    "            logger.info(f\"   {area}: {stats['estimated_rows']:,} rows, {stats['total_size'] / (1024*1024*1024):.2f} GB\")\n",
    "        \n",
    "        return dataset_stats\n",
    "    \n",
    "    @error_handler\n",
    "    def load_area_data_sample(self, area_name, max_samples=SAMPLE_SIZE_PER_AREA):\n",
    "        \"\"\"Load and sample area data with error handling\"\"\"\n",
    "        if not self.s3_client:\n",
    "            logger.error(\"‚ùå S3 client not available\")\n",
    "            raise DataLoadingError(\"S3 client not available\")\n",
    "            \n",
    "        file_key = f\"{DATASET_PREFIX}{self.area_files[area_name]}\"\n",
    "        \n",
    "        logger.info(f\"üì• Loading {area_name}: s3://{DATASET_BUCKET}/{file_key}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if file exists\n",
    "            try:\n",
    "                self.s3_client.head_object(Bucket=DATASET_BUCKET, Key=file_key)\n",
    "            except Exception as e:\n",
    "                raise DataLoadingError(f\"File not found: s3://{DATASET_BUCKET}/{file_key}\")\n",
    "            \n",
    "            # Load data in chunks with retry mechanism\n",
    "            max_retries = 3\n",
    "            retry_count = 0\n",
    "            \n",
    "            while retry_count < max_retries:\n",
    "                try:\n",
    "                    response = self.s3_client.get_object(Bucket=DATASET_BUCKET, Key=file_key)\n",
    "                    \n",
    "                    # Use pandas to read CSV in chunks\n",
    "                    chunk_size = 100000  # 100K rows per chunk\n",
    "                    chunks = []\n",
    "                    \n",
    "                    # Stream data from S3 in chunks and sample\n",
    "                    chunk_iter = pd.read_csv(response['Body'], chunksize=chunk_size)\n",
    "                    \n",
    "                    total_rows = 0\n",
    "                    for i, chunk in enumerate(chunk_iter):\n",
    "                        # Sample from chunk based on ratio\n",
    "                        if max_samples and total_rows + len(chunk) > max_samples:\n",
    "                            # Calculate how many more samples we need\n",
    "                            samples_needed = max_samples - total_rows\n",
    "                            if samples_needed > 0:\n",
    "                                # Sample remaining rows\n",
    "                                chunk = chunk.sample(n=samples_needed, random_state=RANDOM_SEED)\n",
    "                            else:\n",
    "                                break\n",
    "                        \n",
    "                        chunks.append(chunk)\n",
    "                        total_rows += len(chunk)\n",
    "                        \n",
    "                        logger.info(f\"   üìä Chunk {i+1}: {len(chunk):,} rows, Total: {total_rows:,}\")\n",
    "                        \n",
    "                        # Stop if we have enough samples\n",
    "                        if max_samples and total_rows >= max_samples:\n",
    "                            break\n",
    "                    \n",
    "                    # Combine chunks\n",
    "                    if not chunks:\n",
    "                        raise DataLoadingError(f\"No data loaded from {area_name}\")\n",
    "                    \n",
    "                    df = pd.concat(chunks, ignore_index=True)\n",
    "                    \n",
    "                    logger.info(f\"   üìä Loaded {len(df):,} rows from {area_name}\")\n",
    "                    \n",
    "                    # Apply temporal pattern preservation if enabled\n",
    "                    if PRESERVE_TEMPORAL_PATTERNS:\n",
    "                        df = self._preserve_temporal_patterns(df)\n",
    "                    \n",
    "                    # Apply class balancing if enabled\n",
    "                    if ENSURE_CLASS_BALANCE:\n",
    "                        df = self._balance_classes(df)\n",
    "                    \n",
    "                    return self._preprocess_area_data(df, area_name)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    retry_count += 1\n",
    "                    if retry_count < max_retries:\n",
    "                        wait_time = 2 ** retry_count  # Exponential backoff\n",
    "                        logger.warning(f\"   ‚ö†Ô∏è Error loading {area_name}, retrying in {wait_time}s: {e}\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        raise DataLoadingError(f\"Failed to load {area_name} after {max_retries} attempts: {e}\")\n",
    "            \n",
    "        except DataLoadingError as e:\n",
    "            # Re-raise specific errors\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # Wrap other exceptions\n",
    "            raise DataLoadingError(f\"Error loading {area_name}: {e}\")\n",
    "    \n",
    "    def _preserve_temporal_patterns(self, df, timestamp_col='timestamp'):\n",
    "        \"\"\"Ensure temporal patterns are preserved in the sampled data\"\"\"\n",
    "        \n",
    "        if timestamp_col in df.columns:\n",
    "            # Convert to datetime if not already\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
    "                df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "            \n",
    "            # Sort by timestamp\n",
    "            df = df.sort_values(timestamp_col).reset_index(drop=True)\n",
    "            \n",
    "            # Ensure we have continuous segments\n",
    "            # Calculate time differences\n",
    "            time_diffs = df[timestamp_col].diff()\n",
    "            \n",
    "            # Find large gaps (e.g., > 1 hour)\n",
    "            large_gaps = time_diffs > pd.Timedelta(hours=1)\n",
    "            segment_starts = df.index[large_gaps].tolist()\n",
    "            \n",
    "            if segment_starts:\n",
    "                logger.info(f\"   ‚ö†Ô∏è Found {len(segment_starts)} temporal gaps in data\")\n",
    "                \n",
    "                # Add start of dataframe as first segment\n",
    "                segment_starts = [0] + segment_starts\n",
    "                \n",
    "                # Add end of dataframe as last segment\n",
    "                segment_starts.append(len(df))\n",
    "                \n",
    "                # Create segments\n",
    "                segments = []\n",
    "                for i in range(len(segment_starts) - 1):\n",
    "                    start = segment_starts[i]\n",
    "                    end = segment_starts[i + 1]\n",
    "                    segments.append(df.iloc[start:end])\n",
    "                \n",
    "                # Sample from each segment proportionally\n",
    "                sampled_segments = []\n",
    "                for segment in segments:\n",
    "                    # Sample proportionally to segment size\n",
    "                    sample_size = int(len(segment) / len(df) * len(df))\n",
    "                    if sample_size > 0:\n",
    "                        sampled_segment = segment.sample(n=min(sample_size, len(segment)), \n",
    "                                                        random_state=RANDOM_SEED)\n",
    "                        sampled_segments.append(sampled_segment)\n",
    "                \n",
    "                # Combine segments and sort by timestamp\n",
    "                df = pd.concat(sampled_segments).sort_values(timestamp_col).reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _balance_classes(self, df):\n",
    "        \"\"\"Balance classes according to target distribution\"\"\"\n",
    "        \n",
    "        # Determine label column\n",
    "        label_col = None\n",
    "        if 'is_anomaly' in df.columns:\n",
    "            label_col = 'is_anomaly'\n",
    "        elif 'label' in df.columns:\n",
    "            label_col = 'label'\n",
    "        else:\n",
    "            # Generate synthetic labels based on first feature\n",
    "            feature_col = df.columns[0]\n",
    "            values = df[feature_col].values\n",
    "            q95 = np.percentile(values, 95)\n",
    "            q85 = np.percentile(values, 85)\n",
    "            \n",
    "            df['synthetic_label'] = 0\n",
    "            df.loc[values > q95, 'synthetic_label'] = 2  # Fire (top 5%)\n",
    "            df.loc[(values > q85) & (values <= q95), 'synthetic_label'] = 1  # Warning (85-95%)\n",
    "            label_col = 'synthetic_label'\n",
    "        \n",
    "        # Get current class counts\n",
    "        class_counts = df[label_col].value_counts().to_dict()\n",
    "        total_samples = len(df)\n",
    "        \n",
    "        # Calculate target counts\n",
    "        target_counts = {}\n",
    "        for cls, pct in CLASS_DISTRIBUTION_TARGET.items():\n",
    "            target_counts[cls] = int(total_samples * pct)\n",
    "        \n",
    "        # Adjust to ensure we have exactly total_samples\n",
    "        total_target = sum(target_counts.values())\n",
    "        if total_target != total_samples:\n",
    "            # Add/remove difference to/from majority class\n",
    "            majority_class = max(class_counts, key=class_counts.get)\n",
    "            target_counts[majority_class] += (total_samples - total_target)\n",
    "        \n",
    "        # Sample or duplicate to reach target counts\n",
    "        balanced_samples = []\n",
    "        \n",
    "        for cls, target_count in target_counts.items():\n",
    "            class_df = df[df[label_col] == cls]\n",
    "            current_count = len(class_df)\n",
    "            \n",
    "            if current_count == 0:\n",
    "                logger.warning(f\"   ‚ö†Ô∏è No samples for class {cls}\")\n",
    "                continue\n",
    "            \n",
    "            if current_count > target_count:\n",
    "                # Downsample\n",
    "                sampled = class_df.sample(n=target_count, random_state=RANDOM_SEED)\n",
    "            else:\n",
    "                # Upsample with replacement\n",
    "                sampled = class_df.sample(n=target_count, replace=True, random_state=RANDOM_SEED)\n",
    "            \n",
    "            balanced_samples.append(sampled)\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        balanced_df = pd.concat(balanced_samples).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "        \n",
    "        # Verify balanced distribution\n",
    "        balanced_distribution = balanced_df[label_col].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        logger.info(f\"   üìä Balanced class distribution:\")\n",
    "        logger.info(f\"      Normal (0): {balanced_distribution.get(0, 0):.2%}\")\n",
    "        logger.info(f\"      Warning (1): {balanced_distribution.get(1, 0):.2%}\")\n",
    "        logger.info(f\"      Fire (2): {balanced_distribution.get(2, 0):.2%}\")\n",
    "        \n",
    "        return balanced_df\n",
    "    \n",
    "    def _preprocess_area_data(self, df, area_name):\n",
    "        \"\"\"Preprocess area data for model training\"\"\"\n",
    "        \n",
    "        logger.info(f\"üîß Preprocessing {area_name}...\")\n",
    "        \n",
    "        # Handle timestamp\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Extract features\n",
    "        exclude_cols = ['timestamp', 'is_anomaly', 'label', 'synthetic_label']\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        \n",
    "        # Limit features by area type\n",
    "        if area_name == 'basement':\n",
    "            feature_cols = feature_cols[:4]\n",
    "        elif area_name == 'laundry':\n",
    "            feature_cols = feature_cols[:3]\n",
    "        else:\n",
    "            feature_cols = feature_cols[:2]\n",
    "        \n",
    "        X = df[feature_cols].fillna(0).values\n",
    "        \n",
    "        # Create intelligent labels\n",
    "        if 'is_anomaly' in df.columns:\n",
    "            y = df['is_anomaly'].values.astype(int)\n",
    "        elif 'label' in df.columns:\n",
    "            y = df['label'].values.astype(int)\n",
    "        elif 'synthetic_label' in df.columns:\n",
    "            y = df['synthetic_label'].values.astype(int)\n",
    "        else:\n",
    "            # Generate realistic fire detection labels\n",
    "            values = df[feature_cols[0]].values\n",
    "            q95 = np.percentile(values, 95)\n",
    "            q85 = np.percentile(values, 85)\n",
    "            \n",
    "            y = np.zeros(len(values))\n",
    "            y[values > q95] = 2  # Fire (top 5%)\n",
    "            y[(values > q85) & (values <= q95)] = 1  # Warning (85-95%)\n",
    "        \n",
    "        # Standardize to 6 features\n",
    "        if X.shape[1] < 6:\n",
    "            padding = np.zeros((X.shape[0], 6 - X.shape[1]))\n",
    "            X = np.hstack([X, padding])\n",
    "        elif X.shape[1] > 6:\n",
    "            X = X[:, :6]\n",
    "        \n",
    "        logger.info(f\"   ‚úÖ {area_name}: {X.shape}, anomaly_rate={np.mean(y > 0):.4f}\")\n",
    "        return X, y\n",
    "    \n",
    "    def create_sequences(self, X, y, seq_len=60, step=10):\n",
    "        \"\"\"Create time series sequences\"\"\"\n",
    "        \n",
    "        logger.info(f\"üîÑ Creating sequences with length {seq_len} and step {step}\")\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(0, len(X) - seq_len, step):\n",
    "            sequences.append(X[i:i+seq_len])\n",
    "            labels.append(y[i+seq_len-1])  # Use label of last timestep\n",
    "        \n",
    "        logger.info(f\"   ‚úÖ Created {len(sequences):,} sequences from {len(X):,} samples\")\n",
    "        \n",
    "        return np.array(sequences), np.array(labels)\n",
    "    \n",
    "    @error_handler\n",
    "    def load_all_data_sample(self):\n",
    "        \"\"\"Load and sample the complete dataset\"\"\"\n",
    "        \n",
    "        logger.info(\"üöÄ LOADING AND SAMPLING DATASET FROM S3\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        \n",
    "        # Analyze dataset structure\n",
    "        dataset_stats = self.analyze_dataset_structure()\n",
    "        \n",
    "        all_X = []\n",
    "        all_y = []\n",
    "        all_areas = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for area_idx, area_name in enumerate(self.area_files.keys()):\n",
    "            logger.info(f\"\\nüìÅ PROCESSING AREA {area_idx+1}/5: {area_name.upper()}\")\n",
    "            \n",
    "            X, y = self.load_area_data_sample(area_name, SAMPLE_SIZE_PER_AREA)\n",
    "            if X is None:\n",
    "                continue\n",
    "            \n",
    "            sequences, labels = self.create_sequences(X, y, seq_len=60, step=10)\n",
    "            areas = np.full(len(sequences), area_idx)\n",
    "            \n",
    "            all_X.append(sequences)\n",
    "            all_y.append(labels)\n",
    "            all_areas.append(areas)\n",
    "            \n",
    "            logger.info(f\"   ‚úÖ Created {len(sequences):,} sequences\")\n",
    "        \n",
    "        # Combine all areas\n",
    "        X_combined = np.vstack(all_X)\n",
    "        y_combined = np.hstack(all_y)\n",
    "        areas_combined = np.hstack(all_areas)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        logger.info(f\"\\nüéØ DATASET SUMMARY:\")\n",
    "        logger.info(f\"   üìä Total sequences: {X_combined.shape[0]:,}\")\n",
    "        logger.info(f\"   üìê Shape: {X_combined.shape}\")\n",
    "        logger.info(f\"   üìà Classes: {np.bincount(y_combined.astype(int))}\")\n",
    "        logger.info(f\"   üíæ Memory: {X_combined.nbytes / (1024**3):.2f} GB\")\n",
    "        logger.info(f\"   ‚è±Ô∏è Time: {total_time:.1f}s\")\n",
    "        \n",
    "        return X_combined, y_combined, areas_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Class Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_class_distribution(y_train, y_val, y_test):\n",
    "    \"\"\"Visualize class distribution across train/val/test sets\"\"\"\n",
    "    \n",
    "    class_names = ['Normal', 'Warning', 'Fire']\n",
    "    \n",
    "    # Count classes in each set\n",
    "    train_counts = np.bincount(y_train.astype(int), minlength=3)\n",
    "    val_counts = np.bincount(y_val.astype(int), minlength=3)\n",
    "    test_counts = np.bincount(y_test.astype(int), minlength=3)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    train_pct = train_counts / train_counts.sum() * 100\n",
    "    val_pct = val_counts / val_counts.sum() * 100\n",
    "    test_pct = test_counts / test_counts.sum() * 100\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Raw counts\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax1.bar(x - width, train_counts, width, label='Train')\n",
    "    ax1.bar(x, val_counts, width, label='Validation')\n",
    "    ax1.bar(x + width, test_counts, width, label='Test')\n",
    "    \n",
    "    ax1.set_title('Class Distribution (Counts)')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(class_names)\n",
    "    ax1.set_ylabel('Number of Samples')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, v in enumerate(train_counts):\n",
    "        ax1.text(i - width, v + 0.1, f\"{v:,}\", ha='center')\n",
    "    for i, v in enumerate(val_counts):\n",
    "        ax1.text(i, v + 0.1, f\"{v:,}\", ha='center')\n",
    "    for i, v in enumerate(test_counts):\n",
    "        ax1.text(i + width, v + 0.1, f\"{v:,}\", ha='center')\n",
    "    \n",
    "    # Percentages\n",
    "    ax2.bar(x - width, train_pct, width, label='Train')\n",
    "    ax2.bar(x, val_pct, width, label='Validation')\n",
    "    ax2.bar(x + width, test_pct, width, label='Test')\n",
    "    \n",
    "    ax2.set_title('Class Distribution (Percentage)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(class_names)\n",
    "    ax2.set_ylabel('Percentage (%)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(train_pct):\n",
    "        ax2.text(i - width, v + 0.5, f\"{v:.1f}%\", ha='center')\n",
    "    for i, v in enumerate(val_pct):\n",
    "        ax2.text(i, v + 0.5, f\"{v:.1f}%\", ha='center')\n",
    "    for i, v in enumerate(test_pct):\n",
    "        ax2.text(i + width, v + 0.5, f\"{v:.1f}%\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if enabled\n",
    "    if VISUALIZATION_CONFIG['save_figures']:\n",
    "        plt.savefig(f\"{VISUALIZATION_CONFIG['figure_dir']}/class_distribution.png\", dpi=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = OptimizedDataLoader()\n",
    "\n",
    "# Load and sample data\n",
    "X, y, areas = data_loader.load_all_data_sample()\n",
    "\n",
    "# Split data into train/validation/test sets\n",
    "X_train, X_test, y_train, y_test, areas_train, areas_test = train_test_split(\n",
    "    X, y, areas, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, areas_train, areas_val = train_test_split(\n",
    "    X_train, y_train, areas_train, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train\n",
    ")\n",
    "\n",
    "# Print data split summary\n",
    "logger.info(f\"üìä Data splits:\"