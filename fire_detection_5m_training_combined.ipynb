{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Fire Detection AI - 5M Dataset Training (Combined)\n",
    "\n",
    "This notebook implements an optimized training pipeline for Fire Detection AI models using a 5M sample from the 50M dataset. The goal is to significantly reduce training time from 43 hours while maintaining reasonable accuracy.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Optimized Data Sampling**: Stratified sampling with temporal pattern preservation\n",
    "- **Optimized Model Architecture**: Reduced transformer size with efficient ML ensemble\n",
    "- **Enhanced Visualizations**: Real-time training dashboard and comprehensive visualizations\n",
    "- **Comprehensive Error Handling**: Custom exceptions, recovery mechanisms, and checkpointing\n",
    "- **Detailed Logging**: Multi-destination logging with structured format\n",
    "- **Multi-GPU Support**: Optimized for ml.p3.16xlarge with 8 NVIDIA V100 GPUs\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- **Training Time**: 2-4 hours (vs 43 hours for full dataset)\n",
    "- **Model Accuracy**: 94-96% (vs 97-98% for full dataset)\n",
    "- **Cost Savings**: ~90% reduction in training costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import functools\n",
    "import threading\n",
    "import gc\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "\n",
    "# Machine learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy import stats\n",
    "\n",
    "# AWS libraries\n",
    "try:\n",
    "    import boto3\n",
    "    import sagemaker\n",
    "    from botocore.exceptions import ClientError\n",
    "    AWS_AVAILABLE = True\n",
    "    print(\"‚úÖ AWS libraries available\")\n",
    "except ImportError:\n",
    "    AWS_AVAILABLE = False\n",
    "    print(\"‚ùå AWS libraries not available - install with: pip install boto3 sagemaker\")\n",
    "\n",
    "# Optional ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ùå XGBoost not available - install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"‚úÖ LightGBM available\")\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"‚ùå LightGBM not available - install with: pip install lightgbm\")\n",
    "\n",
    "# Configure notebook settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GPU Setup for ml.p3.16xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Set environment variables for optimal performance\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'  # All 8 GPUs on p3.16xlarge\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1'  # May help with some multi-GPU issues\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to monitor GPU memory\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nGPU Memory Usage:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB\")\n",
    "            print(f\"  Memory Reserved: {torch.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")\n",
    "\n",
    "# Run nvidia-smi for detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_BUCKET = \"processedd-synthetic-data\"\n",
    "DATASET_PREFIX = \"cleaned-data/\"\n",
    "SAMPLE_SIZE_PER_AREA = 1000000  # 1M samples per area = 5M total\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Sampling configuration\n",
    "PRESERVE_TEMPORAL_PATTERNS = True\n",
    "ENSURE_CLASS_BALANCE = True\n",
    "CLASS_DISTRIBUTION_TARGET = {\n",
    "    0: 0.70,  # Normal: 70%\n",
    "    1: 0.20,  # Warning: 20%\n",
    "    2: 0.10   # Fire: 10%\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 50  # Reduced from 100\n",
    "BATCH_SIZE = 256\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "# Model configuration\n",
    "TRANSFORMER_CONFIG = {\n",
    "    'd_model': 128,  # Reduced from 256\n",
    "    'num_heads': 4,  # Reduced from 8\n",
    "    'num_layers': 3,  # Reduced from 6\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Visualization configuration\n",
    "VISUALIZATION_CONFIG = {\n",
    "    'update_interval': 1,  # Update visualizations every N epochs\n",
    "    'save_figures': True,\n",
    "    'figure_dir': 'figures',\n",
    "    'create_animations': True\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs(VISUALIZATION_CONFIG['figure_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Configure comprehensive logging for the training process\"\"\"\n",
    "    \n",
    "    # Generate timestamp for log files\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = f'logs/fire_detection_5m_training_{timestamp}.log'\n",
    "    \n",
    "    # Configure root logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Get root logger\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    logger.handlers = []\n",
    "    \n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    \n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    \n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # Create S3 handler if AWS is available\n",
    "    if AWS_AVAILABLE:\n",
    "        try:\n",
    "            s3_handler = S3LogHandler(\n",
    "                bucket=DATASET_BUCKET,\n",
    "                key_prefix=f'logs/fire_detection_5m_training_{timestamp}/'\n",
    "            )\n",
    "            s3_handler.setLevel(logging.INFO)\n",
    "            s3_handler.setFormatter(file_formatter)\n",
    "            logger.addHandler(s3_handler)\n",
    "            logger.info(\"‚úÖ S3 logging configured\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Failed to configure S3 logging: {e}\")\n",
    "    \n",
    "    logger.info(\"üîß Logging configured successfully\")\n",
    "    logger.info(f\"üìù Log file: {log_file}\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class S3LogHandler(logging.Handler):\n",
    "    \"\"\"Custom logging handler that writes logs to S3\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket, key_prefix):\n",
    "        super().__init__()\n",
    "        self.bucket = bucket\n",
    "        self.key_prefix = key_prefix\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 100  # Number of logs to buffer before writing to S3\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Start background thread for uploading logs\n",
    "        self.stop_event = threading.Event()\n",
    "        self.upload_thread = threading.Thread(target=self._upload_logs_periodically)\n",
    "        self.upload_thread.daemon = True\n",
    "        self.upload_thread.start()\n",
    "    \n",
    "    def emit(self, record):\n",
    "        \"\"\"Process a log record\"\"\"\n",
    "        log_entry = self.format(record)\n",
    "        self.buffer.append(log_entry)\n",
    "        \n",
    "        # Upload logs if buffer is full\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self._upload_logs()\n",
    "    \n",
    "    def _upload_logs(self):\n",
    "        \"\"\"Upload buffered logs to S3\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Create log content\n",
    "            log_content = '\\n'.join(self.buffer)\n",
    "            \n",
    "            # Generate key with timestamp\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            key = f\"{self.key_prefix}log_{timestamp}.txt\"\n",
    "            \n",
    "            # Upload to S3\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=self.bucket,\n",
    "                Key=key,\n",
    "                Body=log_content\n",
    "            )\n",
    "            \n",
    "            # Clear buffer\n",
    "            self.buffer = []\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Don't raise exception, just print to stderr\n",
    "            print(f\"Error uploading logs to S3: {e}\", file=sys.stderr)\n",
    "    \n",
    "    def _upload_logs_periodically(self):\n",
    "        \"\"\"Upload logs periodically in background thread\"\"\"\n",
    "        while not self.stop_event.is_set():\n",
    "            time.sleep(60)  # Upload every minute\n",
    "            self._upload_logs()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up handler resources\"\"\"\n",
    "        self.stop_event.set()\n",
    "        self._upload_logs()  # Final upload\n",
    "        super().close()\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Error Handling Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingError(Exception):\n",
    "    \"\"\"Base class for training-related exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class DataLoadingError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during data loading\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelInitializationError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model initialization\"\"\"\n",
    "    pass\n",
    "\n",
    "class TrainingProcessError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during training process\"\"\"\n",
    "    pass\n",
    "\n",
    "class EvaluationError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model evaluation\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelSavingError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model saving\"\"\"\n",
    "    pass\n",
    "\n",
    "def error_handler(func):\n",
    "    \"\"\"Decorator for handling errors in functions\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TrainingError as e:\n",
    "            # Log specific training errors\n",
    "            logger.error(f\"‚ùå {e.__class__.__name__}: {str(e)}\")\n",
    "            # Re-raise to be caught by global handler\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # Log unexpected errors\n",
    "            logger.error(f\"‚ùå Unexpected error in {func.__name__}: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            # Wrap in TrainingProcessError\n",
    "            raise TrainingProcessError(f\"Error in {func.__name__}: {str(e)}\") from e\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Synthetic Data\n",
    "\n",
    "For demonstration purposes, we'll create synthetic data. In a real scenario, you would load data from S3 or local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def create_synthetic_data(n_samples=5000000, n_features=6, n_timesteps=60, n_areas=5):\n",
    "    \"\"\"Create synthetic data for demonstration\"\"\"\n",
    "    logger.info(f\"Creating synthetic data with {n_samples} samples\")\n",
    "    \n",
    "    # Create features\n",
    "    X = np.random.randn(n_samples, n_timesteps, n_features).astype(np.float32)\n",
    "    \n",
    "    # Create labels (0: normal, 1: warning, 2: fire)\n",
    "    y = np.random.choice([0, 1, 2], size=n_samples, p=[0.7, 0.2, 0.1])\n",
    "    \n",
    "    # Create area IDs\n",
    "    areas = np.random.choice(range(n_areas), size=n_samples)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Created synthetic data: X={X.shape}, y={y.shape}, areas={areas.shape}\")\n",
    "    \n",
    "    return X, y, areas\n",
    "\n",
    "@error_handler\n",
    "def load_data_from_s3(bucket, prefix, sample_size_per_area=1000000):\n",
    "    \"\"\"Load data from S3 bucket with sampling\"\"\"\n",
    "    if not AWS_AVAILABLE:\n",
    "        raise DataLoadingError(\"AWS libraries not available\")\n",
    "    \n",
    "    logger.info(f\"Loading data from S3: s3://{bucket}/{prefix}\")\n",
    "    \n",
    "    try:\n",
    "        # This is a placeholder for actual S3 data loading\n",
    "        # In a real implementation, you would use boto3 to load data from S3\n",
    "        logger.warning(\"S3 data loading not implemented - using synthetic data instead\")\n",
    "        return create_synthetic_data(sample_size_per_area * 5)\n",
    "    except Exception as e:\n",
    "        raise DataLoadingError(f\"Failed to load data from S3: {str(e)}\")\n",
    "\n",
    "@error_handler\n",
    "def stratified_temporal_sampling(X, y, areas, sample_size_per_area, class_distribution=None):\n",
    "    \"\"\"Perform stratified sampling while preserving temporal patterns\"\"\"\n",
    "    logger.info(\"Performing stratified temporal sampling\")\n",
    "    \n",
    "    unique_areas = np.unique(areas)\n",
    "    X_sampled_list = []\n",
    "    y_sampled_list = []\n",
    "    areas_sampled_list = []\n",
    "    \n",
    "    for area in unique_areas:\n",
    "        # Get data for this area\n",
    "        area_mask = areas == area\n",
    "        X_area = X[area_mask]\n",
    "        y_area = y[area_mask]\n",
    "        \n",
    "        # Sample from each class according to target distribution\n",
    "        if class_distribution is not None:\n",
    "            X_area_sampled = []\n",
    "            y_area_sampled = []\n",
    "            \n",
    "            for class_label, target_proportion in class_distribution.items():\n",
    "                class_mask = y_area == class_label\n",
    "                X_class = X_area[class_mask]\n",
    "                y_class = y_area[class_mask]\n",
    "                \n",
    "                # Calculate number of samples for this class\n",
    "                n_samples = int(sample_size_per_area * target_proportion)\n",
    "                \n",
    "                # If we don't have enough samples, use what we have\n",
    "                if len(X_class) <= n_samples:\n",
    "                    X_area_sampled.append(X_class)\n",
    "                    y_area_sampled.append(y_class)\n",
    "                else:\n",
    "                    # Sample with temporal preservation (take contiguous blocks)\n",
    "                    block_size = max(1, len(X_class) // n_samples)\n",
    "                    indices = np.arange(0, len(X_class), block_size)[:n_samples]\n",
    "                    X_area_sampled.append(X_class[indices])\n",
    "                    y_area_sampled.append(y_class[indices])\n",
    "            \n",
    "            # Combine samples from all classes\n",
    "            X_area = np.vstack(X_area_sampled)\n",
    "            y_area = np.concatenate(y_area_sampled)\n",
    "        else:\n",
    "            # Simple random sampling if no class distribution specified\n",
    "            if len(X_area) > sample_size_per_area:\n",
    "                indices = np.random.choice(len(X_area), sample_size_per_area, replace=False)\n",
    "                X_area = X_area[indices]\n",
    "                y_area = y_area[indices]\n",
    "        \n",
    "        # Add to result lists\n",
    "        X_sampled_list.append(X_area)\n",
    "        y_sampled_list.append(y_area)\n",
    "        areas_sampled_list.append(np.full(len(X_area), area))\n",
    "    \n",
    "    # Combine results from all areas\n",
    "    X_sampled = np.vstack(X_sampled_list)\n",
    "    y_sampled = np.concatenate(y_sampled_list)\n",
    "    areas_sampled = np.concatenate(areas_sampled_list)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Sampled data: X={X_sampled.shape}, y={y_sampled.shape}, areas={areas_sampled.shape}\")\n",
    "    \n",
    "    return X_sampled, y_sampled, areas_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start notebook timer\n",
    "notebook_start_time = time.time()\n",
    "\n",
    "# Load data (using synthetic data for demonstration)\n",
    "logger.info(\"üîÑ Loading and preparing data...\")\n",
    "\n",
    "try:\n",
    "    # Try to load from S3 if AWS is available\n",
    "    if AWS_AVAILABLE:\n",
    "        X, y, areas = load_data_from_s3(\n",
    "            bucket=DATASET_BUCKET,\n",
    "            prefix=DATASET_PREFIX,\n",
    "            sample_size_per_area=SAMPLE_SIZE_PER_AREA\n",
    "        )\n",
    "    else:\n",
    "        # Create synthetic data if AWS is not available\n",
    "        X, y, areas = create_synthetic_data(\n",
    "            n_samples=SAMPLE_SIZE_PER_AREA * 5,  # 5 areas\n",
    "            n_features=6,\n",
    "            n_timesteps=60,\n",
    "            n_areas=5\n",
    "        )\n",
    "    \n",
    "    # For demonstration, use a smaller subset to speed up execution\n",
    "    # Remove this in production\n",
    "    demo_size = 50000  # Small size for demonstration\n",
    "    logger.info(f\"‚ö†Ô∏è Using small subset ({demo_size} samples) for demonstration\")\n",
    "    indices = np.random.choice(len(X), demo_size, replace=False)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    areas = areas[indices]\n",
    "    \n",
    "    # Perform stratified sampling if needed\n",
    "    if ENSURE_CLASS_BALANCE:\n",
    "        X, y, areas = stratified_temporal_sampling(\n",
    "            X, y, areas,\n",
    "            sample_size_per_area=SAMPLE_SIZE_PER_AREA // 5,  # Adjust for demo\n",
    "            class_distribution=CLASS_DISTRIBUTION_TARGET\n",
    "        )\n",
    "    \n",
    "    # Split into train/validation sets\n",
    "    X_train, X_val, y_train, y_val, areas_train, areas_val = train_test_split(\n",
    "        X, y, areas, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Train set: X={X_train.shape}, y={y_train.shape}, areas={areas_train.shape}\")\n",
    