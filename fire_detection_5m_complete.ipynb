{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Detection AI - 5M Dataset Training\n",
    "\n",
    "This notebook trains a fire detection model using a 5M sample dataset instead of the full 50M dataset to reduce training time while maintaining reasonable accuracy. It's optimized for AWS SageMaker with an ml.p3.16xlarge instance (8 NVIDIA V100 GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Install other required packages\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "!pip install \"numexpr>=2.8.4\"  # Update numexpr to required version\n",
    "!pip install xgboost lightgbm\n",
    "!pip install boto3 sagemaker\n",
    "\n",
    "print(\"\\nAfter running this cell, restart the kernel (Kernel > Restart) and then continue with the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import functools\n",
    "import threading\n",
    "import gc\n",
    "import pickle\n",
    "import math\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Machine learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy import stats\n",
    "\n",
    "# AWS libraries\n",
    "try:\n",
    "    import boto3\n",
    "    import sagemaker\n",
    "    from botocore.exceptions import ClientError\n",
    "    AWS_AVAILABLE = True\n",
    "    print(\"âœ… AWS libraries available\")\n",
    "except ImportError:\n",
    "    AWS_AVAILABLE = False\n",
    "    print(\"âŒ AWS libraries not available - install with: pip install boto3 sagemaker\")\n",
    "\n",
    "# Optional ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"âœ… XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"âŒ XGBoost not available - install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"âœ… LightGBM available\")\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"âŒ LightGBM not available - install with: pip install lightgbm\")\n",
    "\n",
    "# Configure notebook settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Setup for ml.p3.16xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Set environment variables for optimal performance\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'  # All 8 GPUs on p3.16xlarge\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1'  # May help with some multi-GPU issues\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to monitor GPU memory\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nGPU Memory Usage:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB\")\n",
    "            print(f\"  Memory Reserved: {torch.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")\n",
    "\n",
    "# Run nvidia-smi for detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_BUCKET = \"synthetic-data-4\"\n",
    "DATASET_PREFIX = \"datasets/\"\n",
    "SAMPLE_SIZE = 5000000  # 5M samples\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Sampling configuration\n",
    "PRESERVE_TEMPORAL_PATTERNS = True\n",
    "ENSURE_CLASS_BALANCE = True\n",
    "CLASS_DISTRIBUTION_TARGET = {\n",
    "    0: 0.70,  # Normal: 70%\n",
    "    1: 0.20,  # Warning: 20%\n",
    "    2: 0.10   # Fire: 10%\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 50  # Reduced from 100\n",
    "BATCH_SIZE = 256\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "# Model configuration\n",
    "TRANSFORMER_CONFIG = {\n",
    "    'd_model': 128,  # Reduced from 256\n",
    "    'num_heads': 4,  # Reduced from 8\n",
    "    'num_layers': 3,  # Reduced from 6\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Visualization configuration\n",
    "VISUALIZATION_CONFIG = {\n",
    "    'update_interval': 1,  # Update visualizations every N epochs\n",
    "    'save_figures': True,\n",
    "    'figure_dir': 'figures',\n",
    "    'create_animations': True\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs(VISUALIZATION_CONFIG['figure_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Configure comprehensive logging for the training process\"\"\"\n",
    "    \n",
    "    # Generate timestamp for log files\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = f'logs/fire_detection_5m_training_{timestamp}.log'\n",
    "    \n",
    "    # Configure root logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Get root logger\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    logger.handlers = []\n",
    "    \n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    \n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    \n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # Create S3 handler if AWS is available\n",
    "    if AWS_AVAILABLE:\n",
    "        try:\n",
    "            s3_handler = S3LogHandler(\n",
    "                bucket=DATASET_BUCKET,\n",
    "                key_prefix=f'logs/fire_detection_5m_training_{timestamp}/'\n",
    "            )\n",
    "            s3_handler.setLevel(logging.INFO)\n",
    "            s3_handler.setFormatter(file_formatter)\n",
    "            logger.addHandler(s3_handler)\n",
    "            logger.info(\"âœ… S3 logging configured\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"âš ï¸ Failed to configure S3 logging: {e}\")\n",
    "    \n",
    "    logger.info(\"ðŸ”§ Logging configured successfully\")\n",
    "    logger.info(f\"ðŸ“ Log file: {log_file}\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class S3LogHandler(logging.Handler):\n",
    "    \"\"\"Custom logging handler that writes logs to S3\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket, key_prefix):\n",
    "        super().__init__()\n",
    "        self.bucket = bucket\n",
    "        self.key_prefix = key_prefix\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 100  # Number of logs to buffer before writing to S3\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Start background thread for uploading logs\n",
    "        self.stop_event = threading.Event()\n",
    "        self.upload_thread = threading.Thread(target=self._upload_logs_periodically)\n",
    "        self.upload_thread.daemon = True\n",
    "        self.upload_thread.start()\n",
    "    \n",
    "    def emit(self, record):\n",
    "        \"\"\"Process a log record\"\"\"\n",
    "        log_entry = self.format(record)\n",
    "        self.buffer.append(log_entry)\n",
    "        \n",
    "        # Upload logs if buffer is full\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self._upload_logs()\n",
    "    \n",
    "    def _upload_logs(self):\n",
    "        \"\"\"Upload buffered logs to S3\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Create log content\n",
    "            log_content = '\\n'.join(self.buffer)\n",
    "            \n",
    "            # Generate key with timestamp\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            key = f\"{self.key_prefix}log_{timestamp}.txt\"\n",
    "            \n",
    "            # Upload to S3\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=self.bucket,\n",
    "                Key=key,\n",
    "                Body=log_content\n",
    "            )\n",
    "            \n",
    "            # Clear buffer\n",
    "            self.buffer = []\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Don't raise exception, just print to stderr\n",
    "            print(f\"Error uploading logs to S3: {e}\", file=sys.stderr)\n",
    "    \n",
    "    def _upload_logs_periodically(self):\n",
    "        \"\"\"Upload logs periodically in background thread\"\"\"\n",
    "        while not self.stop_event.is_set():\n",
    "            time.sleep(60)  # Upload every minute\n",
    "            self._upload_logs()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up handler resources\"\"\"\n",
    "        self.stop_event.set()\n",
    "        self._upload_logs()  # Final upload\n",
    "        super().close()\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingError(Exception):\n",
    "    \"\"\"Base class for training-related exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class DataLoadingError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during data loading\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelInitializationError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model initialization\"\"\"\n",
    "    pass\n",
    "\n",
    "class TrainingProcessError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during training process\"\"\"\n",
    "    pass\n",
    "\n",
    "class EvaluationError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model evaluation\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelSavingError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model saving\"\"\"\n",
    "    pass\n",
    "\n",
    "def error_handler(func):\n",
    "    \"\"\"Decorator for handling errors in functions\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TrainingError as e:\n",
    "            # Log specific training errors\n",
    "            logger.error(f\"âŒ {e.__class__.__name__}: {str(e)}\")\n",
    "            # Re-raise to be caught by global handler\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # Log unexpected errors\n",
    "            logger.error(f\"âŒ Unexpected error in {func.__name__}: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            # Wrap in TrainingProcessError\n",
    "            raise TrainingProcessError(f\"Error in {func.__name__}: {str(e)}\") from e\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def create_synthetic_data(n_samples=5000000, n_features=6, n_timesteps=60, n_areas=5):\n",
    "    \"\"\"Create synthetic data for demonstration\"\"\"\n",
    "    logger.info(f\"Creating synthetic data with {n_samples} samples\")\n",
    "    \n",
    "    # Create features\n",
    "    X = np.random.randn(n_samples, n_features).astype(np.float32)\n",
    "    \n",
    "    # Create labels (0: normal, 1: warning, 2: fire)\n",
    "    y = np.random.choice([0, 1, 2], size=n_samples, p=[0.7, 0.2, 0.1])\n",
    "    \n",
    "    # Create area IDs\n",
    "    areas = np.random.choice(range(n_areas), size=n_samples)\n",
    "    \n",
    "    logger.info(f\"âœ… Created synthetic data: X={X.shape}, y={y.shape}, areas={areas.shape}\")\n",
    "    \n",
    "    return X, y, areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we should use saved numpy files\n",
    "if os.path.exists('X_train.npy') and os.path.exists('y_train.npy'):\n",
    "    logger.info(\"Loading data from saved numpy files...\")\n",
    "    X_train = np.load('X_train.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    X_val = np.load('X_val.npy')\n",
    "    y_val = np.load('y_val.npy')\n",
    "    X_test = np.load('X_test.npy')\n",
    "    y_test = np.load('y_test.npy')\n",
    "    \n",
    "    if os.path.exists('areas_train.npy'):\n",
    "        areas_train = np.load('areas_train.npy')\n",
    "        areas_val = np.load('areas_val.npy')\n",
    "        areas_test = np.load('areas_test.npy')\n",
    "    else:\n",
    "        # Create dummy area values if not available\n",
    "        areas_train = np.zeros(len(y_train), dtype=int)\n",
    "        areas_val = np.zeros(len(y_val), dtype=int)\n",
    "        areas_test = np.zeros(len(y_test), dtype=int)\n",
    "    \n",
    "    logger.info(f\"Loaded data: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "\n",
    "# If numpy files don't exist, try loading from S3\n",
    "elif AWS_AVAILABLE:\n",
    "    try:\n",
    "        # List files in S3\n",
    "        s3_client = boto3.client('s3')\n",
    "        logger.info(f\"Listing files in s3://{DATASET_BUCKET}/{DATASET_PREFIX}\")\n",
    "        response = s3_client.list_objects_v2(Bucket=DATASET_BUCKET, Prefix=DATASET_PREFIX)\n",
    "\n",
    "        if 'Contents' not in response:\n",
    "            logger.error(f\"No files found in s3://{DATASET_BUCKET}/{DATASET_PREFIX}\")\n",
    "            files = []\n",
    "        else:\n",
    "            files = [obj['Key'] for obj in response['Contents'] if not obj['Key'].endswith('/')]\n",
    "            \n",
    "            # Continue listing if there are more files\n",
    "            while response.get('IsTruncated', False):\n",
    "                continuation_token = response.get('NextContinuationToken')\n",
    "                response = s3_client.list_objects_v2(\n",
    "                    Bucket=DATASET_BUCKET, \n",
    "                    Prefix=DATASET_PREFIX,\n",
    "                    ContinuationToken=continuation_token\n",
    "                )\n",
    "                files.extend([obj['Key'] for obj in response['Contents'] if not obj['Key'].endswith('/')])\n",
    "\n",
    "        logger.info(f\"Found {len(files)} files in s3://{DATASET_BUCKET}/{DATASET_PREFIX}\")\n",
    "\n",
    "        # For demonstration, limit to first few files\n",
    "        files = files[:5]  # Adjust as needed\n",
    "        logger.info(f\"Using {len(files)} files for sampling\")\n",
    "\n",
    "        # Calculate samples per file\n",
    "        samples_per_file = SAMPLE_SIZE // len(files)\n",
    "        logger.info(f\"Sampling approximately {samples_per_file} rows from each file\")\n",
    "\n",
    "        # Sample data from S3\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        all_areas = []\n",
    "\n",
    "        for i, file_key in enumerate(files):\n",
    "            try:\n",
    "                logger.info(f\"Processing file {i+1}/{len(files)}: {file_key}\")\n",
    "                \n",
    "                # Get file from S3\n",
    "                response = s3_client.get_object(Bucket=DATASET_BUCKET, Key=file_key)\n",
    "                \n",
    "                # Extract area from filename\n",
    "                area_name = file_key.split('/')[-1].split('_')[0]\n",
    "                area_idx = hash(area_name) % 5  # Map to 5 areas\n",
    "                \n",
    "                # Read file in chunks\n",
    "                chunk_size = 100000  # 100K rows per chunk\n",
    "                \n",
    "                # Stream data from S3 in chunks and sample\n",
    "                chunk_iter = pd.read_csv(response['Body'], chunksize=chunk_size)\n",
    "                \n",
    "                total_rows = 0\n",
    "                for chunk in chunk_iter:\n",
    "                    # Sample from chunk\n",
    "                    if len(chunk) > samples_per_file:\n",
    "                        chunk = chunk.sample(n=samples_per_file, random_state=RANDOM_SEED)\n",
    "                    \n",
    "                    # Extract features and labels\n",
    "                    feature_cols = [col for col in chunk.columns if col not in ['timestamp', 'label', 'is_anomaly']]\n",
    "                    \n",
    "                    if 'label' in chunk.columns:\n",
    "                        labels = chunk['label'].values\n",
    "                    elif 'is_anomaly' in chunk.columns:\n",
    "                        labels = chunk['is_anomaly'].values\n",
    "                    else:\n",
    "                        # Generate synthetic labels\n",
    "                        values = chunk[feature_cols[0]].values\n",
    "                        q95 = np.percentile(values, 95)\n",
    "                        q85 = np.percentile(values, 85)\n",
    "                        \n",
    "                        labels = np.zeros(len(values))\n",
    "                        labels[values > q95] = 2  # Fire (top 5%)\n",
    "                        labels[(values > q85) & (values <= q95)] = 1  # Warning (85-95%)\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = chunk[feature_cols].values\n",
    "                    \n",
    "                    # Standardize to 6 features\n",
    "                    if features.shape[1] < 6:\n",
    "                        padding = np.zeros((features.shape[0], 6 - features.shape[1]))\n",
    "                        features = np.hstack([features, padding])\n",
    "                    elif features.shape[1] > 6:\n",
    "                        features = features[:, :6]\n",
    "                    \n",
    "                    # Add to lists\n",
    "                    all_data.append(features)\n",
    "                    all_labels.append(labels)\n",
    "                    all_areas.append(np.full(len(labels), area_idx))\n",
    "                    \n",
    "                    total_rows += len(chunk)\n",
    "                    logger.info(f\"  Sampled {total_rows} rows from {file_key}\")\n",
    "                    \n",
    "                    # Stop if we have enough samples from this file\n",
    "                    if total_rows >= samples_per_file:\n",
    "                        break\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {file_key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all data\n",
    "        X = np.vstack(all_data)\n",
    "        y = np.concatenate(all_labels)\n",
    "        areas = np.concatenate(all_areas)\n",
    "        \n",
    "        # Ensure we have exactly the requested sample size\n",
    "        if len(X) > SAMPLE_SIZE:\n",
    "            indices = np.random.choice(len(X), SAMPLE_SIZE, replace=False)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "            areas = areas[indices]\n",
    "        \n",
    "        logger.info(f\"Raw dataset: X={X.shape}, y={y.shape}, areas={areas.shape}\")\n",
    "        \n",
    "        # Create sequences\n",
    "        seq_len = 60\n",
    "        step = 10\n