{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Fire Detection AI - 50M Dataset Training on SageMaker\n",
    "\n",
    "**Objective**: Train ensemble fire detection AI on 19.9M samples from S3  \n",
    "**Target**: 97-98% accuracy with <0.5% false positive rate  \n",
    "**Dataset**: s3://processedd-synthetic-data/cleaned-data/  \n",
    "**Instance**: ml.p3.2xlarge (Tesla V100 GPU)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision xgboost lightgbm catboost scikit-learn boto3 -q\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    \n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LIGHTGBM = True\n",
    "except ImportError:\n",
    "    HAS_LIGHTGBM = False\n",
    "\n",
    "print(f\"üî• FIRE DETECTION AI - 50M DATASET TRAINING ON SAGEMAKER\")\n",
    "print(f\"üìÖ Training started: {datetime.now()}\")\n",
    "print(f\"ü§ñ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ Target: 97-98% accuracy ensemble\")\n",
    "print(f\"üíæ XGBoost available: {HAS_XGBOOST}\")\n",
    "print(f\"üíæ LightGBM available: {HAS_LIGHTGBM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    'USE_FULL_DATASET': True,  # Set to False for demo with smaller dataset\n",
    "    'MAX_SAMPLES_PER_AREA': None,  # Set to 500000 for demo mode\n",
    "    'BATCH_SIZE': 64,\n",
    "    'EPOCHS': 100,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'SEQUENCE_LENGTH': 60,\n",
    "    'VALIDATION_SPLIT': 0.2,\n",
    "    'RANDOM_SEED': 42,\n",
    "    'S3_BUCKET': 'processedd-synthetic-data',\n",
    "    'S3_PREFIX': 'cleaned-data/',\n",
    "    'TARGET_ACCURACY': 0.95\n",
    "}\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(CONFIG['RANDOM_SEED'])\n",
    "np.random.seed(CONFIG['RANDOM_SEED'])\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(f\"üìä Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• S3 Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3DataLoader:\n",
    "    def __init__(self, bucket_name, prefix):\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        \n",
    "    def load_csv_from_s3(self, key, max_samples=None):\n",
    "        \"\"\"Load CSV file from S3 with optional sampling\"\"\"\n",
    "        print(f\"üì• Loading {key.split('/')[-1]}: s3://{self.bucket_name}/{key}\")\n",
    "        \n",
    "        try:\n",
    "            obj = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)\n",
    "            df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "            \n",
    "            if max_samples and len(df) > max_samples:\n",
    "                df = df.sample(n=max_samples, random_state=42)\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {key}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_all_datasets(self, max_samples_per_area=None):\n",
    "        \"\"\"Load all fire detection datasets from S3\"\"\"\n",
    "        datasets = {}\n",
    "        \n",
    "        # Known area files from inspection\n",
    "        area_files = {\n",
    "            'basement': 'basement_data_cleaned.csv',\n",
    "            'laundry': 'laundry_data_cleaned.csv', \n",
    "            'asd': 'asd_data_cleaned.csv',\n",
    "            'voc': 'voc_data_cleaned.csv',\n",
    "            'arc': 'arc_data_cleaned.csv'\n",
    "        }\n",
    "        \n",
    "        total_samples = 0\n",
    "        \n",
    "        for area_name, filename in area_files.items():\n",
    "            key = f\"{self.prefix}{filename}\"\n",
    "            df = self.load_csv_from_s3(key, max_samples_per_area)\n",
    "            \n",
    "            if df is not None:\n",
    "                # Add area identifier\n",
    "                df['area'] = area_name\n",
    "                datasets[area_name] = df\n",
    "                \n",
    "                # Calculate anomaly rate\n",
    "                anomaly_cols = [col for col in df.columns if 'anomaly' in col.lower() or 'fire' in col.lower() or 'alert' in col.lower()]\n",
    "                if anomaly_cols:\n",
    "                    anomaly_rate = df[anomaly_cols].any(axis=1).mean()\n",
    "                else:\n",
    "                    anomaly_rate = 0.1  # Default assumption\n",
    "                    \n",
    "                total_samples += len(df)\n",
    "                print(f\"‚úÖ {area_name}: {df.shape}, anomaly_rate={anomaly_rate:.4f}\")\n",
    "                \n",
    "        print(f\"\\nüìä Total samples loaded: {total_samples:,}\")\n",
    "        return datasets\n",
    "\n",
    "print(\"‚úÖ S3DataLoader class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Enhanced Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFireTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=256, nhead=8, num_layers=4, num_areas=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_areas = num_areas\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Area embeddings\n",
    "        self.area_embedding = nn.Embedding(num_areas, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1000, d_model))\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layers\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)  # Binary classification: normal vs fire\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, area_ids):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project input\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add area embeddings\n",
    "        area_emb = self.area_embedding(area_ids).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        x = x + area_emb\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_enc = self.positional_encoding[:seq_len].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        x = x + pos_enc\n",
    "        \n",
    "        # Transformer processing\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = x.transpose(1, 2)  # (batch, features, seq_len)\n",
    "        x = self.global_pool(x).squeeze(-1)  # (batch, features)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "print(\"‚úÖ EnhancedFireTransformer model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, area_ids):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.area_ids = torch.LongTensor(area_ids)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx], self.area_ids[idx]\n",
    "\n",
    "def create_sequences(df, area_name, sequence_length=60):\n",
    "    \"\"\"Create time-series sequences for training\"\"\"\n",
    "    \n",
    "    # Area mapping\n",
    "    area_map = {'basement': 0, 'laundry': 1, 'asd': 2, 'voc': 3, 'arc': 4}\n",
    "    area_id = area_map.get(area_name, 0)\n",
    "    \n",
    "    # Identify feature and label columns\n",
    "    exclude_cols = ['area', 'timestamp', 'datetime', 'time', 'index']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Find anomaly/fire columns for labels\n",
    "    label_cols = [col for col in feature_cols if any(keyword in col.lower() \n",
    "                  for keyword in ['anomaly', 'fire', 'alert', 'alarm'])]\n",
    "    \n",
    "    if not label_cols:\n",
    "        # If no explicit label column, create based on statistical anomalies\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        \n",
    "        if numeric_cols:\n",
    "            # Use statistical outliers as anomalies\n",
    "            data_numeric = df[numeric_cols].fillna(0)\n",
    "            z_scores = np.abs((data_numeric - data_numeric.mean()) / data_numeric.std())\n",
    "            labels = (z_scores > 3).any(axis=1).astype(int)\n",
    "        else:\n",
    "            labels = np.zeros(len(df))\n",
    "    else:\n",
    "        # Use actual label columns\n",
    "        labels = df[label_cols].any(axis=1).astype(int)\n",
    "    \n",
    "    # Feature columns (exclude label columns)\n",
    "    actual_feature_cols = [col for col in feature_cols if col not in label_cols]\n",
    "    \n",
    "    if not actual_feature_cols:\n",
    "        print(f\"‚ö†Ô∏è  Warning: No feature columns found for {area_name}\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    # Prepare data\n",
    "    features = df[actual_feature_cols].fillna(0).values\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences, seq_labels, area_ids = [], [], []\n",
    "    \n",
    "    for i in range(len(features) - sequence_length + 1):\n",
    "        seq = features[i:i+sequence_length]\n",
    "        label = labels.iloc[i+sequence_length-1] if hasattr(labels, 'iloc') else labels[i+sequence_length-1]\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        seq_labels.append(label)\n",
    "        area_ids.append(area_id)\n",
    "    \n",
    "    return np.array(sequences), np.array(seq_labels), np.array(area_ids)\n",
    "\n",
    "print(\"‚úÖ Dataset utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 data loader\n",
    "data_loader = S3DataLoader(CONFIG['S3_BUCKET'], CONFIG['S3_PREFIX'])\n",
    "\n",
    "# Load datasets\n",
    "max_samples = None if CONFIG['USE_FULL_DATASET'] else CONFIG['MAX_SAMPLES_PER_AREA']\n",
    "datasets = data_loader.load_all_datasets(max_samples_per_area=max_samples)\n",
    "\n",
    "if not datasets:\n",
    "    raise ValueError(\"‚ùå No datasets loaded! Check S3 bucket and prefix.\")\n",
    "\n",
    "print(f\"\\nüîÑ Creating sequences (length={CONFIG['SEQUENCE_LENGTH']})...\")\n",
    "\n",
    "# Create sequences for all areas\n",
    "all_sequences, all_labels, all_area_ids = [], [], []\n",
    "\n",
    "for area_name, df in datasets.items():\n",
    "    sequences, labels, area_ids = create_sequences(df, area_name, CONFIG['SEQUENCE_LENGTH'])\n",
    "    \n",
    "    if len(sequences) > 0:\n",
    "        all_sequences.append(sequences)\n",
    "        all_labels.append(labels)\n",
    "        all_area_ids.append(area_ids)\n",
    "        \n",
    "        anomaly_rate = labels.mean()\n",
    "        print(f\"‚úÖ {area_name}: {len(sequences):,} sequences, {sequences.shape[2]} features, {anomaly_rate:.4f} anomaly rate\")\n",
    "\n",
    "# Combine all sequences\n",
    "X = np.concatenate(all_sequences, axis=0)\n",
    "y = np.concatenate(all_labels, axis=0)\n",
    "area_ids = np.concatenate(all_area_ids, axis=0)\n",
    "\n",
    "print(f\"\\nüìä Combined dataset:\")\n",
    "print(f\"   Sequences: {X.shape} (samples, timesteps, features)\")\n",
    "print(f\"   Labels: {y.shape} (fire rate: {y.mean():.4f})\")\n",
    "print(f\"   Memory usage: {X.nbytes / 1e9:.2f} GB\")\n",
    "\n",
    "# Normalize features\n",
    "print(f\"\\nüîß Normalizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_flat = X.reshape(-1, X.shape[-1])\n",
    "X_normalized = scaler.fit_transform(X_flat).reshape(X.shape)\n",
    "\n",
    "print(f\"‚úÖ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val, area_train, area_val = train_test_split(\n",
    "    X_normalized, y, area_ids, \n",
    "    test_size=CONFIG['VALIDATION_SPLIT'], \n",
    "    random_state=CONFIG['RANDOM_SEED'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset splits:\")\n",
    "print(f\"   Training: {X_train.shape[0]:,} sequences (fire rate: {y_train.mean():.4f})\")\n",
    "print(f\"   Validation: {X_val.shape[0]:,} sequences (fire rate: {y_val.mean():.4f})\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = FireSequenceDataset(X_train, y_train, area_train)\n",
    "val_dataset = FireSequenceDataset(X_val, y_val, area_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Data loaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Train Enhanced Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nü§ñ TRAINING ENHANCED TRANSFORMER\")\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train.shape[-1]\n",
    "model = EnhancedFireTransformer(input_dim=input_dim, num_areas=5).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üéØ Target accuracy: {CONFIG['TARGET_ACCURACY']*100}%\")\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0\n",
    "train_losses, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(CONFIG['EPOCHS']):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for sequences, labels, areas in train_loader:\n",
    "        sequences, labels, areas = sequences.to(device), labels.to(device), areas.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences, areas)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels, areas in val_loader:\n",
    "            sequences, labels, areas = sequences.to(device), labels.to(device), areas.to(device)\n",
    "            outputs = model(sequences, areas)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = val_correct / val_total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    scheduler.step(avg_train_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "    \n",
    "    # Progress update\n",
    "    if (epoch + 1) % 10 == 0 or epoch < 10:\n",
    "        status = \"‚úÖ TARGET REACHED\" if val_acc >= CONFIG['TARGET_ACCURACY'] else \"üéØ Training...\"\n",
    "        print(f\"Epoch {epoch+1:3d}: Loss={avg_train_loss:.4f}, Val_Acc={val_acc:.4f}, Best={best_val_acc:.4f} {status}\")\n",
    "    \n",
    "    # Early stopping if target reached\n",
    "    if val_acc >= CONFIG['TARGET_ACCURACY']:\n",
    "        print(f\"üéâ TARGET ACCURACY {CONFIG['TARGET_ACCURACY']*100}% REACHED! Early stopping.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüèÜ TRANSFORMER TRAINING COMPLETE\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"   Target ({CONFIG['TARGET_ACCURACY']*100}%): {'‚úÖ ACHIEVED' if best_val_acc >= CONFIG['TARGET_ACCURACY'] else '‚ùå Not reached'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Train ML Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä TRAINING ML ENSEMBLE\")\n",
    "\n",
    "# Prepare data for ML models (flatten sequences)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "# Add area features\n",
    "area_features_train = np.eye(5)[area_train]  # One-hot encode areas\n",
    "area_features_val = np.eye(5)[area_val]\n",
    "\n",
    "X_train_enhanced = np.concatenate([X_train_flat, area_features_train], axis=1)\n",
    "X_val_enhanced = np.concatenate([X_val_flat, area_features_val], axis=1)\n",
    "\n