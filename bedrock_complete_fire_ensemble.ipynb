{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• COMPLETE Fire Detection Ensemble - ALL 17+ Algorithms\n",
    "## AWS Bedrock Compatible - Maximum Performance System\n",
    "\n",
    "**Target: 97-98% accuracy with comprehensive ensemble of 17+ algorithms**\n",
    "\n",
    "### Architecture Overview:\n",
    "- **Tier 1**: Core Ensemble (5 models) - 97%+ accuracy\n",
    "- **Tier 2**: Specialist Models (9 models) - Context-specific\n",
    "- **Tier 3**: Meta-Learning (3 systems) - Optimal combination\n",
    "- **Total**: 17+ algorithms working together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Install ALL required packages for complete ensemble\n",
    "!pip install torch torchvision transformers pytorch-lightning -q\n",
    "!pip install xgboost lightgbm catboost scikit-learn -q\n",
    "!pip install pandas numpy matplotlib seaborn boto3 joblib scipy -q\n",
    "!pip install optuna imbalanced-learn tsfresh pykalman -q\n",
    "!pip install statsmodels pmdarima prophet neuralprophet -q\n",
    "!pip install torch-geometric pyod bayesian-optimization -q\n",
    "!pip install pywavelets spektral networkx -q\n",
    "\n",
    "print(\"‚úÖ ALL packages for complete ensemble installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Import ALL libraries for complete ensemble\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Core\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    HistGradientBoostingClassifier, StackingClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Gradient Boosting Specialists\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Anomaly Detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pyod.models.autoencoder import AutoEncoder\n",
    "from pyod.models.vae import VAE\n",
    "\n",
    "# Time Series Analysis\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pykalman import KalmanFilter\n",
    "import pywt\n",
    "\n",
    "# Advanced ML\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import optuna\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Configuration\n",
    "INPUT_BUCKET = \"synthetic-data-4\"\n",
    "OUTPUT_BUCKET = \"processedd-synthetic-data\"\n",
    "REGION = \"us-east-1\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"üî• COMPLETE FIRE DETECTION ENSEMBLE - ALL ALGORITHMS LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üéØ Target: 98%+ accuracy with 17+ algorithms\")\n",
    "print(f\"üìä Deep Learning: 5 models\")\n",
    "print(f\"üìä Gradient Boosting: 4 models\")\n",
    "print(f\"üìä Time Series: 4 models\")\n",
    "print(f\"üìä Anomaly Detection: 4 models\")\n",
    "print(f\"üìä Meta-Learning: 3 systems\")\n",
    "print(f\"üöÄ Total: 20+ algorithms in ensemble\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Input: s3://{INPUT_BUCKET}/datasets/\")\n",
    "print(f\"Output: s3://{OUTPUT_BUCKET}/fire-models/complete-all/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è TIER 1: Core Deep Learning Ensemble (5 Models)\n",
    "### The foundation models for maximum accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 1. Spatio-Temporal Transformer (Your existing top performer)\n",
    "class SpatioTemporalTransformer(nn.Module):\n",
    "    def __init__(self, input_size=6, d_model=128, nhead=8, num_layers=6, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1000, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=512,\n",
    "            dropout=0.1, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Lead time predictor\n",
    "        self.lead_time_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project input and add positional encoding\n",
    "        x = self.input_projection(x)\n",
    "        x = x + self.positional_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # Transform\n",
    "        transformed = self.transformer(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled = transformed.mean(dim=1)\n",
    "        \n",
    "        # Predictions\n",
    "        fire_class = self.classifier(pooled)\n",
    "        lead_time = self.lead_time_predictor(pooled)\n",
    "        \n",
    "        return fire_class, lead_time\n",
    "\n",
    "print(\"‚úÖ 1/5 Spatio-Temporal Transformer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 2. LSTM-CNN Hybrid (Sequential + Local Pattern Detection)\n",
    "class LSTMCNNHybrid(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=128, num_layers=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN for local pattern detection\n",
    "        self.conv1d = nn.Sequential(\n",
    "            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # LSTM for sequential dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True,\n",
    "            dropout=0.2, bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size*2, num_heads=8, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size*2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, input_size = x.shape\n",
    "        \n",
    "        # CNN processing (batch, features, sequence)\n",
    "        x_cnn = x.transpose(1, 2)  # (batch, input_size, seq_len)\n",
    "        conv_out = self.conv1d(x_cnn)\n",
    "        conv_out = conv_out.transpose(1, 2)  # Back to (batch, seq_len, features)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, (h_n, c_n) = self.lstm(conv_out)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Global max and mean pooling\n",
    "        max_pool = torch.max(attn_out, dim=1)[0]\n",
    "        mean_pool = torch.mean(attn_out, dim=1)\n",
    "        \n",
    "        # Combine pooling strategies\n",
    "        combined = max_pool + mean_pool\n",
    "        \n",
    "        return self.classifier(combined)\n",
    "\n",
    "print(\"‚úÖ 2/5 LSTM-CNN Hybrid defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 3. Graph Neural Network (Sensor Relationship Modeling)\n",
    "class GraphFireDetector(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=128, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Node feature transformation\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.graph_conv1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.graph_conv2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.graph_conv3 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def create_adjacency_matrix(self, batch_size, seq_len):\n",
    "        \"\"\"Create adjacency matrix for sensor relationships\"\"\"\n",
    "        # Simple: each timestep connects to adjacent timesteps\n",
    "        adj = torch.zeros(seq_len, seq_len, device=DEVICE)\n",
    "        for i in range(seq_len):\n",
    "            if i > 0:\n",
    "                adj[i, i-1] = 1\n",
    "            if i < seq_len - 1:\n",
    "                adj[i, i+1] = 1\n",
    "            adj[i, i] = 1  # Self-connection\n",
    "        \n",
    "        # Normalize adjacency matrix\n",
    "        degree = adj.sum(dim=1, keepdim=True)\n",
    "        adj = adj / (degree + 1e-8)\n",
    "        \n",
    "        return adj.unsqueeze(0).repeat(batch_size, 1, 1)  # (batch, seq_len, seq_len)\n",
    "    \n",
    "    def graph_convolution(self, x, adj, conv_layer):\n",
    "        \"\"\"Perform graph convolution\"\"\"\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "        # adj: (batch, seq_len, seq_len)\n",
    "        \n",
    "        # Transform features\n",
    "        x_transformed = conv_layer(x)  # (batch, seq_len, hidden_size)\n",
    "        \n",
    "        # Aggregate neighbors\n",
    "        x_aggregated = torch.bmm(adj, x_transformed)  # (batch, seq_len, hidden_size)\n",
    "        \n",
    "        return F.relu(x_aggregated)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Encode node features\n",
    "        x = self.node_encoder(x)  # (batch, seq_len, hidden_size)\n",
    "        \n",
    "        # Create adjacency matrix\n",
    "        adj = self.create_adjacency_matrix(batch_size, seq_len)\n",
    "        \n",
    "        # Graph convolutions\n",
    "        x = self.graph_convolution(x, adj, self.graph_conv1)\n",
    "        x = self.graph_convolution(x, adj, self.graph_conv2)\n",
    "        x = self.graph_convolution(x, adj, self.graph_conv3)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = x.transpose(1, 2)  # (batch, hidden_size, seq_len)\n",
    "        x = self.global_pool(x)  # (batch, hidden_size)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "print(\"‚úÖ 3/5 Graph Neural Network defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 4. Temporal Convolutional Network (Parallel Processing + Long Dependencies)\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, input_size=6, num_channels=[64, 128, 256], kernel_size=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers.append(\n",
    "                self._make_tcn_block(\n",
    "                    in_channels, out_channels, kernel_size, dilation_size\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_channels[-1], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def _make_tcn_block(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                     dilation=dilation, padding=padding),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                     dilation=dilation, padding=padding),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size) -> (batch, input_size, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # TCN processing\n",
    "        x = self.tcn(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "print(\"‚úÖ 4/5 Temporal Convolutional Network defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 5. LSTM Variational Autoencoder (Unsupervised Anomaly Detection + Uncertainty)\n",
    "class LSTMVariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=128, latent_dim=32, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size, hidden_size, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Variational layers\n",
    "        self.fc_mu = nn.Linear(hidden_size * 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_size * 2, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_fc = nn.Linear(latent_dim, hidden_size)\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            hidden_size, hidden_size, batch_first=True\n",
    "        )\n",
    "        self.decoder_output = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x, return_reconstruction=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Encode\n",
    "        lstm_out, (h_n, c_n) = self.encoder_lstm(x)\n",
    "        # Use the last hidden state\n",
    "        encoded = lstm_out[:, -1, :]  # (batch, hidden_size*2)\n",
    "        \n",
    "        # Variational parameters\n",
    "        mu = self.fc_mu(encoded)\n",
    "        logvar = self.fc_logvar(encoded)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Classification from latent space\n",
    "        fire_class = self.classifier(z)\n",
    "        \n",
    "        if return_reconstruction:\n",
    "            # Decode\n",
    "            decoded_input = self.decoder_fc(z).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "            decoded_lstm, _ = self.decoder_lstm(decoded_input)\n",
    "            reconstruction = self.decoder_output(decoded_lstm)\n",
    "            \n",
    "            return fire_class, reconstruction, mu, logvar\n",
    "        \n",
    "        return fire_class, mu, logvar\n",
    "\n",
    "print(\"‚úÖ 5/5 LSTM Variational Autoencoder defined\")\n",
    "print(\"üéâ TIER 1 COMPLETE: All 5 Core Deep Learning Models Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è TIER 2: Specialist Models (9 Models)\n",
    "### Time Series Specialists + Gradient Boosting + Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà TIME SERIES SPECIALISTS (4 Models)\n",
    "class TimeSeriesSpecialists:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        \n",
    "    def create_prophet_model(self, data):\n",
    "        \"\"\"1. Prophet for long-term trends and seasonality\"\"\"\n",
    "        model = Prophet(\n",
    "            daily_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            yearly_seasonality=False,\n",
    "            changepoint_prior_scale=0.05,\n",
    "            uncertainty_samples=1000\n",
    "        )\n",
    "        self.models['prophet'] = model\n",
    "        return model\n",
    "    \n",
    "    def create_arima_model(self, order=(2, 1, 2)):\n",
    "        \"\"\"2. ARIMA-GARCH for statistical patterns\"\"\"\n",
    "        self.arima_order = order\n",
    "        return order\n",
    "    \n",
    "    def create_kalman_filter(self, n_dim_state=4):\n",
    "        \"\"\"3. Kalman Filter for sensor fusion and noise filtering\"\"\"\n",
    "        transition_matrices = np.eye(n_dim_state)\n",
    "        observation_matrices = np.eye(n_dim_state)\n",
    "        \n",
    "        model = KalmanFilter(\n",
    "            transition_matrices=transition_matrices,\n",
    "            observation_matrices=observation_matrices\n",
    "        )\n",
    "        self.models['kalman'] = model\n",
    "        return model\n",
    "    \n",
    "    def wavelet_analysis(self, signal, wavelet='db4', levels=5):\n",
    "        \"\"\"4. Wavelet Transform for frequency analysis\"\"\"\n",
    "        coeffs = pywt.wavedec(signal, wavelet, level=levels)\n",
    "        # Extract features from wavelet coefficients\n",
    "        features = []\n",
    "        for coeff in coeffs:\n",
    "            features.extend([\n",
    "                np.mean(coeff),\n",
    "                np.std(coeff),\n",
    "                np.max(coeff),\n",
    "                np.min(coeff),\n",
    "                np.percentile(coeff, 25),\n",
    "                np.percentile(coeff, 75)\n",
    "            ])\n",
    "        return np.array(features)\n",
    "\n",
    "print(\"‚úÖ Time Series Specialists defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ GRADIENT BOOSTING ENSEMBLE (4 Models)\n",
    "class GradientBoostingEnsemble:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "    \n",
    "    def create_xgboost(self):\n",
    "        \"\"\"1. XGBoost - Feature importance, handles missing data\"\"\"\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=1000,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.01,