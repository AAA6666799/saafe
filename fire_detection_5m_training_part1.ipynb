{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Fire Detection AI - 5M Dataset Training\n",
    "\n",
    "This notebook implements an optimized training pipeline for Fire Detection AI models using a 5M sample from the 50M dataset. The goal is to significantly reduce training time from 43 hours while maintaining reasonable accuracy.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Optimized Data Sampling**: Stratified sampling with temporal pattern preservation\n",
    "- **Optimized Model Architecture**: Reduced transformer size with efficient ML ensemble\n",
    "- **Enhanced Visualizations**: Real-time training dashboard and comprehensive visualizations\n",
    "- **Comprehensive Error Handling**: Custom exceptions, recovery mechanisms, and checkpointing\n",
    "- **Detailed Logging**: Multi-destination logging with structured format\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- **Training Time**: 2-4 hours (vs 43 hours for full dataset)\n",
    "- **Model Accuracy**: 94-96% (vs 97-98% for full dataset)\n",
    "- **Cost Savings**: ~90% reduction in training costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import functools\n",
    "import threading\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "\n",
    "# Machine learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy import stats\n",
    "\n",
    "# AWS libraries\n",
    "try:\n",
    "    import boto3\n",
    "    import sagemaker\n",
    "    from botocore.exceptions import ClientError\n",
    "    AWS_AVAILABLE = True\n",
    "    print(\"‚úÖ AWS libraries available\")\n",
    "except ImportError:\n",
    "    AWS_AVAILABLE = False\n",
    "    print(\"‚ùå AWS libraries not available - install with: pip install boto3 sagemaker\")\n",
    "\n",
    "# Optional ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ùå XGBoost not available - install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"‚úÖ LightGBM available\")\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"‚ùå LightGBM not available - install with: pip install lightgbm\")\n",
    "\n",
    "# Configure notebook settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_BUCKET = \"processedd-synthetic-data\"\n",
    "DATASET_PREFIX = \"cleaned-data/\"\n",
    "SAMPLE_SIZE_PER_AREA = 1000000  # 1M samples per area = 5M total\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Sampling configuration\n",
    "PRESERVE_TEMPORAL_PATTERNS = True\n",
    "ENSURE_CLASS_BALANCE = True\n",
    "CLASS_DISTRIBUTION_TARGET = {\n",
    "    0: 0.70,  # Normal: 70%\n",
    "    1: 0.20,  # Warning: 20%\n",
    "    2: 0.10   # Fire: 10%\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 50  # Reduced from 100\n",
    "BATCH_SIZE = 256\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "# Model configuration\n",
    "TRANSFORMER_CONFIG = {\n",
    "    'd_model': 128,  # Reduced from 256\n",
    "    'num_heads': 4,  # Reduced from 8\n",
    "    'num_layers': 3,  # Reduced from 6\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Visualization configuration\n",
    "VISUALIZATION_CONFIG = {\n",
    "    'update_interval': 1,  # Update visualizations every N epochs\n",
    "    'save_figures': True,\n",
    "    'figure_dir': 'figures',\n",
    "    'create_animations': True\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs(VISUALIZATION_CONFIG['figure_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Configure comprehensive logging for the training process\"\"\"\n",
    "    \n",
    "    # Generate timestamp for log files\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = f'logs/fire_detection_5m_training_{timestamp}.log'\n",
    "    \n",
    "    # Configure root logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Get root logger\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    logger.handlers = []\n",
    "    \n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    \n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    \n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # Create S3 handler if AWS is available\n",
    "    if AWS_AVAILABLE:\n",
    "        try:\n",
    "            s3_handler = S3LogHandler(\n",
    "                bucket=DATASET_BUCKET,\n",
    "                key_prefix=f'logs/fire_detection_5m_training_{timestamp}/'\n",
    "            )\n",
    "            s3_handler.setLevel(logging.INFO)\n",
    "            s3_handler.setFormatter(file_formatter)\n",
    "            logger.addHandler(s3_handler)\n",
    "            logger.info(\"‚úÖ S3 logging configured\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Failed to configure S3 logging: {e}\")\n",
    "    \n",
    "    logger.info(\"üîß Logging configured successfully\")\n",
    "    logger.info(f\"üìù Log file: {log_file}\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class S3LogHandler(logging.Handler):\n",
    "    \"\"\"Custom logging handler that writes logs to S3\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket, key_prefix):\n",
    "        super().__init__()\n",
    "        self.bucket = bucket\n",
    "        self.key_prefix = key_prefix\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 100  # Number of logs to buffer before writing to S3\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Start background thread for uploading logs\n",
    "        self.stop_event = threading.Event()\n",
    "        self.upload_thread = threading.Thread(target=self._upload_logs_periodically)\n",
    "        self.upload_thread.daemon = True\n",
    "        self.upload_thread.start()\n",
    "    \n",
    "    def emit(self, record):\n",
    "        \"\"\"Process a log record\"\"\"\n",
    "        log_entry = self.format(record)\n",
    "        self.buffer.append(log_entry)\n",
    "        \n",
    "        # Upload logs if buffer is full\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self._upload_logs()\n",
    "    \n",
    "    def _upload_logs(self):\n",
    "        \"\"\"Upload buffered logs to S3\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Create log content\n",
    "            log_content = '\\n'.join(self.buffer)\n",
    "            \n",
    "            # Generate key with timestamp\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            key = f\"{self.key_prefix}log_{timestamp}.txt\"\n",
    "            \n",
    "            # Upload to S3\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=self.bucket,\n",
    "                Key=key,\n",
    "                Body=log_content\n",
    "            )\n",
    "            \n",
    "            # Clear buffer\n",
    "            self.buffer = []\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Don't raise exception, just print to stderr\n",
    "            print(f\"Error uploading logs to S3: {e}\", file=sys.stderr)\n",
    "    \n",
    "    def _upload_logs_periodically(self):\n",
    "        \"\"\"Upload logs periodically in background thread\"\"\"\n",
    "        while not self.stop_event.is_set():\n",
    "            time.sleep(60)  # Upload every minute\n",
    "            self._upload_logs()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up handler resources\"\"\"\n",
    "        self.stop_event.set()\n",
    "        self._upload_logs()  # Final upload\n",
    "        super().close()\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Error Handling Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingError(Exception):\n",
    "    \"\"\"Base class for training-related exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class DataLoadingError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during data loading\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelInitializationError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model initialization\"\"\"\n",
    "    pass\n",
    "\n",
    "class TrainingProcessError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during training process\"\"\"\n",
    "    pass\n",
    "\n",
    "class EvaluationError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model evaluation\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelSavingError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model saving\"\"\"\n",
    "    pass\n",
    "\n",
    "def error_handler(func):\n",
    "    \"\"\"Decorator for handling errors in functions\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TrainingError as e:\n",
    "            # Log specific training errors\n",
    "            logger.error(f\"‚ùå {e.__class__.__name__}: {str(e)}\")\n",
    "            # Re-raise to be caught by global handler\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # Log unexpected errors\n",
    "            logger.error(f\"‚ùå Unexpected error in {func.__name__}: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            # Wrap in TrainingProcessError\n",
    "            raise TrainingProcessError(f\"Error in {func.__name__}: {str(e)}\") from e\n",
    "    return wrapper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}