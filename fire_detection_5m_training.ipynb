{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Fire Detection AI - 5M Dataset Training\n",
    "\n",
    "This notebook implements an optimized training pipeline for Fire Detection AI models using a 5M sample from the 50M dataset. The goal is to significantly reduce training time from 43 hours while maintaining reasonable accuracy.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Optimized Data Sampling**: Stratified sampling with temporal pattern preservation\n",
    "- **Optimized Model Architecture**: Reduced transformer size with efficient ML ensemble\n",
    "- **Enhanced Visualizations**: Real-time training dashboard and comprehensive visualizations\n",
    "- **Comprehensive Error Handling**: Custom exceptions, recovery mechanisms, and checkpointing\n",
    "- **Detailed Logging**: Multi-destination logging with structured format\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- **Training Time**: 2-4 hours (vs 43 hours for full dataset)\n",
    "- **Model Accuracy**: 94-96% (vs 97-98% for full dataset)\n",
    "- **Cost Savings**: ~90% reduction in training costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import functools\n",
    "import threading\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "\n",
    "# Machine learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy import stats\n",
    "\n",
    "# AWS libraries\n",
    "try:\n",
    "    import boto3\n",
    "    import sagemaker\n",
    "    from botocore.exceptions import ClientError\n",
    "    AWS_AVAILABLE = True\n",
    "    print(\"‚úÖ AWS libraries available\")\n",
    "except ImportError:\n",
    "    AWS_AVAILABLE = False\n",
    "    print(\"‚ùå AWS libraries not available - install with: pip install boto3 sagemaker\")\n",
    "\n",
    "# Optional ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ùå XGBoost not available - install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"‚úÖ LightGBM available\")\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"‚ùå LightGBM not available - install with: pip install lightgbm\")\n",
    "\n",
    "# Configure notebook settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_BUCKET = \"processedd-synthetic-data\"\n",
    "DATASET_PREFIX = \"cleaned-data/\"\n",
    "SAMPLE_SIZE_PER_AREA = 1000000  # 1M samples per area = 5M total\n",
    "SAMPLE_SIZE = 5000000  # 5M total samples\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Sampling configuration\n",
    "PRESERVE_TEMPORAL_PATTERNS = True\n",
    "ENSURE_CLASS_BALANCE = True\n",
    "CLASS_DISTRIBUTION_TARGET = {\n",
    "    0: 0.70,  # Normal: 70%\n",
    "    1: 0.20,  # Warning: 20%\n",
    "    2: 0.10   # Fire: 10%\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 50  # Reduced from 100\n",
    "BATCH_SIZE = 256\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "# Model configuration\n",
    "D_MODEL = 128  # Reduced from 256\n",
    "NUM_HEADS = 4  # Reduced from 8\n",
    "NUM_LAYERS = 3  # Reduced from 6\n",
    "DROPOUT = 0.1\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "TRANSFORMER_CONFIG = {\n",
    "    'd_model': D_MODEL,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'dropout': DROPOUT\n",
    "}\n",
    "\n",
    "# Visualization configuration\n",
    "VISUALIZATION_CONFIG = {\n",
    "    'update_interval': 1,  # Update visualizations every N epochs\n",
    "    'save_figures': True,\n",
    "    'figure_dir': 'figures',\n",
    "    'create_animations': True\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs(VISUALIZATION_CONFIG['figure_dir'], exist_ok=True)\n",
    "\n",
    "# Start notebook timer\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Error Handling Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingError(Exception):\n",
    "    \"\"\"Base class for training-related exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class DataLoadingError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during data loading\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelInitializationError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model initialization\"\"\"\n",
    "    pass\n",
    "\n",
    "class TrainingProcessError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during training process\"\"\"\n",
    "    pass\n",
    "\n",
    "class EvaluationError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model evaluation\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelSavingError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during model saving\"\"\"\n",
    "    pass\n",
    "\n",
    "class PredictionError(TrainingError):\n",
    "    \"\"\"Exception raised for errors during prediction\"\"\"\n",
    "    pass\n",
    "\n",
    "def error_handler(func):\n",
    "    \"\"\"Decorator for handling errors in functions\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TrainingError as e:\n",
    "            # Log specific training errors\n",
    "            logger.error(f\"‚ùå {e.__class__.__name__}: {str(e)}\")\n",
    "            # Re-raise to be caught by global handler\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # Log unexpected errors\n",
    "            logger.error(f\"‚ùå Unexpected error in {func.__name__}: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            # Wrap in TrainingProcessError\n",
    "            raise TrainingProcessError(f\"Error in {func.__name__}: {str(e)}\") from e\n",
    "    return wrapper\n",
    "\n",
    "# Initialize logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Sampling\n",
    "\n",
    "### 2.1 Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, we'll create synthetic data\n",
    "# In a real scenario, you would load data from S3 or local storage\n",
    "\n",
    "def create_synthetic_data(n_samples=5000000, n_features=6, n_timesteps=60, n_areas=5):\n",
    "    \"\"\"Create synthetic data for demonstration\"\"\"\n",
    "    logger.info(f\"Creating synthetic data with {n_samples} samples\")\n",
    "    \n",
    "    # Create features\n",
    "    X = np.random.randn(n_samples, n_timesteps, n_features).astype(np.float32)\n",
    "    \n",
    "    # Create labels (0: normal, 1: warning, 2: fire)\n",
    "    y = np.random.choice([0, 1, 2], size=n_samples, p=[0.7, 0.2, 0.1])\n",
    "    \n",
    "    # Create area IDs\n",
    "    areas = np.random.choice(range(n_areas), size=n_samples)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Created synthetic data: X={X.shape}, y={y.shape}, areas={areas.shape}\")\n",
    "    \n",
    "    return X, y, areas\n",
    "\n",
    "# Create synthetic data\n",
    "X, y, areas = create_synthetic_data(n_samples=50000)  # Using smaller sample for demonstration\n",
    "\n",
    "# Split into train/validation sets\n",
    "X_train, X_val, y_train, y_val, areas_train, areas_val = train_test_split(\n",
    "    X, y, areas, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "logger.info(f\"Train set: X={X_train.shape}, y={y_train.shape}, areas={areas_train.shape}\")\n",
    "logger.info(f\"Validation set: X={X_val.shape}, y={y_val.shape}, areas={areas_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimized Transformer Model\n",
    "\n",
    "### 3.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedFireTransformer(nn.Module):\n",
    "    \"\"\"Optimized transformer for multi-area fire detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=6, seq_len=60, d_model=128, num_heads=4, \n",
    "                 num_layers=3, num_classes=3, num_areas=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.area_embedding = nn.Embedding(num_areas, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads, dim_feedforward=d_model*4,\n",
    "            dropout=dropout, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.fire_classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.risk_predictor = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, area_types):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        x = self.input_proj(x)\n",
    "        area_emb = self.area_embedding(area_types).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        x = x + area_emb + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Global pooling\n",
    "        \n",
    "        return {\n",
    "            'fire_logits': self.fire_classifier(x),\n",
    "            'risk_score': self.risk_predictor(x) * 100.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"üöÄ Device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = OptimizedFireTransformer(\n",
    "    input_dim=X_train.shape[2],\n",
    "    seq_len=X_train.shape[1],\n",
    "    d_model=TRANSFORMER_CONFIG['d_model'],\n",
    "    num_heads=TRANSFORMER_CONFIG['num_heads'],\n",
    "    num_layers=TRANSFORMER_CONFIG['num_layers'],\n",
    "    num_classes=len(np.unique(y_train)),\n",
    "    num_areas=len(np.unique(areas_train)),\n",
    "    dropout=TRANSFORMER_CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Model parameters: {total_params:,} (trainable: {trainable_params:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def train_transformer_with_early_stopping(model, X_train, y_train, areas_train, X_val, y_val, areas_val, device):\n",
    "    \"\"\"Train the optimized transformer model with error handling\"\"\"\n",
    "    logger.info(\"ü§ñ TRAINING OPTIMIZED TRANSFORMER\")\n",
    "    logger.info(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Convert to tensors with error checking\n",
    "        try:\n",
    "            X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "            X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "            y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "            y_val_tensor = torch.LongTensor(y_val).to(device)\n",
    "            areas_train_tensor = torch.LongTensor(areas_train).to(device)\n",
    "            areas_val_tensor = torch.LongTensor(areas_val).to(device)\n",
    "        except Exception as e:\n",
    "            raise ModelInitializationError(f\"Error converting data to tensors: {e}\")\n",
    "        \n",
    "        # Training setup\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        training_history = []\n",
    "        \n",
    "        # Training loop with error handling and checkpointing\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Training\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(X_train_tensor, areas_train_tensor)\n",
    "                loss = criterion(outputs['fire_logits'], y_train_tensor)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(X_val_tensor, areas_val_tensor)\n",
    "                    val_preds = torch.argmax(val_outputs['fire_logits'], dim=1)\n",
    "                    val_acc = (val_preds == y_val_tensor).float().mean().item()\n",
    "                    \n",
    "                    # Track memory usage\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_usage = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "                    else:\n",
    "                        memory_usage = 0\n",
    "                    \n",
    "                    # Log progress\n",
    "                    epoch_time = time.time() - epoch_start_time\n",
    "                    logger.info(f\"Epoch {epoch:3d}: Loss={loss:.4f}, Val_Acc={val_acc:.4f}, \"\n",
    "                               f\"Time={epoch_time:.1f}s, Memory={memory_usage:.2f}GB\")\n",
    "                    \n",
    "                    # Update training history\n",
    "                    training_history.append({\n",
    "                        'epoch': epoch,\n",
    "                        'train_loss': loss.item(),\n",
    "                        'val_accuracy': val_acc,\n",
    "                        'learning_rate': scheduler.get_last_lr()[0],\n",
    "                        'time': epoch_time,\n",
    "                        'memory_usage': memory_usage\n",
    "                    })\n",
    "                    \n",
    "                    # Check for improvement\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        best_model_state = model.state_dict().copy()\n",
    "                        patience_counter = 0\n",
    "                        \n",
    "                        # Save checkpoint\n",
    "                        checkpoint_path = f'checkpoints/transformer_epoch_{epoch}_acc_{val_acc:.4f}.pt'\n",
    "                        torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'scheduler_state_dict': scheduler.state_dict(),\n",
    "                            'val_accuracy': val_acc,\n",
    "                            'training_history': training_history\n",
    "                        }, checkpoint_path)\n",
    "                        logger.info(f\"   üíæ Saved checkpoint: {checkpoint_path}\")\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                        logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error during epoch {epoch}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "                \n",
    "                # Try to save checkpoint before exiting\n",
    "                if best_model_state:\n",
    "                    recovery_path = f'checkpoints/transformer_recovery_acc_{best_val_acc:.4f}.pt'\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': best_model_state,\n",
    "                        'val_accuracy': best_val_acc,\n",
    "                        'training_history': training_history\n",
    "                    }, recovery_path)\n",
    "                    logger.info(f\"   üíæ Saved recovery checkpoint: {recovery_path}\")\n",
    "                \n",
    "                raise TrainingProcessError(f\"Error during epoch {epoch}: {e}\")\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state:\n",
    "            model.load_state_dict(best_model_state)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        logger.info(f\"‚úÖ Transformer training completed!\")\n",
    "        logger.info(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        logger.info(f\"   Training time: {training_time:.1f}s ({training_time/60:.1f} min)\")\n",
    "        \n",
    "        return model, best_val_acc, training_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Catch any uncaught exceptions\n",
    "        logger.error(f\"‚ùå Unexpected error in transformer training: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise TrainingProcessError(f\"Unexpected error in transformer training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train transformer model\n",
    "model, best_val_acc, training_history = train_transformer_with_early_stopping(\n",
    "    model, X_train, y_train, areas_train, X_val, y_val, areas_val, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ML Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(X):\n",
    "    \"\"\"Advanced feature engineering for ML models\"\"\"\n",
    "    \n",
    "    logger.info(\"üîß Engineering features...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    features = []\n",
    "    for i in range(X.shape[0]):\n",
    "        sample_features = []\n",
    "        for j in range(X.shape[2]):\n",
    "            series = X[i, :, j]\n",
    "            # Statistical features\n",
    "            sample_features.extend([\n",
    "                np.mean(series), np.std(series), np.min(series), np.max(series),\n",
    "                np.median(series), np.percentile(series, 25), np.percentile(series, 75)\n",
    "            ])\n",
    "            # Trend features\n",
    "            if len(series) > 1:\n",
    "                slope = np.polyfit(range(len(series)), series, 1)[0]\n",
    "                sample_features.append(slope)\n",
    "                diff = np.diff(series)\n",
    "                sample_features.extend([np.mean(np.abs(diff)), np.std(diff)])\n",
    "            else:\n",
    "                sample_features.extend([0, 0, 0])\n",
    "        features.append(sample_features)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    \n",
    "    logger.info(f\"   ‚úÖ Engineered {features.shape[1]} features for {features.shape[0]} samples\")\n",
    "    logger.info(f\"   