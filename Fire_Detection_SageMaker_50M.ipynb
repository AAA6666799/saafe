{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Fire Detection AI - 50M Dataset Training on AWS SageMaker\n",
    "\n",
    "**Objective**: Train production-ready fire detection models on 19.9M samples from S3  \n",
    "**Dataset**: `s3://processedd-synthetic-data/cleaned-data/`  \n",
    "**Target**: 97-98% accuracy, <0.5% false positive rate  \n",
    "**Instance**: `ml.p3.2xlarge` (Tesla V100 GPU) or `ml.p3.8xlarge` (4x V100)\n",
    "\n",
    "## Dataset Structure\n",
    "- `basement_data_cleaned.csv` (922.8 MB) - 6 columns\n",
    "- `laundry_data_cleaned.csv` (756.2 MB) - 5 columns  \n",
    "- `asd_data_cleaned.csv` (600.6 MB) - 4 columns\n",
    "- `voc_data_cleaned.csv` (566.4 MB) - 4 columns\n",
    "- `arc_data_cleaned.csv` (489.2 MB) - 4 columns\n",
    "\n",
    "**Total**: ~19.9M samples, 3.26 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for SageMaker\n",
    "!pip install torch torchvision --quiet\n",
    "!pip install xgboost lightgbm catboost --quiet\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn --quiet\n",
    "!pip install boto3 sagemaker --quiet\n",
    "\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Advanced ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸš€ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"ðŸ”¥\" * 60)\n",
    "print(\"FIRE DETECTION AI - 50M DATASET TRAINING\")\n",
    "print(\"ðŸ”¥\" * 60)\n",
    "print(f\"ðŸ“Š Target: 19.9M samples from 5 area datasets\")\n",
    "print(f\"ðŸŽ¯ Goal: 97-98% accuracy\")\n",
    "print(f\"âš¡ XGBoost: {'âœ…' if XGB_AVAILABLE else 'âŒ'}\")\n",
    "print(f\"âš¡ LightGBM: {'âœ…' if LGB_AVAILABLE else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_BUCKET = \"processedd-synthetic-data\"\n",
    "DATASET_PREFIX = \"cleaned-data/\"\n",
    "\n",
    "# SageMaker configuration\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"ðŸ“Š Dataset: s3://{DATASET_BUCKET}/{DATASET_PREFIX}\")\n",
    "print(f\"ðŸ’¾ Output: s3://{bucket}/fire-detection-models/\")\n",
    "print(f\"ðŸ” Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_area_data(area_name, file_name, max_samples=None):\n",
    "    \"\"\"Load data for a specific area from S3\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    file_key = f\"{DATASET_PREFIX}{file_name}\"\n",
    "    \n",
    "    print(f\"ðŸ“¥ Loading {area_name}: s3://{DATASET_BUCKET}/{file_key}\")\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=DATASET_BUCKET, Key=file_key)\n",
    "        df = pd.read_csv(response['Body'])\n",
    "        \n",
    "        if max_samples and len(df) > max_samples:\n",
    "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   âœ… Loaded {len(df):,} samples, {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_area_data(df, area_name):\n",
    "    \"\"\"Preprocess data for specific area\"\"\"\n",
    "    print(f\"ðŸ”§ Preprocessing {area_name}...\")\n",
    "    \n",
    "    # Handle timestamp if present\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Extract features (exclude timestamp and labels)\n",
    "    exclude_cols = ['timestamp', 'is_anomaly', 'label']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Limit features based on area type\n",
    "    if area_name == 'basement':\n",
    "        feature_cols = feature_cols[:4]  # Top 4 features\n",
    "    elif area_name == 'laundry':\n",
    "        feature_cols = feature_cols[:3]  # Top 3 features\n",
    "    else:\n",
    "        feature_cols = feature_cols[:2]  # Top 2 features\n",
    "    \n",
    "    X = df[feature_cols].fillna(0).values\n",
    "    \n",
    "    # Create labels\n",
    "    if 'is_anomaly' in df.columns:\n",
    "        y = df['is_anomaly'].values.astype(int)\n",
    "    elif 'label' in df.columns:\n",
    "        y = df['label'].values.astype(int)\n",
    "    else:\n",
    "        # Generate labels based on statistical thresholds\n",
    "        values = df[feature_cols[0]].values\n",
    "        q95 = np.percentile(values, 95)\n",
    "        q85 = np.percentile(values, 85)\n",
    "        \n",
    "        y = np.zeros(len(values))\n",
    "        y[values > q95] = 2  # Fire\n",
    "        y[(values > q85) & (values <= q95)] = 1  # Warning\n",
    "    \n",
    "    # Pad features to consistent size (6 features)\n",
    "    if X.shape[1] < 6:\n",
    "        padding = np.zeros((X.shape[0], 6 - X.shape[1]))\n",
    "        X = np.hstack([X, padding])\n",
    "    elif X.shape[1] > 6:\n",
    "        X = X[:, :6]\n",
    "    \n",
    "    print(f\"   ðŸ“Š Shape: {X.shape}, Anomaly rate: {y.mean():.4f}\")\n",
    "    return X, y\n",
    "\n",
    "def create_sequences(X, y, seq_len=60, step=10):\n",
    "    \"\"\"Create time series sequences\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(0, len(X) - seq_len, step):\n",
    "        sequences.append(X[i:i+seq_len])\n",
    "        labels.append(y[i+seq_len-1])  # Label at end of sequence\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all area datasets\n",
    "area_files = {\n",
    "    'basement': 'basement_data_cleaned.csv',\n",
    "    'laundry': 'laundry_data_cleaned.csv',\n",
    "    'asd': 'asd_data_cleaned.csv',\n",
    "    'voc': 'voc_data_cleaned.csv',\n",
    "    'arc': 'arc_data_cleaned.csv'\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "USE_FULL_DATASET = False  # Set True for full 50M dataset\n",
    "MAX_SAMPLES_PER_AREA = 1000000 if not USE_FULL_DATASET else None\n",
    "\n",
    "print(f\"Configuration: Full dataset = {USE_FULL_DATASET}\")\n",
    "print(f\"Max samples per area: {MAX_SAMPLES_PER_AREA or 'ALL'}\")\n",
    "\n",
    "# Load and process all areas\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "all_areas = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for area_idx, (area_name, file_name) in enumerate(area_files.items()):\n",
    "    print(f\"\\nðŸ“ PROCESSING {area_name.upper()} ({area_idx+1}/5)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_area_data(area_name, file_name, MAX_SAMPLES_PER_AREA)\n",
    "    if df is None:\n",
    "        continue\n",
    "    \n",
    "    # Preprocess\n",
    "    X, y = preprocess_area_data(df, area_name)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences, labels = create_sequences(X, y, seq_len=60, step=10)\n",
    "    \n",
    "    # Add area labels\n",
    "    areas = np.full(len(sequences), area_idx)\n",
    "    \n",
    "    all_sequences.append(sequences)\n",
    "    all_labels.append(labels)\n",
    "    all_areas.append(areas)\n",
    "    \n",
    "    print(f\"   âœ… Created {len(sequences):,} sequences\")\n",
    "\n",
    "# Combine all data\n",
    "print(f\"\\nðŸ”— COMBINING ALL AREAS\")\n",
    "X_combined = np.vstack(all_sequences)\n",
    "y_combined = np.hstack(all_labels)\n",
    "areas_combined = np.hstack(all_areas)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nðŸŽ¯ DATASET SUMMARY:\")\n",
    "print(f\"   ðŸ“Š Total sequences: {X_combined.shape[0]:,}\")\n",
    "print(f\"   ðŸ“ Sequence shape: {X_combined.shape[1:]}\")\n",
    "print(f\"   ðŸ“ˆ Class distribution: {np.bincount(y_combined.astype(int))}\")\n",
    "print(f\"   ðŸ’¾ Memory: {X_combined.nbytes / (1024**3):.2f} GB\")\n",
    "print(f\"   â±ï¸ Loading time: {total_time:.1f}s\")\n",
    "print(f\"   âœ… Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFireTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=6, seq_len=60, d_model=256, num_heads=8, \n",
    "                 num_layers=6, num_classes=3, num_areas=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.area_embedding = nn.Embedding(num_areas, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads, dim_feedforward=d_model*4,\n",
    "            dropout=dropout, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.fire_classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.risk_predictor = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, area_types):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        x = self.input_proj(x)\n",
    "        area_emb = self.area_embedding(area_types).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        x = x + area_emb + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Global pooling\n",
    "        \n",
    "        return {\n",
    "            'fire_logits': self.fire_classifier(x),\n",
    "            'risk_score': self.risk_predictor(x) * 100.0\n",
    "        }\n",
    "\n",
    "print(\"âœ… Enhanced Fire Transformer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test, areas_train, areas_test = train_test_split(\n",
    "    X_combined, y_combined, areas_combined, \n",
    "    test_size=0.2, random_state=42, stratify=y_combined\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, areas_train, areas_val = train_test_split(\n",
    "    X_train, y_train, areas_train,\n",
    "    test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Data splits:\")\n",
    "print(f\"   Training: {len(X_train):,} sequences\")\n",
    "print(f\"   Validation: {len(X_val):,} sequences\")\n",
    "print(f\"   Test: {len(X_test):,} sequences\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "y_val_tensor = torch.LongTensor(y_val).to(device)\n",
    "areas_train_tensor = torch.LongTensor(areas_train).to(device)\n",
    "areas_val_tensor = torch.LongTensor(areas_val).to(device)\n",
    "\n",
    "print(f\"âœ… Data prepared and moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Enhanced Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ¤– TRAINING ENHANCED TRANSFORMER\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create model\n",
    "model = EnhancedFireTransformer(\n",
    "    input_dim=X_train.shape[2],\n",
    "    seq_len=X_train.shape[1],\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    num_classes=len(np.unique(y_train)),\n",
    "    num_areas=len(np.unique(areas_train))\n",
    ").to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "training_history = []\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Training for {epochs} epochs...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X_train_tensor, areas_train_tensor)\n",
    "    loss = criterion(outputs['fire_logits'], y_train_tensor)\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor, areas_val_tensor)\n",
    "            val_preds = torch.argmax(val_outputs['fire_logits'], dim=1)\n",
    "            val_acc = (val_preds == y_val_tensor).float().mean().item()\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            \n",
    "            training_history.append({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': loss.item(),\n",
    "                'val_accuracy': val_acc\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch {epoch:3d}: Loss={loss:.4f}, Val_Acc={val_acc:.4f}, Best={best_val_acc:.4f}\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Load best model\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "print(f\"\\nâœ… Transformer training completed!\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"   Training time: {training_time:.1f}s ({training_time/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train ML Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for ML models\n",
    "def engineer_features(X):\n",
    "    features = []\n",
    "    for i in range(X.shape[0]):\n",
    "        sample_features = []\n",
    "        for j in range(X.shape[2]):  # Each feature dimension\n",
    "            series = X[i, :, j]\n",
    "            # Statistical features\n",
    "            sample_features.extend([\n",
    "                np.mean(series), np.std(series), np.min(series), np.max(series),\n",
    "                np.median(series), np.percentile(series, 25), np.percentile(series, 75)\n",
    "            ])\n",
    "            # Trend features\n",
    "            if len(series) > 1:\n",
    "                slope = np.polyfit(range(len(series)), series, 1)[0]\n",
    "                sample_features.append(slope)\n",
    "                diff = np.diff(series)\n",
    "                sample_features.extend([np.mean(np.abs(diff)), np.std(diff)])\n",
    "            else:\n",
    "                sample_features.extend([0, 0, 0])\n",
    "        features.append(sample_features)\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"ðŸ”§ Engineering features for ML models...\")\n",
    "X_train_features = engineer_features(X_train)\n",
    "X_val_features = engineer_features(X_val)\n",
    "X_test_features = engineer_features(X_test)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "X_val_scaled = scaler.transform(X_val_features)\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "\n",
    "print(f\"âœ… Features engineered: {X_train_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ML models\n",
    "ml_models = {}\n",
    "ml_results = {}\n",
    "\n",
    "print(\"ðŸ“Š TRAINING ML ENSEMBLE\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Random Forest\n",
    "print(\"ðŸŒ³ Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=15, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_val_acc = rf_model.score(X_val_scaled, y_val)\n",
    "ml_models['random_forest'] = rf_model\n",
    "ml_results['random_forest'] = rf_val_acc\n",
    "print(f\"   âœ… Random Forest Val Acc: {rf_val_acc:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "print(\"ðŸ“ˆ Training Gradient Boosting...\")\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42\n",
    ")\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "gb_val_acc = gb_model.score(X_val_scaled, y_val)\n",
    "ml_models['gradient_boosting'] = gb_model\n",
    "ml_results['gradient_boosting'] = gb_val_acc\n",
    "print(f\"   âœ… Gradient Boosting Val Acc: {gb_val_acc:.4f}\")\n",
    "\n",
    "# XGBoost (if available)\n",
    "if XGB_AVAILABLE:\n",
    "    print(\"âš¡ Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.1, \n",
    "        subsample=0.8, random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    xgb_val_acc = xgb_model.score(X_val_scaled, y_val)\n",