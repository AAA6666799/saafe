{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Model Training\n",
    "This notebook trains multiple anomaly detection models on the cleaned sensor datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn boto3 matplotlib seaborn joblib -q\n",
    "!pip install imbalanced-learn xgboost lightgbm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "INPUT_BUCKET = \"processedd-synthetic-data\"\n",
    "OUTPUT_BUCKET = \"processedd-synthetic-data\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "# Initialize clients\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "sagemaker_session = boto3.Session().region_name\n",
    "\n",
    "print(f\"Training models from: s3://{INPUT_BUCKET}/cleaned-data/\")\n",
    "print(f\"Saving models to: s3://{OUTPUT_BUCKET}/trained-models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets to train on\n",
    "datasets = [\n",
    "    \"arc_data_cleaned.csv\",\n",
    "    \"asd_data_cleaned.csv\", \n",
    "    \"basement_data_cleaned.csv\",\n",
    "    \"laundry_data_cleaned.csv\",\n",
    "    \"voc_data_cleaned.csv\"\n",
    "]\n",
    "\n",
    "print(\"Datasets for training:\")\n",
    "for dataset in datasets:\n",
    "    print(f\"  - {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(dataset_name):\n",
    "    \"\"\"Load and prepare dataset for training\"\"\"\n",
    "    print(f\"\\n=== Loading {dataset_name} ===\")\n",
    "    \n",
    "    # Load data from S3\n",
    "    df = pd.read_csv(f\"s3://{INPUT_BUCKET}/cleaned-data/{dataset_name}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    features = []\n",
    "    \n",
    "    # Time-based features\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "        df['minute'] = df['timestamp'].dt.minute\n",
    "        features.extend(['hour', 'day_of_week', 'minute'])\n",
    "    \n",
    "    # Sensor value features\n",
    "    if 'value' in df.columns:\n",
    "        # Rolling statistics\n",
    "        df['value_rolling_mean_5'] = df['value'].rolling(window=5, min_periods=1).mean()\n",
    "        df['value_rolling_std_5'] = df['value'].rolling(window=5, min_periods=1).std().fillna(0)\n",
    "        df['value_rolling_mean_10'] = df['value'].rolling(window=10, min_periods=1).mean()\n",
    "        df['value_rolling_std_10'] = df['value'].rolling(window=10, min_periods=1).std().fillna(0)\n",
    "        \n",
    "        # Lag features\n",
    "        df['value_lag_1'] = df['value'].shift(1).fillna(df['value'].mean())\n",
    "        df['value_lag_2'] = df['value'].shift(2).fillna(df['value'].mean())\n",
    "        \n",
    "        # Difference features\n",
    "        df['value_diff_1'] = df['value'].diff().fillna(0)\n",
    "        df['value_diff_2'] = df['value'].diff(2).fillna(0)\n",
    "        \n",
    "        features.extend([\n",
    "            'value', 'value_rolling_mean_5', 'value_rolling_std_5',\n",
    "            'value_rolling_mean_10', 'value_rolling_std_10',\n",
    "            'value_lag_1', 'value_lag_2', 'value_diff_1', 'value_diff_2'\n",
    "        ])\n",
    "    \n",
    "    # Sensor ID encoding\n",
    "    if 'sensor_id' in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df['sensor_id_encoded'] = le.fit_transform(df['sensor_id'])\n",
    "        features.append('sensor_id_encoded')\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df[features].fillna(0)\n",
    "    y = df['is_anomaly'] if 'is_anomaly' in df.columns else None\n",
    "    \n",
    "    print(f\"Features: {features}\")\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    if y is not None:\n",
    "        print(f\"Anomaly distribution:\")\n",
    "        print(y.value_counts())\n",
    "        print(f\"Anomaly rate: {y.mean():.4f}\")\n",
    "    \n",
    "    return X, y, features\n",
    "\n",
    "# Test with one dataset\n",
    "X_test, y_test, features_test = load_and_prepare_data(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly_models(X, y, dataset_name):\n",
    "    \"\"\"Train multiple anomaly detection models\"\"\"\n",
    "    print(f\"\\n=== Training Models for {dataset_name} ===\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Handle class imbalance with SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"Original training set: {X_train.shape}\")\n",
    "    print(f\"Balanced training set: {X_train_balanced.shape}\")\n",
    "    \n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Isolation Forest (Unsupervised)\n",
    "    print(\"\\nTraining Isolation Forest...\")\n",
    "    iso_forest = IsolationForest(contamination=y_train.mean(), random_state=42)\n",
    "    iso_forest.fit(X_train_scaled)\n",
    "    iso_pred = iso_forest.predict(X_test_scaled)\n",
    "    iso_pred = np.where(iso_pred == -1, 1, 0)  # Convert to 0/1\n",
    "    \n",
    "    models['isolation_forest'] = {'model': iso_forest, 'scaler': scaler}\n",
    "    results['isolation_forest'] = {\n",
    "        'predictions': iso_pred,\n",
    "        'accuracy': (iso_pred == y_test).mean(),\n",
    "        'classification_report': classification_report(y_test, iso_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 2. One-Class SVM (Unsupervised)\n",
    "    print(\"Training One-Class SVM...\")\n",
    "    oc_svm = OneClassSVM(nu=y_train.mean())\n",
    "    oc_svm.fit(X_train_scaled[y_train == 0])  # Train only on normal data\n",
    "    svm_pred = oc_svm.predict(X_test_scaled)\n",
    "    svm_pred = np.where(svm_pred == -1, 1, 0)  # Convert to 0/1\n",
    "    \n",
    "    models['one_class_svm'] = {'model': oc_svm, 'scaler': scaler}\n",
    "    results['one_class_svm'] = {\n",
    "        'predictions': svm_pred,\n",
    "        'accuracy': (svm_pred == y_test).mean(),\n",
    "        'classification_report': classification_report(y_test, svm_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 3. Random Forest (Supervised)\n",
    "    print(\"Training Random Forest...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf.fit(X_train_balanced, y_train_balanced)\n",
    "    rf_pred = rf.predict(X_test_scaled)\n",
    "    rf_proba = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    models['random_forest'] = {'model': rf, 'scaler': scaler}\n",
    "    results['random_forest'] = {\n",
    "        'predictions': rf_pred,\n",
    "        'probabilities': rf_proba,\n",
    "        'accuracy': (rf_pred == y_test).mean(),\n",
    "        'auc_score': roc_auc_score(y_test, rf_proba),\n",
    "        'classification_report': classification_report(y_test, rf_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 4. XGBoost (Supervised)\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        scale_pos_weight=len(y_train_balanced[y_train_balanced==0])/len(y_train_balanced[y_train_balanced==1])\n",
    "    )\n",
    "    xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "    xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "    xgb_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    models['xgboost'] = {'model': xgb_model, 'scaler': scaler}\n",
    "    results['xgboost'] = {\n",
    "        'predictions': xgb_pred,\n",
    "        'probabilities': xgb_proba,\n",
    "        'accuracy': (xgb_pred == y_test).mean(),\n",
    "        'auc_score': roc_auc_score(y_test, xgb_proba),\n",
    "        'classification_report': classification_report(y_test, xgb_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 5. LightGBM (Supervised)\n",
    "    print(\"Training LightGBM...\")\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        class_weight='balanced',\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "    lgb_pred = lgb_model.predict(X_test_scaled)\n",
    "    lgb_proba = lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    models['lightgbm'] = {'model': lgb_model, 'scaler': scaler}\n",
    "    results['lightgbm'] = {\n",
    "        'predictions': lgb_pred,\n",
    "        'probabilities': lgb_proba,\n",
    "        'accuracy': (lgb_pred == y_test).mean(),\n",
    "        'auc_score': roc_auc_score(y_test, lgb_proba),\n",
    "        'classification_report': classification_report(y_test, lgb_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    return models, results, X_test, y_test\n",
    "\n",
    "# Test with one dataset\n",
    "models_test, results_test, X_test_data, y_test_data = train_anomaly_models(X_test, y_test, datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_visualize_models(models, results, X_test, y_test, dataset_name):\n",
    "    \"\"\"Evaluate and visualize model performance\"\"\"\n",
    "    print(f\"\\n=== Model Evaluation for {dataset_name} ===\")\n",
    "    \n",
    "    # Performance summary\n",
    "    performance_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
    "        'AUC Score': [results[model].get('auc_score', 'N/A') for model in results.keys()]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(performance_df.to_string(index=False))\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Model Performance Analysis - {dataset_name}', fontsize=16)\n",
    "    \n",
    "    # 1. Accuracy comparison\n",
    "    axes[0, 0].bar(performance_df['Model'], performance_df['Accuracy'])\n",
    "    axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. ROC Curves for supervised models\n",
    "    supervised_models = ['random_forest', 'xgboost', 'lightgbm']\n",
    "    for model_name in supervised_models:\n",
    "        if model_name in results and 'probabilities' in results[model_name]:\n",
    "            fpr, tpr, _ = roc_curve(y_test, results[model_name]['probabilities'])\n",
    "            auc_score = results[model_name]['auc_score']\n",
    "            axes[0, 1].plot(fpr, tpr, label=f'{model_name} (AUC: {auc_score:.3f})')\n",
    "    \n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[0, 1].set_xlabel('False Positive Rate')\n",
    "    axes[0, 1].set_ylabel('True Positive Rate')\n",
    "    axes[0, 1].set_title('ROC Curves')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Confusion matrices for best model\n",
    "    best_model = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "    cm = confusion_matrix(y_test, results[best_model]['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 0], cmap='Blues')\n",
    "    axes[1, 0].set_title(f'Confusion Matrix - {best_model}')\n",
    "    axes[1, 0].set_xlabel('Predicted')\n",
    "    axes[1, 0].set_ylabel('Actual')\n",
    "    \n",
    "    # 4. Feature importance (for tree-based models)\n",
    "    if best_model in ['random_forest', 'xgboost', 'lightgbm']:\n",
    "        model = models[best_model]['model']\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_names = [f'Feature_{i}' for i in range(len(importances))]\n",
    "            \n",
    "            # Top 10 features\n",
    "            top_indices = np.argsort(importances)[-10:]\n",
    "            axes[1, 1].barh(range(len(top_indices)), importances[top_indices])\n",
    "            axes[1, 1].set_yticks(range(len(top_indices)))\n",
    "            axes[1, 1].set_yticklabels([feature_names[i] for i in top_indices])\n",
    "            axes[1, 1].set_title(f'Top 10 Feature Importances - {best_model}')\n",
    "            axes[1, 1].set_xlabel('Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, performance_df\n",
    "\n",
    "# Test evaluation\n",
    "best_model_test, perf_df_test = evaluate_and_visualize_models(\n",
    "    models_test, results_test, X_test_data, y_test_data, datasets[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_to_s3(models, dataset_name, performance_df):\n",
    "    \"\"\"Save trained models to S3\"\"\"\n",
    "    print(f\"\\nSaving models for {dataset_name} to S3...\")\n",
    "    \n",
    "    for model_name, model_data in models.items():\n",
    "        # Save model locally first\n",
    "        model_filename = f\"/tmp/{dataset_name}_{model_name}_model.joblib\"\n",
    "        joblib.dump(model_data, model_filename)\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3_key = f\"trained-models/{dataset_name}/{model_name}_model.joblib\"\n",
    "        s3_client.upload_file(model_filename, OUTPUT_BUCKET, s3_key)\n",
    "        print(f\"  ✅ Uploaded {model_name} to s3://{OUTPUT_BUCKET}/{s3_key}\")\n",
    "    \n",
    "    # Save performance metrics\n",
    "    perf_filename = f\"/tmp/{dataset_name}_performance.csv\"\n",
    "    performance_df.to_csv(perf_filename, index=False)\n",
    "    \n",
    "    s3_key = f\"trained-models/{dataset_name}/performance_metrics.csv\"\n",
    "    s3_client.upload_file(perf_filename, OUTPUT_BUCKET, s3_key)\n",
    "    print(f\"  📊 Uploaded performance metrics to s3://{OUTPUT_BUCKET}/{s3_key}\")\n",
    "\n",
    "# Test saving\n",
    "save_models_to_s3(models_test, datasets[0].replace('_cleaned.csv', ''), perf_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for all datasets\n",
    "all_results = {}\n",
    "all_models = {}\n",
    "all_performance = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_name = dataset.replace('_cleaned.csv', '')\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PROCESSING DATASET: {dataset_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Load and prepare data\n",
    "        X, y, features = load_and_prepare_data(dataset)\n",
    "        \n",
    "        # Train models\n",
    "        models, results, X_test, y_test = train_anomaly_models(X, y, dataset_name)\n",
    "        \n",
    "        # Evaluate and visualize\n",
    "        best_model, performance_df = evaluate_and_visualize_models(\n",
    "            models, results, X_test, y_test, dataset_name\n",
    "        )\n",
    "        \n",
    "        # Save models to S3\n",
    "        save_models_to_s3(models, dataset_name, performance_df)\n",
    "        \n",
    "        # Store results\n",
    "        all_results[dataset_name] = results\n",
    "        all_models[dataset_name] = models\n",
    "        all_performance[dataset_name] = {\n",
    "            'best_model': best_model,\n",
    "            'performance_df': performance_df,\n",
    "            'features': features\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n✅ Successfully trained models for {dataset_name}\")\n",
    "        print(f\"Best model: {best_model}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error processing {dataset_name}: {str(e)}\")\n",
    "        all_results[dataset_name] = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overall training summary\n",
    "training_summary = {\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "    'total_datasets': len(datasets),\n",
    "    'successful_trainings': len([r for r in all_results.values() if 'error' not in r]),\n",
    "    'failed_trainings': len([r for r in all_results.values() if 'error' in r]),\n",
    "    'models_trained': ['isolation_forest', 'one_class_svm', 'random_forest', 'xgboost', 'lightgbm'],\n",
    "    'dataset_results': {}\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total datasets: {training_summary['total_datasets']}\")\n",
    "print(f\"Successful trainings: {training_summary['successful_trainings']}\")\n",
    "print(f\"Failed trainings: {training_summary['failed_trainings']}\")\n",
    "print(f\"Models per dataset: {len(training_summary['models_trained'])}\")\n",
    "\n",
    "print(\"\\nDataset Results:\")\n",
    "for dataset_name, perf_data in all_performance.items():\n",
    "    if 'best_model' in perf_data:\n",
    "        best_model = perf_data['best_model']\n",
    "        best_accuracy = perf_data['performance_df'][perf_data['performance_df']['Model'] == best_model]['Accuracy'].iloc[0]\n",
    "        print(f\"  ✅ {dataset_name}: Best model = {best_model} (Accuracy: {best_accuracy:.4f})\")\n",
    "        \n",
    "        training_summary['dataset_results'][dataset_name] = {\n",
    "            'best_model': best_model,\n",
    "            'best_accuracy': float(best_accuracy),\n",
    "            'features_count': len(perf_data['features'])\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  ❌ {dataset_name}: Training failed\")\n",
    "        training_summary['dataset_results'][dataset_name] = {'status': 'failed'}\n",
    "\n",
    "# Save training summary\n",
    "summary_json = json.dumps(training_summary, indent=2)\n",
    "with open('/tmp/training_summary.json', 'w') as f:\n",
    "    f.write(summary_json)\n",
    "\n",
    "s3_client.upload_file('/tmp/training_summary.json', OUTPUT_BUCKET, 'trained-models/training_summary.json')\n",
    "print(f\"\\n📊 Training summary saved to s3://{OUTPUT_BUCKET}/trained-models/training_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison visualization\n",
    "if all_performance:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Best model per dataset\n",
    "    datasets_names = []\n",
    "    best_models = []\n",
    "    best_accuracies = []\n",
    "    \n",
    "    for dataset_name, perf_data in all_performance.items():\n",
    "        if 'best_model' in perf_data:\n",
    "            datasets_names.append(dataset_name)\n",
    "            best_models.append(perf_data['best_model'])\n",
    "            best_accuracy = perf_data['performance_df'][perf_data['performance_df']['Model'] == perf_data['best_model']]['Accuracy'].iloc[0]\n",
    "            best_accuracies.append(best_accuracy)\n",
    "    \n",
    "    # Bar chart of best accuracies\n",
    "    bars = ax1.bar(datasets_names, best_accuracies)\n",
    "    ax1.set_title('Best Model Accuracy by Dataset')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Dataset')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar, acc in zip(bars, best_accuracies):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Model frequency chart\n",
    "    model_counts = pd.Series(best_models).value_counts()\n",
    "    ax2.pie(model_counts.values, labels=model_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Best Model Distribution Across Datasets')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🎉 Model training completed successfully!\")\n",
    "    print(f\"📁 All models saved to s3://{OUTPUT_BUCKET}/trained-models/\")\n",
    "    print(\"\\n🚀 Your models are ready for deployment and inference!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No models were successfully trained. Please check the errors above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}