{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Fire Detection AI - 5M Dataset Training (Compact)\n",
    "\n",
    "This notebook implements an optimized training pipeline for Fire Detection AI models using a 5M sample from the 50M dataset. The goal is to significantly reduce training time from 43 hours while maintaining reasonable accuracy.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Optimized for ml.p3.16xlarge**: Uses all 8 NVIDIA V100 GPUs\n",
    "- **5M Dataset**: Samples from the full 50M dataset\n",
    "- **Multi-GPU Training**: Parallel training across all GPUs\n",
    "- **Reduced Training Time**: 2-4 hours vs 43 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "!pip install \"numexpr>=2.8.4\" xgboost lightgbm boto3 sagemaker\n",
    "\n",
    "# Restart kernel after installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Configure notebook settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup for ml.p3.16xlarge\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Set environment variables for optimal performance\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'  # All 8 GPUs on p3.16xlarge\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1'  # May help with some multi-GPU issues\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Run nvidia-smi for detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "DATASET_BUCKET = \"synthetic-data-4\"\n",
    "DATASET_PREFIX = \"datasets/\"\n",
    "SAMPLE_SIZE = 5000000  # 5M samples\n",
    "RANDOM_SEED = 42\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.002\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "# Model configuration\n",
    "D_MODEL = 128  # Reduced from 256\n",
    "NUM_HEADS = 4  # Reduced from 8\n",
    "NUM_LAYERS = 3  # Reduced from 6\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we should use saved numpy files\n",
    "if os.path.exists('X_train.npy') and os.path.exists('y_train.npy'):\n",
    "    logger.info(\"Loading data from saved numpy files...\")\n",
    "    X_train = np.load('X_train.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    X_val = np.load('X_val.npy')\n",
    "    y_val = np.load('y_val.npy')\n",
    "    X_test = np.load('X_test.npy')\n",
    "    y_test = np.load('y_test.npy')\n",
    "    \n",
    "    if os.path.exists('areas_train.npy'):\n",
    "        areas_train = np.load('areas_train.npy')\n",
    "        areas_val = np.load('areas_val.npy')\n",
    "        areas_test = np.load('areas_test.npy')\n",
    "    else:\n",
    "        # Create dummy area values if not available\n",
    "        areas_train = np.zeros(len(y_train), dtype=int)\n",
    "        areas_val = np.zeros(len(y_val), dtype=int)\n",
    "        areas_test = np.zeros(len(y_test), dtype=int)\n",
    "    \n",
    "    logger.info(f\"Loaded data: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "else:\n",
    "    # Load data from S3\n",
    "    logger.info(\"Loading data from S3...\")\n",
    "    \n",
    "    # List files in S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.list_objects_v2(Bucket=DATASET_BUCKET, Prefix=DATASET_PREFIX)\n",
    "    \n",
    "    if 'Contents' not in response:\n",
    "        logger.error(f\"No files found in s3://{DATASET_BUCKET}/{DATASET_PREFIX}\")\n",
    "        files = []\n",
    "    else:\n",
    "        files = [obj['Key'] for obj in response['Contents'] if not obj['Key'].endswith('/')]\n",
    "        logger.info(f\"Found {len(files)} files in S3\")\n",
    "        \n",
    "        # For demonstration, limit to first few files\n",
    "        files = files[:5]  # Adjust as needed\n",
    "        logger.info(f\"Using {len(files)} files for sampling\")\n",
    "        \n",
    "        # Sample data from S3\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        all_areas = []\n",
    "        \n",
    "        for i, file_key in enumerate(files):\n",
    "            try:\n",
    "                logger.info(f\"Processing file {i+1}/{len(files)}: {file_key}\")\n",
    "                \n",
    "                # Get file from S3\n",
    "                response = s3_client.get_object(Bucket=DATASET_BUCKET, Key=file_key)\n",
    "                \n",
    "                # Extract area from filename\n",
    "                area_name = file_key.split('/')[-1].split('_')[0]\n",
    "                area_idx = hash(area_name) % 5  # Map to 5 areas\n",
    "                \n",
    "                # Read file in chunks\n",
    "                chunk_size = 100000  # 100K rows per chunk\n",
    "                chunk_iter = pd.read_csv(response['Body'], chunksize=chunk_size)\n",
    "                \n",
    "                for chunk in chunk_iter:\n",
    "                    # Extract features and labels\n",
    "                    feature_cols = [col for col in chunk.columns if col not in ['timestamp', 'label', 'is_anomaly']]\n",
    "                    \n",
    "                    if 'label' in chunk.columns:\n",
    "                        labels = chunk['label'].values\n",
    "                    elif 'is_anomaly' in chunk.columns:\n",
    "                        labels = chunk['is_anomaly'].values\n",
    "                    else:\n",
    "                        # Generate synthetic labels\n",
    "                        values = chunk[feature_cols[0]].values\n",
    "                        q95 = np.percentile(values, 95)\n",
    "                        q85 = np.percentile(values, 85)\n",
    "                        \n",
    "                        labels = np.zeros(len(values))\n",
    "                        labels[values > q95] = 2  # Fire (top 5%)\n",
    "                        labels[(values > q85) & (values <= q95)] = 1  # Warning (85-95%)\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = chunk[feature_cols].values\n",
    "                    \n",
    "                    # Standardize to 6 features\n",
    "                    if features.shape[1] < 6:\n",
    "                        padding = np.zeros((features.shape[0], 6 - features.shape[1]))\n",
    "                        features = np.hstack([features, padding])\n",
    "                    elif features.shape[1] > 6:\n",
    "                        features = features[:, :6]\n",
    "                    \n",
    "                    # Add to lists\n",
    "                    all_data.append(features)\n",
    "                    all_labels.append(labels)\n",
    "                    all_areas.append(np.full(len(labels), area_idx))\n",
    "                    \n",
    "                    # Break if we have enough data\n",
    "                    if sum(len(d) for d in all_data) >= SAMPLE_SIZE:\n",
    "                        break\n",
    "                \n",
    "                # Break if we have enough data\n",
    "                if sum(len(d) for d in all_data) >= SAMPLE_SIZE:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {file_key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all data\n",
    "        X = np.vstack(all_data)\n",
    "        y = np.concatenate(all_labels)\n",
    "        areas = np.concatenate(all_areas)\n",
    "        \n",
    "        # Ensure we have exactly the requested sample size\n",
    "        if len(X) > SAMPLE_SIZE:\n",
    "            indices = np.random.choice(len(X), SAMPLE_SIZE, replace=False)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "            areas = areas[indices]\n",
    "        \n",
    "        logger.info(f\"Raw dataset: X={X.shape}, y={y.shape}, areas={areas.shape}\")\n",
    "        \n",
    "        # Create sequences\n",
    "        seq_len = 60\n",
    "        step = 10\n",
    "        logger.info(f\"Creating sequences with length {seq_len} and step {step}\")\n",
    "        \n",
    "        X_sequences = []\n",
    "        y_sequences = []\n",
    "        areas_sequences = []\n",
    "        \n",
    "        # Process each area separately\n",
    "        unique_areas = np.unique(areas)\n",
    "        for area_idx in unique_areas:\n",
    "            area_mask = areas == area_idx\n",
    "            X_area = X[area_mask]\n",
    "            y_area = y[area_mask]\n",
    "            \n",
    "            sequences = []\n",
    "            labels = []\n",
    "            \n",
    "            for i in range(0, len(X_area) - seq_len, step):\n",
    "                sequences.append(X_area[i:i+seq_len])\n",
    "                labels.append(y_area[i+seq_len-1])  # Use label of last timestep\n",
    "            \n",
    "            if sequences:\n",
    "                X_sequences.append(np.array(sequences))\n",
    "                y_sequences.append(np.array(labels))\n",
    "                areas_sequences.append(np.full(len(sequences), area_idx))\n",
    "        \n",
    "        # Combine sequences from all areas\n",
    "        X = np.vstack(X_sequences)\n",
    "        y = np.hstack(y_sequences)\n",
    "        areas = np.hstack(areas_sequences)\n",
    "        \n",
    "        logger.info(f\"Sequence dataset: X={X.shape}, y={y.shape}, areas={areas.shape}\")\n",
    "        \n",
    "        # Split into train/validation/test sets\n",
    "        X_train, X_test, y_train, y_test, areas_train, areas_test = train_test_split(\n",
    "            X, y, areas, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "        )\n",
    "        \n",
    "        X_train, X_val, y_train, y_val, areas_train, areas_val = train_test_split(\n",
    "            X_train, y_train, areas_train, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        # Save to numpy files for faster loading in the future\n",
    "        np.save('X_train.npy', X_train)\n",
    "        np.save('y_train.npy', y_train)\n",
    "        np.save('areas_train.npy', areas_train)\n",
    "        np.save('X_val.npy', X_val)\n",
    "        np.save('y_val.npy', y_val)\n",
    "        np.save('areas_val.npy', areas_val)\n",
    "        np.save('X_test.npy', X_test)\n",
    "        np.save('y_test.npy', y_test)\n",
    "        np.save('areas_test.npy', areas_test)\n",
    "        \n",
    "        logger.info(f\"Data saved to numpy files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedFireTransformer(nn.Module):\n",
    "    \"\"\"Optimized transformer for multi-area fire detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=6, seq_len=60, d_model=128, num_heads=4, \n",
    "                 num_layers=3, num_classes=3, num_areas=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.area_embedding = nn.Embedding(num_areas, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads, dim_feedforward=d_model*4,\n",
    "            dropout=dropout, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.fire_classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.risk_predictor = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, area_types):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        x = self.input_proj(x)\n",
    "        area_emb = self.area_embedding(area_types).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        x = x + area_emb + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Global pooling\n",
    "        \n",
    "        return {\n",
    "            'fire_logits': self.fire_classifier(x),\n",
    "            'risk_score': self.risk_predictor(x) * 100.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = OptimizedFireTransformer(\n",
    "    input_dim=X_train.shape[2],\n",
    "    seq_len=X_train.shape[1],\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=len(np.unique(y_train)),\n",
    "    num_areas=len(np.unique(areas_train)),\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Enable multi-GPU training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Model parameters: {total_params:,} (trainable: {trainable_params:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, areas_train, X_val, y_val, areas_val, epochs=EPOCHS):\n",
    "    \"\"\"Train the model with early stopping\"\"\"\n",
    "    logger.info(\"ðŸ¤– TRAINING MODEL\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "    y_val_tensor = torch.LongTensor(y_val).to(device)\n",
    "    areas_train_tensor = torch.LongTensor(areas_train).to(device)\n",
    "    areas_val_tensor = torch.LongTensor(areas_val).to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    training_history = []\n",
    "    \n",
    "    # Create data loaders for batch training\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor, areas_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor, areas_val_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, areas) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs, areas)\n",
    "            loss = criterion(outputs['fire_logits'], targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs['fire_logits'].max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Print batch progress\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                logger.info(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, areas in val_loader:\n",
    "                outputs = model(inputs, areas)\n",
    "                loss = criterion(outputs['fire_logits'], targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs['fire_logits'].max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Log progress\n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Update training history\n",
    "        training_history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, f'checkpoints/model_checkpoint_epoch_{epoch+1}.pt')\n",
    "            \n",
    "            logger.info(f\"âœ… Checkpoint saved at epoch {epoch+1}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Visualize training progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            visualize_training_progress(training_history)\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    logger.info(f\"âœ… Training completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    return model, best_val_acc, training_history\n",
    "\n",
    "def visualize_training_progress(