{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Pipeline for Sensor Datasets\n",
    "This notebook cleans the synthetic sensor datasets stored in S3 for anomaly detection model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy boto3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "INPUT_BUCKET = \"synthetic-data-4\"\n",
    "OUTPUT_BUCKET = \"processedd-synthetic-data\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "\n",
    "print(f\"Input bucket: s3://{INPUT_BUCKET}\")\n",
    "print(f\"Output bucket: s3://{OUTPUT_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets to process\n",
    "datasets = [\n",
    "    \"datasets/arc_data.csv\",\n",
    "    \"datasets/asd_data.csv\", \n",
    "    \"datasets/basement_data.csv\",\n",
    "    \"datasets/laundry_data.csv\",\n",
    "    \"datasets/voc_data.csv\"\n",
    "]\n",
    "\n",
    "print(\"Datasets to process:\")\n",
    "for dataset in datasets:\n",
    "    print(f\"  - {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df, dataset_name):\n",
    "    \"\"\"Clean a single dataset\"\"\"\n",
    "    print(f\"\\n=== Cleaning {dataset_name} ===\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    \n",
    "    # 1. Remove duplicates\n",
    "    df_clean = df.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {df_clean.shape}\")\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    print(f\"Missing values per column:\")\n",
    "    print(df_clean.isnull().sum())\n",
    "    df_clean = df_clean.dropna()\n",
    "    print(f\"After removing null values: {df_clean.shape}\")\n",
    "    \n",
    "    # 3. Convert timestamp to datetime\n",
    "    if 'timestamp' in df_clean.columns:\n",
    "        df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n",
    "        df_clean = df_clean.sort_values('timestamp')\n",
    "        print(\"Converted timestamp to datetime and sorted\")\n",
    "    \n",
    "    # 4. Convert boolean strings to integers\n",
    "    if 'is_anomaly' in df_clean.columns:\n",
    "        df_clean['is_anomaly'] = df_clean['is_anomaly'].map({\n",
    "            'True': 1, 'False': 0, True: 1, False: 0\n",
    "        })\n",
    "        print(\"Converted is_anomaly to integers\")\n",
    "    \n",
    "    # 5. Handle numeric columns and remove outliers\n",
    "    if 'value' in df_clean.columns:\n",
    "        df_clean['value'] = pd.to_numeric(df_clean['value'], errors='coerce')\n",
    "        df_clean = df_clean.dropna(subset=['value'])\n",
    "        \n",
    "        # Remove outliers (values beyond 3 standard deviations)\n",
    "        mean_val = df_clean['value'].mean()\n",
    "        std_val = df_clean['value'].std()\n",
    "        \n",
    "        if std_val > 0:\n",
    "            outlier_mask = abs(df_clean['value'] - mean_val) <= 3 * std_val\n",
    "            outliers_removed = len(df_clean) - outlier_mask.sum()\n",
    "            df_clean = df_clean[outlier_mask]\n",
    "            print(f\"Removed {outliers_removed} outliers\")\n",
    "    \n",
    "    print(f\"Final shape: {df_clean.shape}\")\n",
    "    reduction = (1 - len(df_clean) / len(df)) * 100\n",
    "    print(f\"Data reduction: {reduction:.2f}%\")\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each dataset\n",
    "results = {}\n",
    "\n",
    "for dataset_path in datasets:\n",
    "    dataset_name = dataset_path.split('/')[-1].replace('.csv', '')\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING: {dataset_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Read dataset from S3\n",
    "        print(f\"Reading s3://{INPUT_BUCKET}/{dataset_path}...\")\n",
    "        df = pd.read_csv(f\"s3://{INPUT_BUCKET}/{dataset_path}\")\n",
    "        \n",
    "        # Clean the dataset\n",
    "        df_clean = clean_dataset(df, dataset_name)\n",
    "        \n",
    "        # Save cleaned dataset to S3\n",
    "        output_path = f\"s3://{OUTPUT_BUCKET}/cleaned-data/{dataset_name}_cleaned.csv\"\n",
    "        print(f\"\\nSaving to {output_path}...\")\n",
    "        df_clean.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Store results\n",
    "        results[dataset_name] = {\n",
    "            'status': 'success',\n",
    "            'original_rows': len(df),\n",
    "            'cleaned_rows': len(df_clean),\n",
    "            'reduction_percent': round((1 - len(df_clean) / len(df)) * 100, 2),\n",
    "            'output_path': output_path\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Successfully processed {dataset_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {dataset_name}: {str(e)}\")\n",
    "        results[dataset_name] = {\n",
    "            'status': 'error',\n",
    "            'message': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display summary\n",
    "summary = {\n",
    "    'cleaning_timestamp': datetime.now().isoformat(),\n",
    "    'input_bucket': INPUT_BUCKET,\n",
    "    'output_bucket': OUTPUT_BUCKET,\n",
    "    'total_datasets': len(results),\n",
    "    'successful_cleanings': sum(1 for r in results.values() if r['status'] == 'success'),\n",
    "    'failed_cleanings': sum(1 for r in results.values() if r['status'] == 'error'),\n",
    "    'details': results\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total datasets processed: {summary['total_datasets']}\")\n",
    "print(f\"Successful: {summary['successful_cleanings']}\")\n",
    "print(f\"Failed: {summary['failed_cleanings']}\")\n",
    "print(\"\\nDataset Details:\")\n",
    "\n",
    "for dataset, result in results.items():\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"  âœ… {dataset}: {result['reduction_percent']}% reduction ({result['original_rows']:,} â†’ {result['cleaned_rows']:,} rows)\")\n",
    "    else:\n",
    "        print(f\"  âŒ {dataset}: FAILED - {result.get('message', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary report to S3\n",
    "summary_json = json.dumps(summary, indent=2)\n",
    "summary_path = f\"s3://{OUTPUT_BUCKET}/cleaning-reports/summary.json\"\n",
    "\n",
    "with open('/tmp/summary.json', 'w') as f:\n",
    "    f.write(summary_json)\n",
    "\n",
    "s3_client.upload_file('/tmp/summary.json', OUTPUT_BUCKET, 'cleaning-reports/summary.json')\n",
    "print(f\"\\nðŸ“Š Summary report saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cleaning results\n",
    "if any(r['status'] == 'success' for r in results.values()):\n",
    "    successful_results = {k: v for k, v in results.items() if v['status'] == 'success'}\n",
    "    \n",
    "    datasets_names = list(successful_results.keys())\n",
    "    original_rows = [r['original_rows'] for r in successful_results.values()]\n",
    "    cleaned_rows = [r['cleaned_rows'] for r in successful_results.values()]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar chart of row counts\n",
    "    x = np.arange(len(datasets_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, original_rows, width, label='Original', alpha=0.8)\n",
    "    ax1.bar(x + width/2, cleaned_rows, width, label='Cleaned', alpha=0.8)\n",
    "    ax1.set_xlabel('Datasets')\n",
    "    ax1.set_ylabel('Number of Rows')\n",
    "    ax1.set_title('Dataset Sizes: Before vs After Cleaning')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(datasets_names, rotation=45)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pie chart of data reduction\n",
    "    reductions = [r['reduction_percent'] for r in successful_results.values()]\n",
    "    ax2.pie(reductions, labels=datasets_names, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Data Reduction Percentage by Dataset')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Data cleaning completed! Your datasets are ready for model training.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No datasets were successfully processed. Please check the errors above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}