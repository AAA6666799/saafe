{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Complete Fire Detection Ensemble - AWS Bedrock Ready\n",
    "\n",
    "## All 17+ Fire Detection Algorithms Implementation\n",
    "\n",
    "**Target: 97-98% accuracy with comprehensive ensemble**\n",
    "\n",
    "### Complete Algorithm List:\n",
    "1. **Spatio-Temporal Transformer** - Multi-attention mechanism\n",
    "2. **LSTM-CNN Hybrid** - Sequential + local patterns\n",
    "3. **Graph Neural Network** - Sensor relationships\n",
    "4. **Temporal Convolutional Network** - Parallel processing\n",
    "5. **LSTM Variational Autoencoder** - Uncertainty quantification\n",
    "6. **XGBoost Classifier** - Gradient boosting specialist\n",
    "7. **LightGBM Classifier** - Fast gradient boosting\n",
    "8. **CatBoost Classifier** - Categorical feature handling\n",
    "9. **Random Forest** - Ensemble decision trees\n",
    "10. **Gradient Boosting** - Sequential error correction\n",
    "11. **Isolation Forest** - Unsupervised anomaly detection\n",
    "12. **One-Class SVM** - Novelty detection\n",
    "13. **Statistical Anomaly Detection** - Z-score & IQR based\n",
    "14. **ARIMA Time Series** - Statistical forecasting\n",
    "15. **Prophet Time Series** - Trend decomposition\n",
    "16. **Stacking Ensemble** - Meta-learning combiner\n",
    "17. **Bayesian Model Averaging** - Uncertainty-weighted ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install required packages for complete ensemble\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"‚úÖ {package} installed\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è {package} installation failed\")\n",
    "\n",
    "packages = [\n",
    "    \"numpy\", \"pandas\", \"scikit-learn\", \n",
    "    \"xgboost\", \"lightgbm\", \"catboost\",\n",
    "    \"torch\", \"matplotlib\", \"seaborn\", \"boto3\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nüî• All packages ready for Fire Detection Ensemble!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Core\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Gradient Boosting Specialists\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not available\")\n",
    "    xgb = None\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"‚úÖ LightGBM available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM not available\")\n",
    "    lgb = None\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    print(\"‚úÖ CatBoost available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è CatBoost not available\")\n",
    "    cb = None\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"‚úÖ PyTorch available - Device: {DEVICE}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyTorch not available\")\n",
    "    torch = None\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# AWS Integration\n",
    "try:\n",
    "    import boto3\n",
    "    print(\"‚úÖ AWS Boto3 available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è AWS Boto3 not available\")\n",
    "    boto3 = None\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "print(f\"\\nüî• Fire Detection Ensemble - AWS Bedrock Ready!\")\n",
    "print(f\"üöÄ Ready to implement 17+ algorithms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Deep Learning Models (Tier 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Fire Detection Models\n",
    "if torch:\n",
    "    class SimpleTransformer(nn.Module):\n",
    "        \"\"\"1. Simplified Transformer for Fire Detection\"\"\"\n",
    "        def __init__(self, input_size=6, d_model=64, num_classes=3):\n",
    "            super().__init__()\n",
    "            self.input_projection = nn.Linear(input_size, d_model)\n",
    "            self.encoder = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=4, dim_feedforward=128, batch_first=True\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model, d_model//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_model//2, num_classes)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.input_projection(x)\n",
    "            x = self.encoder(x)\n",
    "            x = x.mean(dim=1)  # Global average pooling\n",
    "            return self.classifier(x)\n",
    "\n",
    "    class LSTMCNNModel(nn.Module):\n",
    "        \"\"\"2. LSTM-CNN Hybrid\"\"\"\n",
    "        def __init__(self, input_size=6, hidden_size=64, num_classes=3):\n",
    "            super().__init__()\n",
    "            self.conv1d = nn.Conv1d(input_size, 32, 3, padding=1)\n",
    "            self.lstm = nn.LSTM(32, hidden_size, batch_first=True)\n",
    "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = x.transpose(1, 2)  # For conv1d\n",
    "            x = F.relu(self.conv1d(x))\n",
    "            x = x.transpose(1, 2)  # Back for LSTM\n",
    "            _, (h_n, _) = self.lstm(x)\n",
    "            return self.classifier(h_n.squeeze(0))\n",
    "\n",
    "    class GraphFireModel(nn.Module):\n",
    "        \"\"\"3. Simple Graph Neural Network\"\"\"\n",
    "        def __init__(self, input_size=6, hidden_size=64, num_classes=3):\n",
    "            super().__init__()\n",
    "            self.node_encoder = nn.Linear(input_size, hidden_size)\n",
    "            self.graph_conv = nn.Linear(hidden_size, hidden_size)\n",
    "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.node_encoder(x))\n",
    "            x = F.relu(self.graph_conv(x))\n",
    "            x = x.mean(dim=1)  # Graph pooling\n",
    "            return self.classifier(x)\n",
    "\n",
    "    class TemporalCNN(nn.Module):\n",
    "        \"\"\"4. Temporal Convolutional Network\"\"\"\n",
    "        def __init__(self, input_size=6, num_classes=3):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv1d(input_size, 32, 3, padding=1)\n",
    "            self.conv2 = nn.Conv1d(32, 64, 3, padding=2, dilation=2)\n",
    "            self.conv3 = nn.Conv1d(64, 64, 3, padding=4, dilation=4)\n",
    "            self.classifier = nn.Linear(64, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = x.transpose(1, 2)\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.relu(self.conv3(x))\n",
    "            x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)\n",
    "            return self.classifier(x)\n",
    "\n",
    "    class LSTMAutoencoder(nn.Module):\n",
    "        \"\"\"5. LSTM Autoencoder with Classification\"\"\"\n",
    "        def __init__(self, input_size=6, hidden_size=32, num_classes=3):\n",
    "            super().__init__()\n",
    "            self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            _, (h_n, _) = self.encoder(x)\n",
    "            return self.classifier(h_n.squeeze(0))\n",
    "\n",
    "    print(\"‚úÖ All 5 Deep Learning models defined!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PyTorch not available - Deep Learning models skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Traditional ML Models (Tier 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraditionalMLEnsemble:\n",
    "    \"\"\"Traditional ML Models for Fire Detection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        \n",
    "        # Core Models\n",
    "        self.models['random_forest'] = RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        self.models['gradient_boosting'] = GradientBoostingClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        # Gradient Boosting Specialists\n",
    "        if xgb:\n",
    "            self.models['xgboost'] = xgb.XGBClassifier(\n",
    "                n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                random_state=RANDOM_STATE, eval_metric='logloss'\n",
    "            )\n",
    "        \n",
    "        if lgb:\n",
    "            self.models['lightgbm'] = lgb.LGBMClassifier(\n",
    "                n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                random_state=RANDOM_STATE, verbose=-1\n",
    "            )\n",
    "        \n",
    "        if cb:\n",
    "            self.models['catboost'] = cb.CatBoostClassifier(\n",
    "                iterations=100, depth=6, learning_rate=0.1,\n",
    "                random_seed=RANDOM_STATE, verbose=False\n",
    "            )\n",
    "        \n",
    "        # Anomaly Detection Models\n",
    "        self.models['isolation_forest'] = IsolationForest(\n",
    "            n_estimators=100, contamination=0.1, random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        self.models['one_class_svm'] = OneClassSVM(\n",
    "            kernel='rbf', gamma='scale', nu=0.1\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Initialized {len(self.models)} traditional ML models\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train all traditional ML models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                if name in ['isolation_forest', 'one_class_svm']:\n",
    "                    # Train only on normal data\n",
    "                    normal_mask = (y == 0)\n",
    "                    model.fit(X[normal_mask])\n",
    "                else:\n",
    "                    model.fit(X, y)\n",
    "                \n",
    "                results[name] = 'success'\n",
    "                print(f\"  ‚úÖ {name} trained\")\n",
    "            except Exception as e:\n",
    "                results[name] = f'failed: {e}'\n",
    "                print(f\"  ‚ùå {name} failed: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Get predictions from all models\"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                if name in ['isolation_forest', 'one_class_svm']:\n",
    "                    # Convert anomaly detection to multi-class\n",
    "                    anomaly_pred = model.predict(X)\n",
    "                    pred = np.where(anomaly_pred == -1, 2, 0)  # Anomaly -> Fire\n",
    "                else:\n",
    "                    pred = model.predict(X)\n",
    "                \n",
    "                predictions[name] = pred\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è {name} prediction failed: {e}\")\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "traditional_ensemble = TraditionalMLEnsemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Meta-Learning & Advanced Features (Tier 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedFireFeatures:\n",
    "    \"\"\"Advanced feature engineering for fire detection\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_statistical_features(X):\n",
    "        \"\"\"Extract comprehensive statistical features\"\"\"\n",
    "        if X.ndim == 3:  # (samples, timesteps, features)\n",
    "            features = []\n",
    "            \n",
    "            # Basic statistics over time\n",
    "            features.append(np.mean(X, axis=1))     # Mean\n",
    "            features.append(np.std(X, axis=1))      # Standard deviation\n",
    "            features.append(np.max(X, axis=1))      # Maximum\n",
    "            features.append(np.min(X, axis=1))      # Minimum\n",
    "            features.append(np.median(X, axis=1))   # Median\n",
    "            \n",
    "            # Percentiles\n",
    "            features.append(np.percentile(X, 25, axis=1))  # 25th percentile\n",
    "            features.append(np.percentile(X, 75, axis=1))  # 75th percentile\n",
    "            \n",
    "            # Temporal features\n",
    "            if X.shape[1] > 1:\n",
    "                features.append(X[:, -1, :] - X[:, 0, :])  # End - Start\n",
    "                \n",
    "                # First differences\n",
    "                diff = np.diff(X, axis=1)\n",
    "                features.append(np.mean(diff, axis=1))     # Mean change\n",
    "                features.append(np.std(diff, axis=1))      # Change volatility\n",
    "            \n",
    "            return np.hstack(features)\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_time_series_features(X):\n",
    "        \"\"\"Extract time series specific features (ARIMA-style)\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            sample_features = []\n",
    "            \n",
    "            for j in range(X.shape[2]):\n",
    "                series = X[i, :, j]\n",
    "                \n",
    "                # Autocorrelation\n",
    "                if len(series) > 1:\n",
    "                    autocorr = np.corrcoef(series[:-1], series[1:])[0, 1]\n",
    "                    autocorr = 0 if np.isnan(autocorr) else autocorr\n",
    "                else:\n",
    "                    autocorr = 0\n",
    "                \n",
    "                # Trend (linear regression slope)\n",
    "                if len(series) > 1:\n",
    "                    trend = np.polyfit(range(len(series)), series, 1)[0]\n",
    "                else:\n",
    "                    trend = 0\n",
    "                \n",
    "                # Seasonality (simplified)\n",
    "                if len(series) >= 4:\n",
    "                    seasonal_var = np.var([series[k::4] for k in range(4) if k < len(series)])\n",
    "                else:\n",
    "                    seasonal_var = 0\n",
    "                \n",
    "                sample_features.extend([autocorr, trend, seasonal_var])\n",
    "            \n",
    "            features.append(sample_features)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_prophet_features(X):\n",
    "        \"\"\"Create Prophet-style trend decomposition features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Simple trend analysis for each sensor\n",
    "            sample_features = []\n",
    "            \n",
    "            for j in range(X.shape[2]):\n",
    "                series = X[i, :, j]\n",
    "                \n",
    "                # Linear trend\n",
    "                if len(series) > 1:\n",
    "                    linear_trend = np.polyfit(range(len(series)), series, 1)[0]\n",
    "                else:\n",
    "                    linear_trend = 0\n",
    "                \n",
    "                # Residual after detrending\n",
    "                if len(series) > 1:\n",
    "                    detrended = series - (linear_trend * np.arange(len(series)) + np.mean(series))\n",
    "                    residual_std = np.std(detrended)\n",
    "                else:\n",
    "                    residual_std = 0\n",
    "                \n",
    "                sample_features.extend([linear_trend, residual_std])\n",
    "            \n",
    "            features.append(sample_features)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "class MetaLearningSystem:\n",
    "    \"\"\"Meta-learning for ensemble combination\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stacking_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit_stacking(self, base_predictions, y):\n",
    "        \"\"\"Train stacking meta-learner\"\"\"\n",
    "        if len(base_predictions) > 0:\n",
    "            X_meta = np.column_stack(base_predictions)\n",
    "            self.stacking_model.fit(X_meta, y)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def bayesian_averaging(self, predictions, uncertainties=None):\n",
    "        \"\"\"Bayesian model averaging\"\"\"\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        if uncertainties is None:\n",
    "            weights = np.ones(len(predictions)) / len(predictions)\n",
    "        else:\n",
    "            inv_uncertainties = 1.0 / (np.array(uncertainties) + 1e-8)\n",
    "            weights = inv_uncertainties / np.sum(inv_uncertainties)\n",
    "        \n",
    "        averaged = np.average(predictions, axis=0, weights=weights)\n",
    "        uncertainty = np.sqrt(np.average((predictions - averaged)**2, axis=0, weights=weights))\n",
    "        \n",
    "        return averaged, uncertainty, weights\n",
    "    \n",
    "    def ensemble_predict(self, predictions):\n",
    "        \"\"\"Final ensemble prediction\"\"\"\n",
    "        if len(predictions) == 0:\n",
    "            return np.array([0])  # Default to normal\n",
    "        \n",
    "        # Simple majority voting\n",
    "        predictions = np.array(predictions)\n",
    "        if predictions.ndim == 1:\n",
    "            return predictions\n",
    "        \n",
    "        # Vote for each sample\n",
    "        final_predictions = []\n",
    "        for i in range(predictions.shape[1]):\n",
    "            votes = predictions[:, i]\n",
    "            # Get most common prediction\n",
    "            unique, counts = np.unique(votes, return_counts=True)\n",
    "            final_predictions.append(unique[np.argmax(counts)])\n",
    "        \n",
    "        return np.array(final_predictions)\n",
    "\n",
    "print(\"‚úÖ Advanced features and meta-learning systems ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Complete Fire Detection Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteFireDetectionEnsemble:\n",
    "    \"\"\"The Ultimate Fire Detection System - All 17+ Algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.traditional_ml = TraditionalMLEnsemble()\n",
    "        self.feature_extractor = AdvancedFireFeatures()\n",
    "        self.meta_learner = MetaLearningSystem()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Deep learning models (if available)\n",
    "        self.deep_models = {}\n",
    "        if torch:\n",
    "            self.deep_models = {\n",
    "                'transformer': SimpleTransformer(),\n",
    "                'lstm_cnn': LSTMCNNModel(),\n",
    "                'graph_nn': GraphFireModel(),\n",
    "                'temporal_cnn': TemporalCNN(),\n",
    "                'lstm_ae': LSTMAutoencoder()\n",
    "            }\n",
    "        \n",
    "        self.is_fitted = False\n",
    "        \n",
    "        total_models = len(self.traditional_ml.models) + len(self.deep_models)\n",
    "        print(f\"üèóÔ∏è Complete Fire Detection Ensemble Initialized\")\n",
    "        print(f\"üìä Total Models: {total_models}\")\n",
    "        print(f\"üéØ Target Accuracy: 97%+\")\n",
    "    \n",
    "    def create_sample_data(self, n_samples=1000, seq_length=20, n_features=6):\n",
    "        \"\"\"Create realistic fire detection dataset for demonstration\"\"\"\n",
    "        print(f\"üî¨ Creating sample fire dataset: {n_samples} samples\")\n",
    "        \n",
    "        # Generate base sensor readings\n",
    "        X = np.random.randn(n_samples, seq_length, n_features) * 0.1\n",
    "        \n",
    "        # Add realistic sensor baselines\n",
    "        baselines = np.array([25.0, 45.0, 0.02, 101.3, 0.5, 10.0])  # Temp, Humidity, Smoke, Pressure, Gas, Wind\n",
    "        X += baselines\n",
    "        \n",
    "        # Generate labels: 0=Normal, 1=Warning, 2=Fire\n",
    "        y = np.random.choice([0, 1, 2], n_samples, p=[0.7, 0.2, 0.1])\n",
    "        \n",
    "        # Add realistic fire patterns\n",
    "        for i in range(n_samples):\n",
    "            if y[i] == 1:  # Warning patterns\n",
    "                X[i, -10:, 0] += np.linspace(0, 10, 10)  # Temperature rise\n",
    "                X[i, -10:, 2] += np.linspace(0, 0.05, 10)  # Smoke increase\n",
    "                \n",
    "            elif y[i] == 2:  # Fire patterns\n",
    "                X[i, -15:, 0] += np.linspace(0, 30, 15)  # High temperature\n",
    "                X[i, -15:, 2] += np.linspace(0, 0.15, 15)  # Heavy smoke\n",
    "                X[i, -10:, 1] -= np.linspace(0, 15, 10)  # Humidity drop\n",