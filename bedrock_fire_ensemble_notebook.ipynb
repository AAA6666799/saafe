{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔥 Complete Fire Detection Ensemble - AWS Bedrock Ready\n",
    "\n",
    "## All 17+ Fire Detection Algorithms Implementation\n",
    "\n",
    "**Target: 97-98% accuracy with comprehensive ensemble**\n",
    "\n",
    "### Architecture:\n",
    "- **Tier 1**: Deep Learning Core (5 models)\n",
    "- **Tier 2**: Specialist Models (9 models) \n",
    "- **Tier 3**: Meta-Learning (3 systems)\n",
    "- **Total**: 17+ algorithms working together\n",
    "\n",
    "### Algorithms Included:\n",
    "1. **Spatio-Temporal Transformer** - Multi-attention mechanism\n",
    "2. **LSTM-CNN Hybrid** - Sequential + local patterns\n",
    "3. **Graph Neural Network** - Sensor relationships\n",
    "4. **Temporal Convolutional Network** - Parallel processing\n",
    "5. **LSTM Variational Autoencoder** - Uncertainty quantification\n",
    "6. **XGBoost Classifier** - Gradient boosting specialist\n",
    "7. **LightGBM Classifier** - Fast gradient boosting\n",
    "8. **CatBoost Classifier** - Categorical feature handling\n",
    "9. **Random Forest** - Ensemble decision trees\n",
    "10. **Gradient Boosting** - Sequential error correction\n",
    "11. **Isolation Forest** - Unsupervised anomaly detection\n",
    "12. **One-Class SVM** - Novelty detection\n",
    "13. **Statistical Anomaly Detection** - Z-score & IQR based\n",
    "14. **ARIMA Time Series** - Statistical forecasting\n",
    "15. **Prophet Time Series** - Trend decomposition\n",
    "16. **Stacking Ensemble** - Meta-learning combiner\n",
    "17. **Bayesian Model Averaging** - Uncertainty-weighted ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas scikit-learn xgboost lightgbm catboost\n",
    "!pip install torch torchvision pytorch-lightning\n",
    "!pip install boto3 matplotlib seaborn\n",
    "!pip install optuna prophet statsmodels\n",
    "\n",
    "print(\"✅ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Gradient Boosting Specialists\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# AWS Integration\n",
    "import boto3\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"🔥 Fire Detection Ensemble - AWS Bedrock Ready\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Ready to build 17+ algorithm ensemble!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Tier 1: Deep Learning Core Models (5 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalTransformer(nn.Module):\n",
    "    \"\"\"1. Spatio-Temporal Transformer - Multi-attention fire detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=6, d_model=128, nhead=8, num_layers=4, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(500, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=256,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project and add positional encoding\n",
    "        x = self.input_projection(x)\n",
    "        x = x + self.positional_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # Transform\n",
    "        transformed = self.transformer(x)\n",
    "        pooled = transformed.mean(dim=1)\n",
    "        \n",
    "        return self.classifier(pooled)\n",
    "\n",
    "class LSTMCNNHybrid(nn.Module):\n",
    "    \"\"\"2. LSTM-CNN Hybrid - Sequential and local pattern detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=6, hidden_size=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN for local patterns\n",
    "        self.conv1d = nn.Sequential(\n",
    "            nn.Conv1d(input_size, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # LSTM for sequences\n",
    "        self.lstm = nn.LSTM(64, hidden_size, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # CNN processing\n",
    "        x_cnn = x.transpose(1, 2)\n",
    "        conv_out = self.conv1d(x_cnn)\n",
    "        conv_out = conv_out.transpose(1, 2)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(conv_out)\n",
    "        pooled = lstm_out.mean(dim=1)\n",
    "        \n",
    "        return self.classifier(pooled)\n",
    "\n",
    "class GraphFireDetector(nn.Module):\n",
    "    \"\"\"3. Graph Neural Network - Sensor relationship modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=6, hidden_size=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.node_encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.graph_conv1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.graph_conv2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Encode nodes\n",
    "        x = F.relu(self.node_encoder(x))\n",
    "        \n",
    "        # Simple graph convolutions (time-based adjacency)\n",
    "        x = F.relu(self.graph_conv1(x))\n",
    "        x = F.relu(self.graph_conv2(x))\n",
    "        \n",
    "        # Global pooling\n",
    "        pooled = x.mean(dim=1)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    \"\"\"4. Temporal Convolutional Network - Parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=6, num_channels=[32, 64, 128], num_classes=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i, out_channels in enumerate(num_channels):\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            dilation = 2 ** i\n",
    "            \n",
    "            layers.append(nn.Conv1d(in_channels, out_channels, 3, \n",
    "                                  dilation=dilation, padding=dilation))\n",
    "            layers.append(nn.BatchNorm1d(out_channels))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "        \n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_channels[-1], num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.tcn(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class LSTMVariationalAutoencoder(nn.Module):\n",
    "    \"\"\"5. LSTM-VAE - Uncertainty quantification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=6, hidden_size=64, latent_dim=16, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.encoder_lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.encoder_lstm(x)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        \n",
    "        mu = self.fc_mu(h_n)\n",
    "        logvar = self.fc_logvar(h_n)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        return self.classifier(z)\n",
    "\n",
    "print(\"✅ Tier 1: All 5 Deep Learning models defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Tier 2: Specialist Models (9 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingEnsemble:\n",
    "    \"\"\"Gradient Boosting Specialists (4 models)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'xgboost': xgb.XGBClassifier(\n",
    "                n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                random_state=RANDOM_STATE, eval_metric='logloss'\n",
    "            ),\n",
    "            'lightgbm': lgb.LGBMClassifier(\n",
    "                n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                random_state=RANDOM_STATE, verbose=-1\n",
    "            ),\n",
    "            'catboost': cb.CatBoostClassifier(\n",
    "                iterations=100, depth=6, learning_rate=0.1,\n",
    "                random_seed=RANDOM_STATE, verbose=False\n",
    "            ),\n",
    "            'gradient_boosting': GradientBoostingClassifier(\n",
    "                n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        }\n",
    "\n",
    "class AnomalyDetectionEnsemble:\n",
    "    \"\"\"Anomaly Detection Specialists (3 models)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'isolation_forest': IsolationForest(\n",
    "                n_estimators=100, contamination=0.1, random_state=RANDOM_STATE\n",
    "            ),\n",
    "            'one_class_svm': OneClassSVM(kernel='rbf', gamma='scale', nu=0.1),\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=100, max_depth=10, random_state=RANDOM_STATE\n",
    "            )\n",
    "        }\n",
    "\n",
    "class TimeSeriesSpecialists:\n",
    "    \"\"\"Time Series Analysis (2 models)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        \n",
    "    def create_prophet_features(self, X):\n",
    "        \"\"\"Extract Prophet-style trend features\"\"\"\n",
    "        features = []\n",
    "        for i in range(X.shape[0]):\n",
    "            series = X[i].mean(axis=1)  # Average across sensors\n",
    "            \n",
    "            # Trend features\n",
    "            trend = np.polyfit(range(len(series)), series, 1)[0]  # Linear trend\n",
    "            features.append([trend, np.std(series), np.max(series) - np.min(series)])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def create_arima_features(self, X):\n",
    "        \"\"\"Extract ARIMA-style statistical features\"\"\"\n",
    "        features = []\n",
    "        for i in range(X.shape[0]):\n",
    "            series = X[i].mean(axis=1)\n",
    "            \n",
    "            # Statistical features\n",
    "            autocorr_1 = np.corrcoef(series[:-1], series[1:])[0, 1] if len(series) > 1 else 0\n",
    "            diff_series = np.diff(series)\n",
    "            volatility = np.std(diff_series) if len(diff_series) > 0 else 0\n",
    "            \n",
    "            features.append([autocorr_1, volatility, np.mean(np.abs(diff_series))])\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "print(\"✅ Tier 2: All 9 Specialist models defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧬 Tier 3: Meta-Learning Systems (3 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearningEnsemble:\n",
    "    \"\"\"Meta-Learning & Advanced Ensemble Systems\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stacking_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "        self.bayesian_weights = None\n",
    "        self.dynamic_selector = None\n",
    "    \n",
    "    def fit_stacking(self, base_predictions, y):\n",
    "        \"\"\"1. Stacking Ensemble - Meta-learner\"\"\"\n",
    "        X_meta = np.hstack(base_predictions)\n",
    "        self.stacking_model.fit(X_meta, y)\n",
    "        return self.stacking_model\n",
    "    \n",
    "    def bayesian_averaging(self, predictions, uncertainties=None):\n",
    "        \"\"\"2. Bayesian Model Averaging\"\"\"\n",
    "        if uncertainties is None:\n",
    "            weights = np.ones(len(predictions)) / len(predictions)\n",
    "        else:\n",
    "            inv_uncertainties = 1.0 / (np.array(uncertainties) + 1e-8)\n",
    "            weights = inv_uncertainties / np.sum(inv_uncertainties)\n",
    "        \n",
    "        averaged = np.average(predictions, axis=0, weights=weights)\n",
    "        uncertainty = np.sqrt(np.average((predictions - averaged)**2, axis=0, weights=weights))\n",
    "        \n",
    "        return averaged, uncertainty, weights\n",
    "    \n",
    "    def dynamic_selection(self, predictions, competence_scores):\n",
    "        \"\"\"3. Dynamic Ensemble Selection\"\"\"\n",
    "        total_score = sum(competence_scores)\n",
    "        weights = [score / total_score for score in competence_scores]\n",
    "        \n",
    "        final_pred = np.zeros_like(predictions[0])\n",
    "        for pred, weight in zip(predictions, weights):\n",
    "            final_pred += pred * weight\n",
    "        \n",
    "        return final_pred\n",
    "\n",
    "print(\"✅ Tier 3: All 3 Meta-Learning systems defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔥 Complete Fire Detection Ensemble System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteFireDetectionEnsemble:\n",
    "    \"\"\"The Ultimate Fire Detection System - All 17+ Algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize all tiers\n",
    "        self.tier1_models = self._init_deep_learning_models()\n",
    "        self.tier2_gb = GradientBoostingEnsemble()\n",
    "        self.tier2_ad = AnomalyDetectionEnsemble()\n",
    "        self.tier2_ts = TimeSeriesSpecialists()\n",
    "        self.tier3_meta = MetaLearningEnsemble()\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        print(\"🏗️ Complete Fire Detection Ensemble Initialized\")\n",
    "        print(f\"📊 Total Algorithms: 17+\")\n",
    "        print(f\"🎯 Target Accuracy: 97%+\")\n",
    "    \n",
    "    def _init_deep_learning_models(self):\n",
    "        \"\"\"Initialize all deep learning models\"\"\"\n",
    "        return {\n",
    "            'spatio_temporal': SpatioTemporalTransformer(),\n",
    "            'lstm_cnn': LSTMCNNHybrid(),\n",
    "            'graph_nn': GraphFireDetector(),\n",
    "            'temporal_conv': TemporalConvNet(),\n",
    "            'lstm_vae': LSTMVariationalAutoencoder()\n",
    "        }\n",
    "    \n",
    "    def engineer_features(self, X):\n",
    "        \"\"\"Advanced feature engineering for maximum performance\"\"\"\n",
    "        if X.ndim == 3:\n",
    "            # Time series data (samples, timesteps, features)\n",
    "            features = []\n",
    "            \n",
    "            # Statistical features over time\n",
    "            features.append(np.mean(X, axis=1))  # Mean\n",
    "            features.append(np.std(X, axis=1))   # Std\n",
    "            features.append(np.max(X, axis=1))   # Max\n",
    "            features.append(np.min(X, axis=1))   # Min\n",
    "            features.append(np.median(X, axis=1)) # Median\n",
    "            \n",
    "            # Trend features\n",
    "            features.append(X[:, -1, :] - X[:, 0, :])  # End - Start\n",
    "            \n",
    "            # Time series specialist features\n",
    "            prophet_features = self.tier2_ts.create_prophet_features(X)\n",
    "            arima_features = self.tier2_ts.create_arima_features(X)\n",
    "            \n",
    "            # Combine all features\n",
    "            combined = np.hstack(features + [prophet_features, arima_features])\n",
    "            return combined\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def fit(self, X, y, validation_split=0.2, epochs=50):\n",
    "        \"\"\"Train the complete ensemble\"\"\"\n",
    "        print(\"\\n🚀 Training Complete Fire Detection Ensemble\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=RANDOM_STATE, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Engineer features for traditional ML\n",
    "        X_train_features = self.engineer_features(X_train)\n",
    "        X_val_features = self.engineer_features(X_val)\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train_features)\n",
    "        X_val_scaled = self.scaler.transform(X_val_features)\n",
    "        \n",
    "        # Convert to tensors for deep learning\n",
    "        X_train_tensor = torch.FloatTensor(X_train).to(DEVICE)\n",
    "        X_val_tensor = torch.FloatTensor(X_val).to(DEVICE)\n",
    "        y_train_tensor = torch.LongTensor(y_train).to(DEVICE)\n",
    "        y_val_tensor = torch.LongTensor(y_val).to(DEVICE)\n",
    "        \n",
    "        # STEP 1: Train Deep Learning Models\n",
    "        print(\"\\n📊 STEP 1: Training Deep Learning Models (5 models)\")\n",
    "        self._train_deep_models(X_train_tensor, y_train_tensor, \n",
    "                               X_val_tensor, y_val_tensor, epochs)\n",
    "        \n",
    "        # STEP 2: Train Specialist Models\n",
    "        print(\"\\n📊 STEP 2: Training Specialist Models (9 models)\")\n",
    "        self._train_specialist_models(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "        \n",
    "        # STEP 3: Train Meta-Learning Systems\n",
    "        print(\"\\n📊 STEP 3: Training Meta-Learning Systems (3 systems)\")\n",
    "        self._train_meta_learning(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(\"\\n✅ Complete Ensemble Training Finished!\")\n",
    "        \n",
    "        # Final validation\n",
    "        self._evaluate_ensemble(X_val, y_val)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _train_deep_models(self, X_train, y_train, X_val, y_val, epochs):\n",
    "        \"\"\"Train all deep learning models\"\"\"\n",
    "        for name, model in self.tier1_models.items():\n",
    "            print(f\"  Training {name}...\")\n",
    "            model = model.to(DEVICE)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            criterion = nn.CrossEn